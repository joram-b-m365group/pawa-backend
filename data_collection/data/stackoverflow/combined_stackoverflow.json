[
  {
    "prompt": "What does the &quot;yield&quot; keyword do in Python?\n\nWhat functionality does the yield keyword in Python provide?\nFor example, I'm trying to understand this code1:\ndef _get_child_candidates(self, distance, min_dist, max_dist):\n    if self._leftchild and distance - max_dist < self._median:\n        yield self._leftchild\n    if self._rightchild and distance + max_dist >= self._median:\n        yield self._rightchild  \n\nAnd this is the caller:\nresult, candidates = [], [self]\nwhile candidates:\n    node = candidates.pop()\n    distance = node._get_dist(ob",
    "response": "To understand what yield does, you must understand what generators are. And before you can understand generators, you must understand iterables.\nIterables\nWhen you create a list, you can read its items one by one. Reading its items one by one is called iteration:\n>>> mylist = [1, 2, 3]\n>>> for i in mylist:\n...    print(i)\n1\n2\n3\n\nmylist is an iterable. When you use a list comprehension, you create a list, and so an iterable:\n>>> mylist = [x*x for x in range(3)]\n>>> for i in mylist:\n...    print(i)\n0\n1\n4\n\nEverything you can use \"for... in...\" on is an iterable; lists, strings, files...\nThese iterables are handy because you can read them as much as you wish, but you store all the values in memory and this is not always what you want when you have a lot of values.\nGenerators\nGenerators are iterators, a kind of iterable you can only iterate over once. Generators do not store all the values in memory, they generate the values on the fly:\n>>> mygenerator = (x*x for x in range(3))\n>>> for i in",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 231767,
      "answer_id": 231855,
      "question_score": 13109,
      "answer_score": 18258,
      "is_accepted": true,
      "tags": [
        "python",
        "iterator",
        "generator",
        "yield"
      ]
    }
  },
  {
    "prompt": "What does if __name__ == &quot;__main__&quot;: do?\n\nWhat does this do, and why should one include the if statement?\nif __name__ == \"__main__\":\n    print(\"Hello, World!\")\n\n\nIf you are trying to close a question where someone should be using this idiom and isn't, consider closing as a duplicate of Why is Python running my module when I import it, and how do I stop it? instead. For questions where someone simply hasn't called any functions, or incorrectly expects a function named main to be used as an entry point automatically, use Why doesn't the m",
    "response": "Short Answer\nIt's boilerplate code that protects users from accidentally invoking the script when they didn't intend to. Here are some common problems when the guard is omitted from a script:\n\nIf you import the guardless script in another script (e.g. import my_script_without_a_name_eq_main_guard), then the latter script will trigger the former to run at import time and using the second script's command line arguments. This is almost always a mistake.\n\nIf you have a custom class in the guardless script and save it to a pickle file, then unpickling it in another script will trigger an import of the guardless script, with the same problems outlined in the previous bullet.\n\n\nLong Answer\nTo better understand why and how this matters, we need to take a step back to understand how Python initializes scripts and how this interacts with its module import mechanism.\nWhenever the Python interpreter reads a source file, it does two things:\n\nit sets a few special variables like __name__, and then\n",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 419163,
      "answer_id": 419185,
      "question_score": 8404,
      "answer_score": 9007,
      "is_accepted": true,
      "tags": [
        "python",
        "namespaces",
        "program-entry-point",
        "python-module",
        "idioms"
      ]
    }
  },
  {
    "prompt": "Does Python have a ternary conditional operator?\n\nIs there a ternary conditional operator in Python?",
    "response": "Yes, it was added in version 2.5. The expression syntax is:\na if condition else b\n\nFirst condition is evaluated, then exactly one of either a or b is evaluated and returned based on the Boolean value of condition. If condition evaluates to True, then a is evaluated and returned but b is ignored, or else when b is evaluated and returned but a is ignored.\nThis allows short-circuiting because when condition is true only a is evaluated and b is not evaluated at all, but when condition is false only b is evaluated and a is not evaluated at all.\nFor example:\n>>> 'true' if True else 'false'\n'true'\n>>> 'true' if False else 'false'\n'false'\n\nNote that conditionals are an expression, not a statement. This means you can't use statements such as pass, or assignments with = (or \"augmented\" assignments like +=), within a conditional expression:\n>>> pass if False else pass\n  File \"<stdin>\", line 1\n    pass if False else pass\n         ^\nSyntaxError: invalid syntax\n\n>>> # Python parses this as `x = (1 i",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 394809,
      "answer_id": 394814,
      "question_score": 8106,
      "answer_score": 9333,
      "is_accepted": true,
      "tags": [
        "python",
        "operators",
        "conditional-operator"
      ]
    }
  },
  {
    "prompt": "What are metaclasses in Python?\n\nWhat are metaclasses? What are they used for?",
    "response": "A metaclass is the class of a class. A class defines how an instance of the class (i.e. an object) behaves while a metaclass defines how a class behaves. A class is an instance of a metaclass.\n\nWhile in Python you can use arbitrary callables for metaclasses (like Jerub shows), the better approach is to make it an actual class itself. type is the usual metaclass in Python. type is itself a class, and it is its own type. You won't be able to recreate something like type purely in Python, but Python cheats a little. To create your own metaclass in Python you really just want to subclass type.\n\nA metaclass is most commonly used as a class-factory. When you create an object by calling the class, Python creates a new class (when it executes the 'class' statement) by calling the metaclass. Combined with the normal __init__ and __new__ methods, metaclasses therefore allow you to do 'extra things' when creating a class, like registering the new class with some registry or replace the class with",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 100003,
      "answer_id": 100146,
      "question_score": 7513,
      "answer_score": 3442,
      "is_accepted": true,
      "tags": [
        "python",
        "oop",
        "metaclass",
        "python-class",
        "python-datamodel"
      ]
    }
  },
  {
    "prompt": "How do I check whether a file exists without exceptions?\n\nHow do I check whether a file exists or not, without using the try statement?",
    "response": "If the reason you're checking is so you can do something like if file_exists: open_it(), it's safer to use a try around the attempt to open it. Checking and then opening risks the file being deleted or moved or something between when you check and when you try to open it.\nIf you're not planning to open the file immediately, you can use os.path.isfile if you need to be sure it's a file.\n\nReturn True if path is an existing regular file. This follows symbolic links, so both islink() and isfile() can be true for the same path.\n\nimport os.path\nos.path.isfile(fname)\n\npathlib\nStarting with Python 3.4, the pathlib module offers an object-oriented approach (backported to pathlib2 in Python 2.7):\nfrom pathlib import Path\n\nmy_file = Path(\"/path/to/file\")\nif my_file.is_file():\n    # file exists\n\nTo check a directory, do:\nif my_file.is_dir():\n    # directory exists\n\nTo check whether a Path object exists independently of whether is it a file or directory, use exists():\nif my_file.exists():\n    # pat",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 82831,
      "answer_id": 82852,
      "question_score": 7320,
      "answer_score": 6761,
      "is_accepted": false,
      "tags": [
        "python",
        "file",
        "file-exists"
      ]
    }
  },
  {
    "prompt": "How do I merge two dictionaries in a single expression in Python?\n\nI want to merge two dictionaries into a new dictionary.\nx = {'a': 1, 'b': 2}\ny = {'b': 3, 'c': 4}\nz = merge(x, y)\n\n>>> z\n{'a': 1, 'b': 3, 'c': 4}\n\nWhenever a key k is present in both dictionaries, only the value y[k] should be kept.",
    "response": "How can I merge two Python dictionaries in a single expression?\nFor dictionaries x and y, their shallowly-merged dictionary z takes values from y, replacing those from x.\n\nIn Python 3.9.0 or greater (released 17 October 2020, PEP-584, discussed here):\nz = x | y\n\n\nIn Python 3.5 or greater:\nz = {**x, **y}\n\n\nIn Python 2, (or 3.4 or lower) write a function:\ndef merge_two_dicts(x, y):\n    z = x.copy()   # start with keys and values of x\n    z.update(y)    # modifies z with keys and values of y\n    return z\n\nand now:\nz = merge_two_dicts(x, y)\n\n\n\nExplanation\nSay you have two dictionaries and you want to merge them into a new dictionary without altering the original dictionaries:\nx = {'a': 1, 'b': 2}\ny = {'b': 3, 'c': 4}\n\nThe desired result is to get a new dictionary (z) with the values merged, and the second dictionary's values overwriting those from the first.\n>>> z\n{'a': 1, 'b': 3, 'c': 4}\n\nA new syntax for this, proposed in PEP 448 and available as of Python 3.5, is\nz = {**x, **y}\n\nAnd it ",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 38987,
      "answer_id": 26853961,
      "question_score": 7112,
      "answer_score": 9375,
      "is_accepted": false,
      "tags": [
        "python",
        "dictionary",
        "merge"
      ]
    }
  },
  {
    "prompt": "How do I execute a program or call a system command?\n\nHow do I call an external command within Python as if I had typed it in a shell or command prompt?",
    "response": "Use subprocess.run:\nimport subprocess\n\nsubprocess.run([\"ls\", \"-l\"]) \n\nAnother common way is os.system but you shouldn't use it because it is unsafe if any parts of the command come from outside your program or can contain spaces or other special characters, also subprocess.run is generally more flexible (you can get the stdout, stderr, the \"real\" status code, better error handling, etc.). Even the documentation for os.system recommends using subprocess instead.\nOn Python 3.4 and earlier, use subprocess.call instead of .run:\nsubprocess.call([\"ls\", \"-l\"])",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 89228,
      "answer_id": 89243,
      "question_score": 6274,
      "answer_score": 5950,
      "is_accepted": true,
      "tags": [
        "python",
        "shell",
        "terminal",
        "subprocess",
        "command"
      ]
    }
  },
  {
    "prompt": "How do I create a directory, and any missing parent directories?\n\nHow do I create a directory at a given path, and also create any missing parent directories along that path? For example, the Bash command mkdir -p /path/to/nested/directory does this.",
    "response": "On Python ≥ 3.5, use pathlib.Path.mkdir:\n\nfrom pathlib import Path\nPath(\"/my/directory\").mkdir(parents=True, exist_ok=True)\n\n\nFor older versions of Python, I see two answers with good qualities, each with a small flaw, so I will give my take on it:\n\nTry os.path.exists, and consider os.makedirs for the creation.\n\nimport os\nif not os.path.exists(directory):\n    os.makedirs(directory)\n\n\nAs noted in comments and elsewhere, there's a race condition – if the directory is created between the os.path.exists and the os.makedirs calls, the os.makedirs will fail with an OSError. Unfortunately, blanket-catching OSError and continuing is not foolproof, as it will ignore a failure to create the directory due to other factors, such as insufficient permissions, full disk, etc.\n\nOne option would be to trap the OSError and examine the embedded error code (see Is there a cross-platform way of getting information from Python’s OSError):\n\nimport os, errno\n\ntry:\n    os.makedirs(directory)\nexcept OSError as ",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 273192,
      "answer_id": 273227,
      "question_score": 5802,
      "answer_score": 7276,
      "is_accepted": true,
      "tags": [
        "python",
        "exception",
        "path",
        "directory",
        "operating-system"
      ]
    }
  },
  {
    "prompt": "How can I access the index value in a &#39;for&#39; loop?\n\nHow do I access the index while iterating over a sequence with a for loop?\nxs = [8, 23, 45]\n\nfor x in xs:\n    print(\"item #{} = {}\".format(index, x))\n\nDesired output:\nitem #1 = 8\nitem #2 = 23\nitem #3 = 45",
    "response": "Use the built-in function enumerate():\nfor idx, x in enumerate(xs):\n    print(idx, x)\n\nIt is non-Pythonic to manually index via for i in range(len(xs)): x = xs[i] or manually manage an additional state variable.\nCheck out PEP 279 for more.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 522563,
      "answer_id": 522578,
      "question_score": 5654,
      "answer_score": 9225,
      "is_accepted": true,
      "tags": [
        "python",
        "loops",
        "list"
      ]
    }
  },
  {
    "prompt": "How do I make a flat list out of a list of lists?\n\nI have a list of lists like\n[\n    [1, 2, 3],\n    [4, 5, 6],\n    [7],\n    [8, 9]\n]\n\nHow can I flatten it to get [1, 2, 3, 4, 5, 6, 7, 8, 9]?\n\nIf your list of lists comes from a nested list comprehension, the problem can be solved more simply/directly by fixing the comprehension; please see How can I get a flat result from a list comprehension instead of a nested list?.\nThe most popular solutions here generally only flatten one \"level\" of the nested list. See Flatten an irregular (arbitrarily nest",
    "response": "A list of lists named xss can be flattened using a nested list comprehension:\nflat_list = [\n    x\n    for xs in xss\n    for x in xs\n]\n\nThe above is equivalent to:\nflat_list = []\n\nfor xs in xss:\n    for x in xs:\n        flat_list.append(x)\n\nHere is the corresponding function:\ndef flatten(xss):\n    return [x for xs in xss for x in xs]\n\nThis is the fastest method.\nAs evidence, using the timeit module in the standard library, we see:\n$ python -mtimeit -s'xss=[[1,2,3],[4,5,6],[7],[8,9]]*99' '[x for xs in xss for x in xs]'\n10000 loops, best of 3: 143 usec per loop\n\n$ python -mtimeit -s'xss=[[1,2,3],[4,5,6],[7],[8,9]]*99' 'sum(xss, [])'\n1000 loops, best of 3: 969 usec per loop\n\n$ python -mtimeit -s'xss=[[1,2,3],[4,5,6],[7],[8,9]]*99' 'reduce(lambda xs, ys: xs + ys, xss)'\n1000 loops, best of 3: 1.1 msec per loop\n\nExplanation: the methods based on + (including the implied use in sum) are, of necessity, O(L**2) when there are L sublists -- as the intermediate result list keeps getting longer, at",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 952914,
      "answer_id": 952952,
      "question_score": 5483,
      "answer_score": 7475,
      "is_accepted": true,
      "tags": [
        "python",
        "list",
        "multidimensional-array",
        "flatten"
      ]
    }
  },
  {
    "prompt": "What is the difference between @staticmethod and @classmethod in Python?\n\nWhat is the difference between a method decorated with @staticmethod and one decorated with @classmethod?",
    "response": "Maybe a bit of example code will help: Notice the difference in the call signatures of foo, class_foo and static_foo:\nclass A(object):\n    def foo(self, x):\n        print(f\"executing foo({self}, {x})\")\n\n    @classmethod\n    def class_foo(cls, x):\n        print(f\"executing class_foo({cls}, {x})\")\n\n    @staticmethod\n    def static_foo(x):\n        print(f\"executing static_foo({x})\")\n\na = A()\n\nBelow is the usual way an object instance calls a method. The object instance, a, is implicitly passed as the first argument.\na.foo(1)\n# executing foo(<__main__.A object at 0xb7dbef0c>, 1)\n\n\nWith classmethods, the class of the object instance is implicitly passed as the first argument instead of self.\na.class_foo(1)\n# executing class_foo(<class '__main__.A'>, 1)\n\nYou can also call class_foo using the class. In fact, if you define something to be\na classmethod, it is probably because you intend to call it from the class rather than from a class instance. A.foo(1) would have raised a TypeError, but A.c",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 136097,
      "answer_id": 1669524,
      "question_score": 4775,
      "answer_score": 4006,
      "is_accepted": true,
      "tags": [
        "python",
        "oop",
        "static-methods",
        "python-decorators",
        "class-method"
      ]
    }
  },
  {
    "prompt": "How slicing in Python works\n\nHow does Python's slice notation work? That is: when I write code like a[x:y:z], a[:], a[::2] etc., how can I understand which elements end up in the slice?\n\nSee Why are slice and range upper-bound exclusive? to learn why xs[0:2] == [xs[0], xs[1]], not [..., xs[2]].\nSee Make a new list containing every Nth item in the original list for xs[::N].\nSee How does assignment work with list slices? to learn what xs[0:2] = [\"a\", \"b\"] does.",
    "response": "The syntax is:\na[start:stop]  # items start through stop-1\na[start:]      # items start through the rest of the array\na[:stop]       # items from the beginning through stop-1\na[:]           # a copy of the whole array\n\nThere is also the step value, which can be used with any of the above:\na[start:stop:step] # start through not past stop, by step\n\nThe key point to remember is that the :stop value represents the first value that is not in the selected slice. So, the difference between stop and start is the number of elements selected (if step is 1, the default).\nThe other feature is that start or stop may be a negative number, which means it counts from the end of the array instead of the beginning. So:\na[-1]    # last item in the array\na[-2:]   # last two items in the array\na[:-2]   # everything except the last two items\n\nSimilarly, step may be a negative number:\na[::-1]    # all items in the array, reversed\na[1::-1]   # the first two items, reversed\na[:-3:-1]  # the last two items, rev",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 509211,
      "answer_id": 509295,
      "question_score": 4693,
      "answer_score": 6645,
      "is_accepted": true,
      "tags": [
        "python",
        "slice",
        "sequence"
      ]
    }
  },
  {
    "prompt": "How can I find the index for a given item in a list?\n\nGiven a list [\"foo\", \"bar\", \"baz\"] and an item in the list \"bar\", how do I get its index 1?",
    "response": ">>> [\"foo\", \"bar\", \"baz\"].index(\"bar\")\n1\n\nSee the documentation for the built-in .index() method of the list:\n\nlist.index(x[, start[, end]])\n\nReturn zero-based index in the list of the first item whose value is equal to x. Raises a ValueError if there is no such item.\nThe optional arguments start and end are interpreted as in the slice notation and are used to limit the search to a particular subsequence of the list. The returned index is computed relative to the beginning of the full sequence rather than the start argument.\n\nCaveats\nLinear time-complexity in list length\nAn index call checks every element of the list in order, until it finds a match. If the list is long, and if there is no guarantee that the value will be near the beginning, this can slow down the code.\nThis problem can only be completely avoided by using a different data structure. However, if the element is known to be within a certain part of the list, the start and end parameters can be used to narrow the search.\nF",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 176918,
      "answer_id": 176921,
      "question_score": 4481,
      "answer_score": 6098,
      "is_accepted": true,
      "tags": [
        "python",
        "list",
        "indexing"
      ]
    }
  },
  {
    "prompt": "Iterating over dictionaries using &#39;for&#39; loops\n\nd = {'x': 1, 'y': 2, 'z': 3}\n\nfor key in d:\n    print(key, 'corresponds to', d[key])\n\nHow does Python recognize that it needs only to read the key from the dictionary? Is key a special keyword, or is it simply a variable?",
    "response": "key is just a variable name.  \n\nfor key in d:\n\n\nwill simply loop over the keys in the dictionary, rather than the keys and values.  To loop over both key and value you can use the following:\n\nFor Python 3.x:\n\nfor key, value in d.items():\n\n\nFor Python 2.x:\n\nfor key, value in d.iteritems():\n\n\nTo test for yourself, change the word key to poop.\n\nIn Python 3.x, iteritems() was replaced with simply items(), which returns a set-like view backed by the dict, like iteritems() but even better. \nThis is also available in 2.7 as viewitems(). \n\nThe operation items() will work for both 2 and 3, but in 2 it will return a list of the dictionary's (key, value) pairs, which will not reflect changes to the dict that happen after the items() call. If you want the 2.x behavior in 3.x, you can call list(d.items()).",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 3294889,
      "answer_id": 3294899,
      "question_score": 4418,
      "answer_score": 7014,
      "is_accepted": true,
      "tags": [
        "python",
        "loops",
        "dictionary",
        "key"
      ]
    }
  },
  {
    "prompt": "How can I iterate over rows in a Pandas DataFrame?\n\nI have a pandas dataframe, df:\n   c1   c2\n0  10  100\n1  11  110\n2  12  120\n\nHow do I iterate over the rows of this dataframe? For every row, I want to access its elements (values in cells) by the name of the columns. For example:\nfor row in df.rows:\n    print(row['c1'], row['c2'])\n\n\nI found a similar question, which suggests using either of these:\n\n\nfor date, row in df.T.iteritems():\n\n\n\nfor row in df.iterrows():\n\n\n\nBut I do not understand what the row object is and how I can work with it.",
    "response": "DataFrame.iterrows is a generator which yields both the index and row (as a Series):\nimport pandas as pd\n\ndf = pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 120]})\ndf = df.reset_index()  # make sure indexes pair with number of rows\n\nfor index, row in df.iterrows():\n    print(row['c1'], row['c2'])\n\n\n10 100\n11 110\n12 120\n\n\nObligatory disclaimer from the documentation\n\nIterating through pandas objects is generally slow. In many cases, iterating manually over the rows is not needed and can be avoided with one of the following approaches:\n\nLook for a vectorized solution: many operations can be performed using built-in methods or NumPy functions, (boolean) indexing, …\nWhen you have a function that cannot work on the full DataFrame/Series at once, it is better to use apply() instead of iterating over the values. See the docs on function application.\nIf you need to do iterative manipulations on the values but performance is important, consider writing the inner loop with cython or numba. ",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 16476924,
      "answer_id": 16476974,
      "question_score": 4238,
      "answer_score": 5504,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "loops"
      ]
    }
  },
  {
    "prompt": "How can I use a global variable in a function?\n\nHow do I create or use a global variable inside a function?\nHow do I use a global variable that was defined in one function inside other functions?\n\nFailing to use the global keyword where appropriate often causes UnboundLocalError. The precise rules for this are explained at UnboundLocalError on local variable when reassigned after first use. Generally, please close other questions as a duplicate of that question when an explanation is sought, and this question when someone simply needs to know",
    "response": "You can use a global variable within other functions by declaring it as global within each function that assigns a value to it:\nglobvar = 0\n\ndef set_globvar_to_one():\n    global globvar    # Needed to modify global copy of globvar\n    globvar = 1\n\ndef print_globvar():\n    print(globvar)     # No need for global declaration to read value of globvar\n\nset_globvar_to_one()\nprint_globvar()       # Prints 1\n\nSince it's unclear whether globvar = 1 is creating a local variable or changing a global variable, Python defaults to creating a local variable, and makes you explicitly choose the other behavior with the global keyword.\nSee other answers if you want to share a global variable across modules.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 423379,
      "answer_id": 423596,
      "question_score": 4041,
      "answer_score": 5275,
      "is_accepted": true,
      "tags": [
        "python",
        "global-variables",
        "scope"
      ]
    }
  },
  {
    "prompt": "How do I get the current time in Python?\n\nHow do I get the current time in Python?",
    "response": "Use datetime:\n>>> import datetime\n>>> now = datetime.datetime.now()\n>>> now\ndatetime.datetime(2009, 1, 6, 15, 8, 24, 78915)\n>>> print(now)\n2009-01-06 15:08:24.789150\n\nFor just the clock time without the date:\n>>> now.time()\ndatetime.time(15, 8, 24, 78915)\n>>> print(now.time())\n15:08:24.789150\n\n\nTo save typing, you can import the datetime object from the datetime module:\n>>> from datetime import datetime\n\nThen remove the prefix datetime. from all of the above.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 415511,
      "answer_id": 415519,
      "question_score": 3981,
      "answer_score": 4023,
      "is_accepted": true,
      "tags": [
        "python",
        "datetime",
        "time"
      ]
    }
  },
  {
    "prompt": "How can I catch multiple exceptions in one line? (in the &quot;except&quot; block)\n\nI know that I can do:\ntry:\n    # do something that may fail\nexcept:\n    # do this if ANYTHING goes wrong\n\nI can also do this:\ntry:\n    # do something that may fail\nexcept IDontLikeYouException:\n    # say please\nexcept YouAreTooShortException:\n    # stand on a ladder\n\nBut if I want to do the same thing inside two different exceptions, the best I can think of right now is to do this:\ntry:\n    # do something that may fail\nexcept IDontLikeYouException:\n    # say please\nexcept YouAreBeingMeanExceptio",
    "response": "From Python Documentation:\n\n\n  An except clause may name multiple exceptions as a parenthesized tuple, for example\n\n\nexcept (IDontLikeYouException, YouAreBeingMeanException) as e:\n    pass\n\n\nOr, for Python 2 only:\n\nexcept (IDontLikeYouException, YouAreBeingMeanException), e:\n    pass\n\n\nSeparating the exception from the variable with a comma will still work in Python 2.6 and 2.7, but is now deprecated and does not work in Python 3; now you should be using as.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 6470428,
      "answer_id": 6470452,
      "question_score": 3951,
      "answer_score": 5260,
      "is_accepted": true,
      "tags": [
        "python",
        "exception"
      ]
    }
  },
  {
    "prompt": "How do I copy a file?\n\nHow do I copy a file in Python?",
    "response": "shutil has many methods you can use. One of which is:\nimport shutil\n\nshutil.copyfile(src, dst)\n\n# 2nd option\nshutil.copy(src, dst)  # dst can be a folder; use shutil.copy2() to preserve timestamp\n\n\nCopy the contents of the file named src to a file named dst. Both src and dst need to be the entire filename of the files, including path.\nThe destination location must be writable; otherwise, an IOError exception will be raised.\nIf dst already exists, it will be replaced.\nSpecial files such as character or block devices and pipes cannot be copied with this function.\nWith copy, src and dst are path names given as strs.\n\nAnother shutil method to look at is shutil.copy2(). It's similar but preserves more metadata (e.g. time stamps).\nIf you use os.path operations, use copy rather than copyfile. copyfile will only accept strings.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 123198,
      "answer_id": 123212,
      "question_score": 3939,
      "answer_score": 4851,
      "is_accepted": true,
      "tags": [
        "python",
        "file",
        "copy",
        "filesystems",
        "file-copying"
      ]
    }
  },
  {
    "prompt": "What is __init__.py for?\n\nWhat is __init__.py for in a Python source directory?",
    "response": "It used to be a required part of a package (old, pre-3.3 \"regular package\", not newer 3.3+ \"namespace package\").\n\nHere's the documentation.\n\n\n  Python defines two types of packages, regular packages and namespace packages. Regular packages are traditional packages as they existed in Python 3.2 and earlier. A regular package is typically implemented as a directory containing an __init__.py file. When a regular package is imported, this __init__.py file is implicitly executed, and the objects it defines are bound to names in the package’s namespace. The __init__.py file can contain the same Python code that any other module can contain, and Python will add some additional attributes to the module when it is imported.\n\n\nBut just click the link, it contains an example, more information, and an explanation of namespace packages, the kind of packages without __init__.py.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 448271,
      "answer_id": 448279,
      "question_score": 3907,
      "answer_score": 2095,
      "is_accepted": true,
      "tags": [
        "python",
        "module",
        "package",
        "python-packaging"
      ]
    }
  },
  {
    "prompt": "Convert bytes to a string in Python 3\n\nI captured the standard output of an external program into a bytes object:\n>>> from subprocess import *\n>>> stdout = Popen(['ls', '-l'], stdout=PIPE).communicate()[0]\n>>> stdout\nb'total 0\\n-rw-rw-r-- 1 thomas thomas 0 Mar  3 07:03 file1\\n-rw-rw-r-- 1 thomas thomas 0 Mar  3 07:03 file2\\n'\n\nI want to convert that to a normal Python string, so that I can print it like this:\n>>> print(stdout)\n-rw-rw-r-- 1 thomas thomas 0 Mar  3 07:03 file1\n-rw-rw-r-- 1 thomas thomas 0 Mar  3 07:03 file2\n\nHow do I co",
    "response": "Decode the bytes object to produce a string:\n>>> b\"abcde\".decode(\"utf-8\")\n'abcde'\n\nThe above example assumes that the bytes object is in UTF-8, because it is a common encoding. However, you should use the encoding your data is actually in!",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 606191,
      "answer_id": 606199,
      "question_score": 3877,
      "answer_score": 5859,
      "is_accepted": true,
      "tags": [
        "python",
        "string",
        "python-3.x"
      ]
    }
  },
  {
    "prompt": "What is the difference between __str__ and __repr__?\n\nWhat is the difference between __str__ and __repr__ in Python?",
    "response": "Alex Martelli summarized well but, surprisingly, was too succinct.\nFirst, let me reiterate the main points in Alex’s post:\n\nThe default implementation is useless (it’s hard to think of one which wouldn’t be, but yeah)\n__repr__ goal is to be unambiguous\n__str__ goal is to be readable\nContainer’s __str__ uses contained objects’ __repr__\n\nDefault implementation is useless\nThis is mostly a surprise because Python’s defaults tend to be fairly useful. However, in this case, having a default for __repr__ which would act like:\nreturn \"%s(%r)\" % (self.__class__, self.__dict__)\n\nOr in new f-string formatting:\nreturn f\"{self.__class__!s}({self.__dict__!r})\"\n\nwould have been too dangerous (for example, too easy to get into infinite recursion if objects reference each other). So Python cops out. Note that there is one default which is true: if __repr__ is defined, and __str__ is not, the object will behave as though __str__=__repr__.\nThis means, in simple terms: almost every object you implement sh",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 1436703,
      "answer_id": 2626364,
      "question_score": 3818,
      "answer_score": 3599,
      "is_accepted": true,
      "tags": [
        "python",
        "magic-methods",
        "repr"
      ]
    }
  },
  {
    "prompt": "How do I select rows from a DataFrame based on column values?\n\nHow can I select rows from a DataFrame based on values in some column in Pandas?\nIn SQL, I would use:\nSELECT *\nFROM table\nWHERE column_name = some_value",
    "response": "To select rows whose column value equals a scalar, some_value, use ==:\ndf.loc[df['column_name'] == some_value]\n\nTo select rows whose column value is in an iterable, some_values, use isin:\ndf.loc[df['column_name'].isin(some_values)]\n\nCombine multiple conditions with &:\ndf.loc[(df['column_name'] >= A) & (df['column_name'] <= B)]\n\nNote the parentheses. Due to Python's operator precedence rules, & binds more tightly than <= and >=. Thus, the parentheses in the last example are necessary. Without the parentheses\ndf['column_name'] >= A & df['column_name'] <= B\n\nis parsed as\ndf['column_name'] >= (A & df['column_name']) <= B\n\nwhich results in a Truth value of a Series is ambiguous error.\n\nTo select rows whose column value does not equal some_value, use !=:\ndf.loc[df['column_name'] != some_value]\n\nThe isin returns a boolean Series, so to select rows whose value is not in some_values, negate the boolean Series using ~:\ndf = df.loc[~df['column_name'].isin(some_values)] # .loc is not in-place repl",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 17071871,
      "answer_id": 17071908,
      "question_score": 3625,
      "answer_score": 6625,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "indexing",
        "filter"
      ]
    }
  },
  {
    "prompt": "How can I add new keys to a dictionary?\n\nHow do I add a new key to an existing dictionary? It doesn't have an .add() method.",
    "response": "You create a new key/value pair on a dictionary by assigning a value to that key\nd = {'key': 'value'}\nprint(d)  # {'key': 'value'}\n\nd['mynewkey'] = 'mynewvalue'\n\nprint(d)  # {'key': 'value', 'mynewkey': 'mynewvalue'}\n\nIf the key doesn't exist, it's added and points to that value. If it exists, the current value it points to is overwritten.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 1024847,
      "answer_id": 1024851,
      "question_score": 3598,
      "answer_score": 4450,
      "is_accepted": true,
      "tags": [
        "python",
        "dictionary",
        "key",
        "lookup"
      ]
    }
  },
  {
    "prompt": "Does Python have a string &#39;contains&#39; substring method?\n\nI'm looking for a string.contains or string.indexof method in Python.\n\nI want to do:\n\nif not somestring.contains(\"blah\"):\n   continue",
    "response": "Use the in operator:\nif \"blah\" not in somestring: \n    continue\n\nNote: This is case-sensitive.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 3437059,
      "answer_id": 3437070,
      "question_score": 3586,
      "answer_score": 8649,
      "is_accepted": true,
      "tags": [
        "python",
        "string",
        "substring",
        "contains"
      ]
    }
  },
  {
    "prompt": "How can I delete a file or folder in Python?\n\nHow can I delete a file or folder in Python?",
    "response": "Use one of these methods:\n\npathlib.Path.unlink() removes a file or symbolic link.\n\npathlib.Path.rmdir() removes an empty directory.\n\nshutil.rmtree() deletes a directory and all its contents.\n\n\n\nOn Python 3.3 and below, you can use these methods instead of the pathlib ones:\n\nos.remove() removes a file.\n\nos.unlink() removes a symbolic link.\n\nos.rmdir() removes an empty directory.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 6996603,
      "answer_id": 6996628,
      "question_score": 3532,
      "answer_score": 5029,
      "is_accepted": true,
      "tags": [
        "python",
        "file-io",
        "directory",
        "delete-file"
      ]
    }
  },
  {
    "prompt": "&quot;Least Astonishment&quot; and the Mutable Default Argument\n\ndef foo(a=[]):\n    a.append(5)\n    return a\n\nPython novices expect this function called with no parameter to always return a list with only one element: [5]. The result is  different and astonishing:\n>>> foo()\n[5]\n>>> foo()\n[5, 5]\n>>> foo()\n[5, 5, 5]\n>>> foo()\n[5, 5, 5, 5]\n>>> foo()\n\nThe behavior has an underlying explanation, but it is unexpected if you don't understand internals. What is the reason for binding the default argument at function definition, and not at function execution? I doubt ",
    "response": "Actually, this is not a design flaw, and it is not because of internals or performance. It comes simply from the fact that functions in Python are first-class objects, and not only a piece of code.\nAs soon as you think of it this way, then it completely makes sense: a function is an object being evaluated on its definition; default parameters are kind of \"member data\" and therefore their state may change from one call to the other - exactly as in any other object.\nIn any case, the Effbot (Fredrik Lundh) has a very nice explanation of the reasons for this behavior in Default Parameter Values in Python. I found it very clear, and I really suggest reading it for a better knowledge of how function objects work.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 1132941,
      "answer_id": 1145781,
      "question_score": 3503,
      "answer_score": 1957,
      "is_accepted": true,
      "tags": [
        "python",
        "language-design",
        "default-parameters",
        "least-astonishment"
      ]
    }
  },
  {
    "prompt": "What does ** (double star/asterisk) and * (star/asterisk) do for parameters?\n\nWhat do *args and **kwargs mean in these function definitions?\ndef foo(x, y, *args):\n    pass\n\ndef bar(x, y, **kwargs):\n    pass\n\n\nSee What do ** (double star/asterisk) and * (star/asterisk) mean in a function call? for the complementary question about arguments.",
    "response": "The *args and **kwargs are common idioms to allow an arbitrary number of arguments to functions, as described in the section more on defining functions in the Python tutorial.\nThe *args will give you all positional arguments as a tuple:\ndef foo(*args):\n    for a in args:\n        print(a)        \n\nfoo(1)\n# 1\n\nfoo(1, 2, 3)\n# 1\n# 2\n# 3\n\nThe **kwargs will give you all\nkeyword arguments as a dictionary:\ndef bar(**kwargs):\n    for a in kwargs:\n        print(a, kwargs[a])  \n\nbar(name='one', age=27)\n# name one\n# age 27\n\nBoth idioms can be mixed with normal arguments to allow a set of fixed and some variable arguments:\ndef foo(kind, *args, bar=None, **kwargs):\n    print(kind, args, bar, kwargs)\n\nfoo(123, 'a', 'b', apple='red')\n# 123 ('a', 'b') None {'apple': 'red'}\n\nIt is also possible to use this the other way around:\ndef foo(a, b, c):\n    print(a, b, c)\n\nobj = {'b':10, 'c':'lee'}\n\nfoo(100, **obj)\n# 100 10 lee\n\nAnother usage of the *l idiom is to unpack argument lists when calling a function.\n",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 36901,
      "answer_id": 36908,
      "question_score": 3486,
      "answer_score": 3332,
      "is_accepted": true,
      "tags": [
        "python",
        "syntax",
        "parameter-passing",
        "variadic-functions",
        "argument-unpacking"
      ]
    }
  },
  {
    "prompt": "How do I list all files of a directory?\n\nHow can I list all files of a directory in Python and add them to a list?",
    "response": "os.listdir() returns everything inside a directory -- including both files and directories.\nos.path's isfile() can be used to only list files:\nfrom os import listdir\nfrom os.path import isfile, join\nonlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n\nAlternatively, os.walk() yields two lists for each directory it visits -- one for files and one for dirs. If you only want the top directory you can break the first time it yields:\nfrom os import walk\n\nf = []\nfor (dirpath, dirnames, filenames) in walk(mypath):\n    f.extend(filenames)\n    break\n\nor, shorter:\nfrom os import walk\n\nfilenames = next(walk(mypath), (None, None, []))[2]  # [] if no file",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 3207219,
      "answer_id": 3207973,
      "question_score": 3464,
      "answer_score": 6581,
      "is_accepted": false,
      "tags": [
        "python",
        "directory"
      ]
    }
  },
  {
    "prompt": "How can I access environment variables in Python?\n\nHow can I get the value of an environment variable in Python?",
    "response": "Environment variables are accessed through os.environ:\nimport os\nprint(os.environ['HOME'])\n\nTo see a list of all environment variables:\nprint(os.environ)\n\n\nIf a key is not present, attempting to access it will raise a KeyError. To avoid this:\n# Returns `None` if the key doesn't exist\nprint(os.environ.get('KEY_THAT_MIGHT_EXIST'))\n\n# Returns `default_value` if the key doesn't exist\nprint(os.environ.get('KEY_THAT_MIGHT_EXIST', default_value))\n\n# Returns `default_value` if the key doesn't exist\nprint(os.getenv('KEY_THAT_MIGHT_EXIST', default_value))",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 4906977,
      "answer_id": 4907053,
      "question_score": 3452,
      "answer_score": 4833,
      "is_accepted": true,
      "tags": [
        "python",
        "environment-variables"
      ]
    }
  },
  {
    "prompt": "How do I sort a dictionary by value?\n\nI have a dictionary of values read from two fields in a database: a string field and a numeric field. The string field is unique, so that is the key of the dictionary.\n\nI can sort on the keys, but how can I sort based on the values?\n\nNote: I have read Stack Overflow question here How do I sort a list of dictionaries by a value of the dictionary? and probably could change my code to have a list of dictionaries, but since I do not really need a list of dictionaries I wanted to know if there is a s",
    "response": "Python 3.7+ or CPython 3.6\nDicts preserve insertion order in Python 3.7+. Same in CPython 3.6, but it's an implementation detail.\n>>> x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\n>>> {k: v for k, v in sorted(x.items(), key=lambda item: item[1])}\n{0: 0, 2: 1, 1: 2, 4: 3, 3: 4}\n\nor\n>>> dict(sorted(x.items(), key=lambda item: item[1]))\n{0: 0, 2: 1, 1: 2, 4: 3, 3: 4}\n\nOlder Python\nIt is not possible to sort a dictionary, only to get a representation of a dictionary that is sorted. Dictionaries are inherently orderless, but other types, such as lists and tuples, are not. So you need an ordered data type to represent sorted values, which will be a list—probably a list of tuples.\nFor instance,\nimport operator\nx = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\nsorted_x = sorted(x.items(), key=operator.itemgetter(1))\n\nsorted_x will be a list of tuples sorted by the second element in each tuple. dict(sorted_x) == x.\nAnd for those wishing to sort on keys instead of values:\nimport operator\nx = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 613183,
      "answer_id": 613218,
      "question_score": 3413,
      "answer_score": 7072,
      "is_accepted": true,
      "tags": [
        "python",
        "sorting",
        "dictionary"
      ]
    }
  },
  {
    "prompt": "How do I clone a list so that it doesn&#39;t change unexpectedly after assignment?\n\nWhile using new_list = my_list, any modifications to new_list changes my_list every time. Why is this, and how can I clone or copy the list to prevent it? For example:\n>>> my_list = [1, 2, 3]\n>>> new_list = my_list\n>>> new_list.append(4)\n>>> my_list\n[1, 2, 3, 4]",
    "response": "new_list = my_list doesn't actually create a second list. The assignment just copies the reference to the list, not the actual list, so both new_list and my_list refer to the same list after the assignment.\nTo actually copy the list, you have several options:\n\nYou can use the built-in list.copy() method (available since Python 3.3):\nnew_list = old_list.copy()\n\n\nYou can slice it:\nnew_list = old_list[:]\n\nAlex Martelli's opinion (at least back in 2007) about this is, that it is a weird syntax and it does not make sense to use it ever. ;) (In his opinion, the next one is more readable).\n\nYou can use the built-in list() constructor:\nnew_list = list(old_list)\n\n\nYou can use generic copy.copy():\nimport copy\nnew_list = copy.copy(old_list)\n\nThis is a little slower than list() because it has to find out the datatype of old_list first.\n\nIf you need to copy the elements of the list as well, use generic copy.deepcopy():\nimport copy\nnew_list = copy.deepcopy(old_list)\n\nObviously the slowest and most m",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 2612802,
      "answer_id": 2612815,
      "question_score": 3365,
      "answer_score": 4145,
      "is_accepted": true,
      "tags": [
        "python",
        "list",
        "clone",
        "mutable"
      ]
    }
  },
  {
    "prompt": "How do I pass a variable by reference?\n\nI wrote this class for testing:\nclass PassByReference:\n    def __init__(self):\n        self.variable = 'Original'\n        self.change(self.variable)\n        print(self.variable)\n\n    def change(self, var):\n        var = 'Changed'\n\nWhen I tried creating an instance, the output was Original. So it seems like parameters in Python are passed by value. Is that correct? How can I modify the code to get the effect of pass-by-reference, so that the output is Changed?\n\nSometimes people are surprised that",
    "response": "Arguments are passed by assignment. The rationale behind this is twofold:\n\n\nthe parameter passed in is actually a reference to an object (but the reference is passed by value)\nsome data types are mutable, but others aren't\n\n\nSo:\n\n\nIf you pass a mutable object into a method, the method gets a reference to that same object and you can mutate it to your heart's delight, but if you rebind the reference in the method, the outer scope will know nothing about it, and after you're done, the outer reference will still point at the original object. \nIf you pass an immutable object to a method, you still can't rebind the outer reference, and you can't even mutate the object.\n\n\nTo make it even more clear, let's have some examples. \n\nList - a mutable type\n\nLet's try to modify the list that was passed to a method:\n\ndef try_to_change_list_contents(the_list):\n    print('got', the_list)\n    the_list.append('four')\n    print('changed to', the_list)\n\nouter_list = ['one', 'two', 'three']\n\nprint('before, o",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 986006,
      "answer_id": 986145,
      "question_score": 3359,
      "answer_score": 3591,
      "is_accepted": true,
      "tags": [
        "python",
        "reference",
        "parameter-passing",
        "pass-by-reference"
      ]
    }
  },
  {
    "prompt": "How do I print colored text to the terminal?\n\nHow do I output colored text to the terminal in Python?",
    "response": "This somewhat depends on what platform you are on. The most common way to do this is by printing ANSI escape sequences. For a simple example, here's some Python code from the Blender build scripts:\nclass bcolors:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKCYAN = '\\033[96m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n\nTo use code like this, you can do something like:\nprint(bcolors.WARNING + \"Warning: No active frommets remain. Continue?\" + bcolors.ENDC)\n\nOr, with Python 3.6+:\nprint(f\"{bcolors.WARNING}Warning: No active frommets remain. Continue?{bcolors.ENDC}\")\n\nThis will work on unixes including OS X, Linux and Windows (provided you use ANSICON, or in Windows 10 provided you enable VT100 emulation). There are ANSI codes for setting the color, moving the cursor, and more.\nIf you are going to get complicated with this (and it sounds like you are if you are writing a game), you shoul",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 287871,
      "answer_id": 287944,
      "question_score": 3316,
      "answer_score": 3020,
      "is_accepted": false,
      "tags": [
        "python",
        "terminal",
        "output",
        "ansi-colors"
      ]
    }
  },
  {
    "prompt": "Manually raising (throwing) an exception in Python\n\nHow do I raise an exception in Python so that it can later be caught via an except block?",
    "response": "How do I manually throw/raise an exception in Python?\n\nUse the most specific Exception constructor that semantically fits your issue.\nBe specific in your message, e.g.:\nraise ValueError('A very specific bad thing happened.')\n\nDon't raise generic exceptions\nAvoid raising a generic Exception. To catch it, you'll have to catch all other more specific exceptions that subclass it.\nProblem 1: Hiding bugs\nraise Exception('I know Python!') # Don't! If you catch, likely to hide bugs.\n\nFor example:\ndef demo_bad_catch():\n    try:\n        raise ValueError('Represents a hidden bug, do not catch this')\n        raise Exception('This is the exception you expect to handle')\n    except Exception as error:\n        print('Caught this error: ' + repr(error))\n\n>>> demo_bad_catch()\nCaught this error: ValueError('Represents a hidden bug, do not catch this',)\n\nProblem 2: Won't catch\nAnd more specific catches won't catch the general exception:\ndef demo_no_catch():\n    try:\n        raise Exception('general excep",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 2052390,
      "answer_id": 24065533,
      "question_score": 3313,
      "answer_score": 4315,
      "is_accepted": true,
      "tags": [
        "python",
        "exception"
      ]
    }
  },
  {
    "prompt": "Understanding Python super() with __init__() methods\n\nWhy is super() used?\nIs there a difference between using Base.__init__ and super().__init__?\nclass Base(object):\n    def __init__(self):\n        print \"Base created\"\n        \nclass ChildA(Base):\n    def __init__(self):\n        Base.__init__(self)\n        \nclass ChildB(Base):\n    def __init__(self):\n        super(ChildB, self).__init__()\n        \nChildA() \nChildB()",
    "response": "super() lets you avoid referring to the base class explicitly, which can be nice. But the main advantage comes with multiple inheritance, where all sorts of fun stuff can happen. See the standard docs on super if you haven't already.\n\nNote that the syntax changed in Python 3.0: you can just say super().__init__() instead of super(ChildB, self).__init__() which IMO is quite a bit nicer. The standard docs also refer to a guide to using super() which is quite explanatory.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 576169,
      "answer_id": 576183,
      "question_score": 3291,
      "answer_score": 2371,
      "is_accepted": true,
      "tags": [
        "python",
        "class",
        "oop",
        "inheritance",
        "super"
      ]
    }
  },
  {
    "prompt": "How do I make a time delay?\n\nHow do I put a time delay in a Python script?",
    "response": "This delays for 2.5 seconds:\nimport time\n\ntime.sleep(2.5)\n\n\nHere is another example where something is run approximately once a minute:\nimport time\n\nwhile True:\n    print(\"This prints once a minute.\")\n    time.sleep(60) # Delay for 1 minute (60 seconds).",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 510348,
      "answer_id": 510351,
      "question_score": 3280,
      "answer_score": 3543,
      "is_accepted": false,
      "tags": [
        "python",
        "delay",
        "sleep",
        "timedelay"
      ]
    }
  },
  {
    "prompt": "How do I change the size of figures drawn with Matplotlib?\n\nHow do I change the size of figure drawn with Matplotlib?",
    "response": "figure tells you the call signature:\nfrom matplotlib.pyplot import figure\n\nfigure(figsize=(8, 6), dpi=80)\n\nfigure(figsize=(1,1)) would create an inch-by-inch image, which would be 80-by-80 pixels unless you also give a different dpi argument.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 332289,
      "answer_id": 638443,
      "question_score": 3263,
      "answer_score": 1931,
      "is_accepted": false,
      "tags": [
        "python",
        "pandas",
        "matplotlib",
        "seaborn",
        "figsize"
      ]
    }
  },
  {
    "prompt": "How do I concatenate two lists in Python?\n\nHow do I concatenate two lists in Python?\n\nExample:\n\nlistone = [1, 2, 3]\nlisttwo = [4, 5, 6]\n\n\nExpected outcome:\n\n>>> joinedlist\n[1, 2, 3, 4, 5, 6]",
    "response": "Use the + operator to combine the lists:\nlistone = [1, 2, 3]\nlisttwo = [4, 5, 6]\n\njoinedlist = listone + listtwo\n\nOutput:\n>>> joinedlist\n[1, 2, 3, 4, 5, 6]\n\nNOTE: This will create a new list with a shallow copy of the items in the first list, followed by a shallow copy of the items in the second list. Use copy.deepcopy() to get deep copies of lists.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 1720421,
      "answer_id": 1720432,
      "question_score": 3238,
      "answer_score": 5622,
      "is_accepted": true,
      "tags": [
        "python",
        "list",
        "concatenation"
      ]
    }
  },
  {
    "prompt": "How do I check if a list is empty?\n\nFor example, if passed the following:\n\na = []\n\n\nHow do I check to see if a is empty?",
    "response": "if not a:\n    print(\"List is empty\")\n\nUsing the implicit booleanness of the empty list is quite Pythonic.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 53513,
      "answer_id": 53522,
      "question_score": 3222,
      "answer_score": 7297,
      "is_accepted": true,
      "tags": [
        "python",
        "list"
      ]
    }
  },
  {
    "prompt": "How do I make function decorators and chain them together?\n\nHow do I make two decorators in Python that would do the following?\n@make_bold\n@make_italic\ndef say():\n   return \"Hello\"\n\nCalling say() should return:\n\"<b><i>Hello</i></b>\"",
    "response": "Check out the documentation to see how decorators work. Here is what you asked for:\nfrom functools import wraps\n\ndef makebold(fn):\n    @wraps(fn)\n    def wrapper(*args, **kwargs):\n        return \"<b>\" + fn(*args, **kwargs) + \"</b>\"\n    return wrapper\n\ndef makeitalic(fn):\n    @wraps(fn)\n    def wrapper(*args, **kwargs):\n        return \"<i>\" + fn(*args, **kwargs) + \"</i>\"\n    return wrapper\n\n@makebold\n@makeitalic\ndef hello():\n    return \"hello world\"\n\n@makebold\n@makeitalic\ndef log(s):\n    return s\n\nprint hello()        # returns \"<b><i>hello world</i></b>\"\nprint hello.__name__ # with functools.wraps() this returns \"hello\"\nprint log('hello')   # returns \"<b><i>hello</i></b>\"",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 739654,
      "answer_id": 739665,
      "question_score": 3197,
      "answer_score": 3102,
      "is_accepted": true,
      "tags": [
        "python",
        "function",
        "decorator",
        "python-decorators",
        "chain"
      ]
    }
  },
  {
    "prompt": "How do I split a list into equally-sized chunks?\n\nHow do I split a list of arbitrary length into equal sized chunks?\n\nSee also: How to iterate over a list in chunks.\nTo chunk strings, see Split string every nth character?.",
    "response": "Here's a generator that yields evenly-sized chunks:\ndef chunks(lst, n):\n    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n    for i in range(0, len(lst), n):\n        yield lst[i:i + n]\n\nimport pprint\npprint.pprint(list(chunks(range(10, 75), 10)))\n[[10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\n [70, 71, 72, 73, 74]]\n\nFor Python 2, using xrange instead of range:\ndef chunks(lst, n):\n    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n    for i in xrange(0, len(lst), n):\n        yield lst[i:i + n]\n\n\nBelow is a list comprehension one-liner. The method above is preferable, though, since using named functions makes code easier to understand. For Python 3:\n[lst[i:i + n] for i in range(0, len(lst), n)]\n\nFor Python 2:\n[lst[i:i + n] for i in xrange(0, len(lst), n)]",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 312443,
      "answer_id": 312464,
      "question_score": 3170,
      "answer_score": 4499,
      "is_accepted": true,
      "tags": [
        "python",
        "list",
        "split",
        "chunks"
      ]
    }
  },
  {
    "prompt": "Find the current directory and file&#39;s directory\n\nHow do I determine:\n\nthe current directory (where I was in the shell when I ran the Python script), and\nwhere the Python file I am executing is?",
    "response": "To get the full path to the directory a Python file is contained in, write this in that file:\n\nimport os \ndir_path = os.path.dirname(os.path.realpath(__file__))\n\n\n(Note that the incantation above won't work if you've already used os.chdir() to change your current working directory, since the value of the __file__ constant is relative to the current working directory and is not changed by an os.chdir() call.)\n\n\n\nTo get the current working directory use \n\nimport os\ncwd = os.getcwd()\n\n\n\n\nDocumentation references for the modules, constants and functions used above:\n\n\nThe os and os.path modules.\nThe __file__ constant\nos.path.realpath(path) (returns \"the canonical path of the specified filename, eliminating any symbolic links encountered in the path\")\nos.path.dirname(path) (returns \"the directory name of pathname path\")\nos.getcwd() (returns \"a string representing the current working directory\")\nos.chdir(path) (\"change the current working directory to path\")",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 5137497,
      "answer_id": 5137509,
      "question_score": 3128,
      "answer_score": 4827,
      "is_accepted": true,
      "tags": [
        "python",
        "directory"
      ]
    }
  },
  {
    "prompt": "What is the difference between Python&#39;s list methods append and extend?\n\nWhat's the difference between the list methods append() and extend()?",
    "response": ".append() appends a single object at the end of the list:\n>>> x = [1, 2, 3]\n>>> x.append([4, 5])\n>>> print(x)\n[1, 2, 3, [4, 5]]\n\n.extend() appends multiple objects that are taken from inside the specified iterable:\n>>> x = [1, 2, 3]\n>>> x.extend([4, 5])\n>>> print(x)\n[1, 2, 3, 4, 5]",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 252703,
      "answer_id": 252711,
      "question_score": 3109,
      "answer_score": 5922,
      "is_accepted": true,
      "tags": [
        "python",
        "list",
        "data-structures",
        "append",
        "extend"
      ]
    }
  },
  {
    "prompt": "Renaming column names in Pandas\n\nI want to change the column labels of a Pandas DataFrame from\n['$a', '$b', '$c', '$d', '$e']\n\nto\n['a', 'b', 'c', 'd', 'e']",
    "response": "Just assign it to the .columns attribute:\n>>> df = pd.DataFrame({'$a':[1,2], '$b': [10,20]})\n>>> df\n   $a  $b\n0   1  10\n1   2  20\n\n>>> df.columns = ['a', 'b']\n>>> df\n   a   b\n0  1  10\n1  2  20",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 11346283,
      "answer_id": 11346337,
      "question_score": 3068,
      "answer_score": 2571,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "replace",
        "dataframe",
        "rename"
      ]
    }
  },
  {
    "prompt": "Why is &quot;1000000000000000 in range(1000000000000001)&quot; so fast in Python 3?\n\nIt is my understanding that the range() function, which is actually an object type in Python 3, generates its contents on the fly, similar to a generator.\nThis being the case, I would have expected the following line to take an inordinate amount of time because, in order to determine whether 1 quadrillion is in the range, a quadrillion values would have to be generated:\n1_000_000_000_000_000 in range(1_000_000_000_000_001)\n\nFurthermore: it seems that no matter how many zeroes I add on, the calcu",
    "response": "The Python 3 range() object doesn't produce numbers immediately; it is a smart sequence object that produces numbers on demand. All it contains is your start, stop and step values, then as you iterate over the object the next integer is calculated each iteration.\nThe object also implements the object.__contains__ hook, and calculates if your number is part of its range. Calculating is a (near) constant time operation *. There is never a need to scan through all possible integers in the range.\nFrom the range() object documentation:\n\nThe advantage of the range type over a regular list or tuple is that a range object will always take the same (small) amount of memory, no matter the size of the range it represents (as it only stores the start, stop and step values, calculating individual items and subranges as needed).\n\nSo at a minimum, your range() object would do:\nclass my_range:\n    def __init__(self, start, stop=None, step=1, /):\n        if stop is None:\n            start, stop = 0, st",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 30081275,
      "answer_id": 30081318,
      "question_score": 3068,
      "answer_score": 3132,
      "is_accepted": true,
      "tags": [
        "python",
        "performance",
        "python-3.x",
        "range",
        "python-internals"
      ]
    }
  },
  {
    "prompt": "How can I remove a key from a Python dictionary?\n\nI want to remove a key from a dictionary if it is present. I currently use this code:\nif key in my_dict:\n    del my_dict[key]\n\nWithout the if statement, the code will raise KeyError if the key is not present. How can I handle this more simply?\n\nSee Delete an element from a dictionary for more general approaches to the problem of removing a key from a dict (including ones which produce a modified copy).",
    "response": "To delete a key regardless of whether it is in the dictionary, use the two-argument form of dict.pop():\nmy_dict.pop('key', None)\n\nThis will return my_dict[key] if key exists in the dictionary, and None otherwise. If the second parameter is not specified (i.e. my_dict.pop('key')) and key does not exist, a KeyError is raised.\nTo delete a key that is guaranteed to exist, you can also use\ndel my_dict['key']\n\nThis will raise a KeyError if the key is not in the dictionary.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 11277432,
      "answer_id": 11277439,
      "question_score": 3006,
      "answer_score": 4799,
      "is_accepted": true,
      "tags": [
        "python",
        "dictionary",
        "unset"
      ]
    }
  },
  {
    "prompt": "Convert string &quot;Jun 1 2005 1:33PM&quot; into datetime\n\nI have a huge list of datetime strings like the following\n[\"Jun 1 2005 1:33PM\", \"Aug 28 1999 12:00AM\"]\n\nHow do I convert them into datetime objects?",
    "response": "datetime.strptime parses an input string in the user-specified format into a timezone-naive datetime object:\n>>> from datetime import datetime\n>>> datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\ndatetime.datetime(2005, 6, 1, 13, 33)\n\nTo obtain a date object using an existing datetime object, convert it using .date():\n>>> datetime.strptime('Jun 1 2005', '%b %d %Y').date()\ndate(2005, 6, 1)\n\n\nLinks:\n\nstrptime docs: Python 2, Python 3\n\nstrptime/strftime format string docs: Python 2, Python 3\n\nstrftime.org format string cheatsheet\n\n\nNotes:\n\nstrptime = \"string parse time\"\nstrftime = \"string format time\"",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 466345,
      "answer_id": 466376,
      "question_score": 3000,
      "answer_score": 4465,
      "is_accepted": true,
      "tags": [
        "python",
        "datetime",
        "type-conversion"
      ]
    }
  },
  {
    "prompt": "How to upgrade all Python packages with pip\n\nIs it possible to upgrade all Python packages at one time with pip?\nNote: that there is a feature request for this on the official issue tracker.",
    "response": "There isn't a built-in flag yet. Starting with pip version 22.3, the --outdated and --format=freeze have become mutually exclusive. Use Python, to parse the JSON output:\npip --disable-pip-version-check list --outdated --format=json | python -c \"import json, sys; print('\\n'.join([x['name'] for x in json.load(sys.stdin)]))\" | xargs -n1 pip install -U\n\nIf you are using pip<22.3 you can use:\npip list --outdated --format=freeze | grep -v '^\\-e' | cut -d = -f 1  | xargs -n1 pip install -U\n\nFor older versions of pip:\npip freeze --local | grep -v '^\\-e' | cut -d = -f 1  | xargs -n1 pip install -U\n\n\n\nThe grep is to skip editable (\"-e\") package definitions, as suggested by @jawache. (Yes, you could replace grep+cut with sed or awk or perl or...).\n\nThe -n1 flag for xargs prevents stopping everything if updating one package fails (thanks @andsens).\n\n\n\nNote: there are infinite potential variations for this. I'm trying to keep this answer short and simple, but please do suggest variations in the com",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 2720014,
      "answer_id": 3452888,
      "question_score": 2883,
      "answer_score": 2944,
      "is_accepted": true,
      "tags": [
        "python",
        "pip",
        "pypi"
      ]
    }
  },
  {
    "prompt": "How can I sort a list of dictionaries by a value of the dictionary in Python?\n\nHow do I sort a list of dictionaries by a specific key's value? Given:\n[{'name': 'Homer', 'age': 39}, {'name': 'Bart', 'age': 10}]\n\nWhen sorted by name, it should become:\n[{'name': 'Bart', 'age': 10}, {'name': 'Homer', 'age': 39}]",
    "response": "The sorted() function takes a key= parameter\nnewlist = sorted(list_to_be_sorted, key=lambda d: d['name'])\n\nAlternatively, you can use operator.itemgetter instead of defining the function yourself\nfrom operator import itemgetter\nnewlist = sorted(list_to_be_sorted, key=itemgetter('name'))\n\nFor completeness, add reverse=True to sort in descending order\nnewlist = sorted(list_to_be_sorted, key=itemgetter('name'), reverse=True)",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 72899,
      "answer_id": 73050,
      "question_score": 2852,
      "answer_score": 3739,
      "is_accepted": true,
      "tags": [
        "python",
        "list",
        "sorting",
        "dictionary",
        "data-structures"
      ]
    }
  },
  {
    "prompt": "How to leave/exit/deactivate a Python virtualenv\n\nI'm using virtualenv and the virtualenvwrapper. I can switch between virtualenv's just fine using the workon command.\nme@mymachine:~$ workon env1\n(env1)me@mymachine:~$ workon env2\n(env2)me@mymachine:~$ workon env1\n(env1)me@mymachine:~$ \n\nHow do I exit all virtual environments and work on my system environment again? Right now, the only way I have of getting back to me@mymachine:~$ is to exit the shell and start a new one. That's kind of annoying. Is there a command to work on \"nothing\", and if s",
    "response": "Usually, activating a virtualenv gives you a shell function named:\n\n$ deactivate\n\n\nwhich puts things back to normal.\n\nI have just looked specifically again at the code for virtualenvwrapper, and, yes, it too supports deactivate as the way to escape from all virtualenvs.\n\nIf you are trying to leave an Anaconda environment, the command depends upon your version of conda. Recent versions (like 4.6) install a conda function directly in your shell, in which case you run:\n\nconda deactivate\n\n\nOlder conda versions instead implement deactivation using a stand-alone script:\n\nsource deactivate",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 990754,
      "answer_id": 990779,
      "question_score": 2827,
      "answer_score": 4513,
      "is_accepted": true,
      "tags": [
        "python",
        "virtualenv",
        "exit",
        "virtualenvwrapper"
      ]
    }
  },
  {
    "prompt": "How do I get the last element of a list?\n\nHow do I get the last element of a list? Which way is preferred?\nalist[-1]\nalist[len(alist) - 1]",
    "response": "some_list[-1] is the shortest and most Pythonic.\nIn fact, you can do much more with this syntax. The some_list[-n] syntax gets the nth-to-last element. So some_list[-1] gets the last element, some_list[-2] gets the second to last, etc, all the way down to some_list[-len(some_list)], which gives you the first element.\nYou can also set list elements in this way. For instance:\n>>> some_list = [1, 2, 3]\n>>> some_list[-1] = 5 # Set the last element\n>>> some_list[-2] = 3 # Set the second to last element\n>>> some_list\n[1, 3, 5]\n\nNote that getting a list item by index will raise an IndexError if the expected item doesn't exist. This means that some_list[-1] will raise an exception if some_list is empty, because an empty list can't have a last element.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 930397,
      "answer_id": 930398,
      "question_score": 2807,
      "answer_score": 4013,
      "is_accepted": true,
      "tags": [
        "python",
        "list",
        "indexing"
      ]
    }
  },
  {
    "prompt": "How do I parse a string to a float or int?\n\nHow can I convert an str to a float?\n\"545.2222\" -> 545.2222\n\nOr an str to a int?\n\"31\" -> 31\n\n\nFor the reverse, see Convert integer to string in Python and Converting a float to a string without rounding it.\nPlease instead use How can I read inputs as numbers? to close duplicate questions where OP received a string from user input and immediately wants to convert it, or was hoping for input (in 3.x) to convert the type automatically.",
    "response": ">>> a = \"545.2222\"\n>>> float(a)\n545.22220000000004\n>>> int(float(a))\n545",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 379906,
      "answer_id": 379910,
      "question_score": 2774,
      "answer_score": 3112,
      "is_accepted": true,
      "tags": [
        "python",
        "parsing",
        "floating-point",
        "type-conversion",
        "integer"
      ]
    }
  },
  {
    "prompt": "How do I install pip on Windows?\n\npip is a replacement for easy_install. But should I install pip using easy_install on Windows? Is there a better way?",
    "response": "Python 3.4+ and 2.7.9+\nGood news! Python 3.4 (released March 2014) and Python 2.7.9 (released December 2014) ship with Pip. This is the best feature of any Python release. It makes the community's wealth of libraries accessible to everyone. Newbies are no longer excluded from using community libraries by the prohibitive difficulty of setup. In shipping with a package manager, Python joins Ruby, Node.js, Haskell, Perl, Go—almost every other contemporary language with a majority open-source community. Thank you, Python.\nIf you do find that pip is not available, simply run ensurepip.\n\nOn Windows:\npy -3 -m ensurepip\n\n\nOtherwise:\npython3 -m ensurepip\n\n\n\nOf course, that doesn't mean Python packaging is problem solved. The experience remains frustrating. I discuss this in the Stack Overflow question Does Python have a package/module management system?.\nPython 3 ≤ 3.3 and 2 ≤ 2.7.8\nFlying in the face of its 'batteries included' motto, Python ships without a package manager. To make matters wor",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 4750806,
      "answer_id": 12476379,
      "question_score": 2773,
      "answer_score": 1962,
      "is_accepted": true,
      "tags": [
        "python",
        "windows",
        "pip"
      ]
    }
  },
  {
    "prompt": "How do I get a substring of a string in Python?\n\nI want to get a new string from the third character to the end of the string, e.g. myString[2:end]. If omitting the second part means 'to the end', and if you omit the first part, does it start from the start?",
    "response": ">>> x = \"Hello World!\"\n>>> x[2:]\n'llo World!'\n>>> x[:2]\n'He'\n>>> x[:-2]\n'Hello Worl'\n>>> x[-2:]\n'd!'\n>>> x[2:-2]\n'llo Worl'\n\n\nPython calls this concept \"slicing\" and it works on more than just strings. Take a look here for a comprehensive introduction.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 663171,
      "answer_id": 663175,
      "question_score": 2720,
      "answer_score": 3819,
      "is_accepted": true,
      "tags": [
        "python",
        "string",
        "substring"
      ]
    }
  },
  {
    "prompt": "How do I escape curly-brace ({}) characters characters in a string while using .format?\n\nNon-working example:\nprint(\" \\{ Hello \\} {0} \".format(42))\n\nDesired output:\n {Hello} 42",
    "response": "You need to double the {{ and }}:\n\n>>> x = \" {{ Hello }} {0} \"\n>>> print(x.format(42))\n' { Hello } 42 '\n\n\nHere's the relevant part of the Python documentation for format string syntax:\n\n\n  Format strings contain “replacement fields” surrounded by curly braces {}. Anything that is not contained in braces is considered literal text, which is copied unchanged to the output. If you need to include a brace character in the literal text, it can be escaped by doubling: {{ and }}.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 5466451,
      "answer_id": 5466478,
      "question_score": 2705,
      "answer_score": 3553,
      "is_accepted": true,
      "tags": [
        "python",
        "string",
        "format",
        "string-formatting",
        "curly-braces"
      ]
    }
  },
  {
    "prompt": "Check if a given key already exists in a dictionary\n\nI wanted to test if a key exists in a dictionary before updating the value for the key.\nI wrote the following code:\n\nif 'key1' in dict.keys():\n  print \"blah\"\nelse:\n  print \"boo\"\n\n\nI think this is not the best way to accomplish this task. Is there a better way to test for a key in the dictionary?",
    "response": "in tests for the existence of a key in a dict:\nd = {\"key1\": 10, \"key2\": 23}\n\nif \"key1\" in d:\n    print(\"this will execute\")\n\nif \"nonexistent key\" in d:\n    print(\"this will not\")\n\n\nUse dict.get() to provide a default value when the key does not exist:\nd = {}\n\nfor i in range(100):\n    key = i % 10\n    d[key] = d.get(key, 0) + 1\n\n\nTo provide a default value for every key, either use dict.setdefault() on each assignment:\nd = {}\n\nfor i in range(100):\n    d[i % 10] = d.setdefault(i % 10, 0) + 1    \n\n...or better, use defaultdict from the collections module:\nfrom collections import defaultdict\n\nd = defaultdict(int)\n\nfor i in range(100):\n    d[i % 10] += 1",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 1602934,
      "answer_id": 1602964,
      "question_score": 2675,
      "answer_score": 5704,
      "is_accepted": true,
      "tags": [
        "python",
        "dictionary"
      ]
    }
  },
  {
    "prompt": "Importing files from different folder\n\nI have this folder structure:\napplication\n├── app\n│   └── folder\n│       └── file.py\n└── app2\n    └── some_folder\n        └── some_file.py\n\nHow can I import a function from file.py, from within some_file.py? I tried:\nfrom application.app.folder.file import func_name\n\nbut it doesn't work.",
    "response": "Note: This answer was intended for a very specific question. For most programmers coming here from a search engine, this is not the answer you are looking for. Typically you would structure your files into packages (see other answers) instead of modifying the search path.\n\nBy default, you can't. When importing a file, Python only searches the directory that the entry-point script is running from and sys.path which includes locations such as the package installation directory (it's actually a little more complex than this, but this covers most cases).\nHowever, you can add to the Python path at runtime:\n# some_file.py\nimport sys\n# caution: path[0] is reserved for script path (or '' in REPL)\nsys.path.insert(1, '/path/to/application/app/folder')\n\nimport file",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 4383571,
      "answer_id": 4383597,
      "question_score": 2655,
      "answer_score": 2352,
      "is_accepted": false,
      "tags": [
        "python",
        "importerror",
        "python-import"
      ]
    }
  },
  {
    "prompt": "Class (static) variables and methods\n\nHow do I create class (i.e. static) variables or methods in Python?",
    "response": "Variables declared inside the class definition, but not inside a method are class or static variables:\n>>> class MyClass:\n...     i = 3\n...\n>>> MyClass.i\n3 \n\nAs @millerdev points out, this creates a class-level i variable, but this is distinct from any instance-level i variable, so you could have\n>>> m = MyClass()\n>>> m.i = 4\n>>> MyClass.i, m.i\n>>> (3, 4)\n\nThis is different from C++ and Java, but not so different from C#, where a static member can't be accessed using a reference to an instance.\nSee what the Python tutorial has to say on the subject of classes and class objects.\n@Steve Johnson has already answered regarding static methods, also documented under \"Built-in Functions\" in the Python Library Reference.\nclass C:\n    @staticmethod\n    def f(arg1, arg2, ...): ...\n\n@beidy recommends classmethods over staticmethod, as the method then receives the class type as the first argument.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 68645,
      "answer_id": 68672,
      "question_score": 2629,
      "answer_score": 2399,
      "is_accepted": true,
      "tags": [
        "python",
        "class",
        "static",
        "class-variables"
      ]
    }
  },
  {
    "prompt": "How do I lowercase a string in Python?\n\nIs there a way to convert a string to lowercase?\n\"Kilometers\"  →  \"kilometers\"\n\n\nSee How to change a string into uppercase? for the opposite.",
    "response": "Use str.lower():\n\"Kilometer\".lower()",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 6797984,
      "answer_id": 6797990,
      "question_score": 2581,
      "answer_score": 3656,
      "is_accepted": true,
      "tags": [
        "python",
        "string",
        "uppercase",
        "lowercase"
      ]
    }
  },
  {
    "prompt": "How can I check if an object has an attribute?\n\nHow do I check if an object has some attribute? For example:\n>>> a = SomeClass()\n>>> a.property\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: SomeClass instance has no attribute 'property'\n\nHow do I tell if a has the attribute property before using it?",
    "response": "Try hasattr():\nif hasattr(a, 'property'):\n    a.property\n\nSee zweiterlinde's answer, which offers good advice about asking forgiveness! It is a very Pythonic approach!\nThe general practice in Python is that, if the property is likely to be there most of the time, simply call it and either let the exception propagate, or trap it with a try/except block. This will likely be faster than hasattr. If the property is likely to not be there most of the time, or you're not sure, using hasattr will probably be faster than repeatedly falling into an exception block.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 610883,
      "answer_id": 610893,
      "question_score": 2529,
      "answer_score": 3516,
      "is_accepted": true,
      "tags": [
        "python",
        "class",
        "object",
        "attributes",
        "attributeerror"
      ]
    }
  },
  {
    "prompt": "How to print without a newline or space\n\nConsider these examples using print in Python:\n>>> for i in range(4): print('.')\n.\n.\n.\n.\n>>> print('.', '.', '.', '.')\n. . . .\n\nEither a newline or a space is added between each value. How can I avoid that, so that the output is .... instead? In other words, how can I \"append\" strings to the standard output stream?",
    "response": "In Python 3, you can use the sep= and end= parameters of the print function:\n\nTo not add a newline to the end of the string:\n\nprint('.', end='')\n\n\nTo not add a space between all the function arguments you want to print:\n\nprint('a', 'b', 'c', sep='')\n\n\nYou can pass any string to either parameter, and you can use both parameters at the same time.\n\nIf you are having trouble with buffering, you can flush the output by adding flush=True keyword argument:\n\nprint('.', end='', flush=True)\n\n\nPython 2.6 and 2.7\n\nFrom Python 2.6 you can either import the print function from Python 3 using the __future__ module:\n\nfrom __future__ import print_function\n\n\nwhich allows you to use the Python 3 solution above.\n\nHowever, note that the flush keyword is not available in the version of the print function imported from __future__ in Python 2; it only works in Python 3, more specifically 3.3 and later. In earlier versions you'll still need to flush manually with a call to sys.stdout.flush(). You'll also have ",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 493386,
      "answer_id": 493399,
      "question_score": 2516,
      "answer_score": 3407,
      "is_accepted": true,
      "tags": [
        "python",
        "trailing-newline"
      ]
    }
  },
  {
    "prompt": "Calling a function of a module by using its name (a string)\n\nHow do I call a function, using a string with the function's name? For example:\nimport foo\nfunc_name = \"bar\"\ncall(foo, func_name)  # calls foo.bar()",
    "response": "Given a module foo with method bar:\nimport foo\nbar = getattr(foo, 'bar')\nresult = bar()\n\ngetattr can similarly be used on class instance bound methods, module-level methods, class methods... the list goes on.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 3061,
      "answer_id": 3071,
      "question_score": 2493,
      "answer_score": 2978,
      "is_accepted": true,
      "tags": [
        "python",
        "object",
        "reflection"
      ]
    }
  },
  {
    "prompt": "Limiting floats to two decimal points\n\nI want a to be rounded to 13.95. I tried using round, but I get:\n>>> a\n13.949999999999999\n>>> round(a, 2)\n13.949999999999999\n\n\nFor the analogous issue with the standard library Decimal class, see How can I format a decimal to always show 2 decimal places?.",
    "response": "You are running into the old problem with floating point numbers that not all numbers can be represented exactly. The command line is just showing you the full floating point form from memory.\n\nWith floating point representation, your rounded version is the same number. Since computers are binary, they store floating point numbers as an integer and then divide it by a power of two so 13.95 will be represented in a similar fashion to 125650429603636838/(2**53).\n\nDouble precision numbers have 53 bits (16 digits) of precision and regular floats have 24 bits (8 digits) of precision. The floating point type in Python uses double precision to store the values.\n\nFor example,\n\n>>> 125650429603636838/(2**53)\n13.949999999999999\n\n>>> 234042163/(2**24)\n13.949999988079071\n\n>>> a = 13.946\n>>> print(a)\n13.946\n>>> print(\"%.2f\" % a)\n13.95\n>>> round(a,2)\n13.949999999999999\n>>> print(\"%.2f\" % round(a, 2))\n13.95\n>>> print(\"{:.2f}\".format(a))\n13.95\n>>> print(\"{:.2f}\".format(round(a, 2)))\n13.95\n>>> print(\"{",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 455612,
      "answer_id": 455634,
      "question_score": 2453,
      "answer_score": 2324,
      "is_accepted": false,
      "tags": [
        "python",
        "floating-point",
        "rounding",
        "precision"
      ]
    }
  },
  {
    "prompt": "How can I randomly select (choose) an item from a list (get a random element)?\n\nHow do I retrieve an item at random from the following list?\nfoo = ['a', 'b', 'c', 'd', 'e']",
    "response": "Use random.choice():\nimport random\n\nfoo = ['a', 'b', 'c', 'd', 'e']\nprint(random.choice(foo))\n\nFor cryptographically secure random choices (e.g., for generating a passphrase from a wordlist), use secrets.choice():\nimport secrets\n\nfoo = ['battery', 'correct', 'horse', 'staple']\nprint(secrets.choice(foo))\n\nsecrets is new in Python 3.6. On older versions of Python you can use the random.SystemRandom class:\nimport random\n\nsecure_random = random.SystemRandom()\nprint(secure_random.choice(foo))",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 306400,
      "answer_id": 306417,
      "question_score": 2434,
      "answer_score": 3534,
      "is_accepted": true,
      "tags": [
        "python",
        "list",
        "random"
      ]
    }
  },
  {
    "prompt": "How to remove an element from a list by index\n\nHow do I remove an element from a list by index?\nI found list.remove(), but this slowly scans the list for an item by value.",
    "response": "Use del and specify the index of the element you want to delete:\n\n>>> a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n>>> del a[-1]\n>>> a\n[0, 1, 2, 3, 4, 5, 6, 7, 8]\n\n\nAlso supports slices:\n\n>>> del a[2:4]\n>>> a\n[0, 1, 4, 5, 6, 7, 8, 9]\n\n\nHere is the section from the tutorial.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 627435,
      "answer_id": 627453,
      "question_score": 2344,
      "answer_score": 2607,
      "is_accepted": true,
      "tags": [
        "python",
        "list",
        "indexing"
      ]
    }
  },
  {
    "prompt": "Delete a column from a Pandas DataFrame\n\nTo delete a column in a DataFrame, I can successfully use:\ndel df['column_name']\n\nBut why can't I use the following?\ndel df.column_name\n\nSince it is possible to access the Series via df.column_name, I expected this to work.",
    "response": "As you've guessed, the right syntax is \n\ndel df['column_name']\n\n\nIt's difficult to make del df.column_name work simply as the result of syntactic limitations in Python. del df[name] gets translated to df.__delitem__(name) under the covers by Python.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 13411544,
      "answer_id": 13485766,
      "question_score": 2330,
      "answer_score": 1335,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "del"
      ]
    }
  },
  {
    "prompt": "How do I get the number of elements in a list (length of a list) in Python?\n\nHow do I get the number of elements in the list items?\nitems = [\"apple\", \"orange\", \"banana\"]\n\n# There are 3 items.",
    "response": "The len() function can be used with several different types in Python - both built-in types and library types. For example:\n>>> len([1, 2, 3])\n3",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 1712227,
      "answer_id": 1712236,
      "question_score": 2302,
      "answer_score": 2979,
      "is_accepted": true,
      "tags": [
        "python",
        "list"
      ]
    }
  },
  {
    "prompt": "What is the difference between venv, pyvenv, pyenv, virtualenv, virtualenvwrapper, pipenv, etc?\n\nPython 3.3 includes in its standard library the new package venv. What does it do, and how does it differ from all the other packages that match the regex (py)?(v|virtual|pip)?env?",
    "response": "This is my personal recommendation for beginners: start by learning virtualenv and pip, tools which work with both Python 2 and 3 and in a variety of situations, and pick up other tools once you start needing them.\nNow on to answer the question: what is the difference between these similarly named things: venv, virtualenv, etc?\nPyPI packages not in the standard library:\n\nvirtualenv is a very popular tool that creates isolated Python environments for Python libraries. If you're not familiar with this tool, I highly recommend learning it, as it is a very useful tool.\nIt works by installing a bunch of files in a directory (eg: env/), and then modifying the PATH environment variable to prefix it with a custom bin directory (eg: env/bin/). An exact copy of the python or python3 binary is placed in this directory, but Python is programmed to look for libraries relative to its path first, in the environment directory. It's not part of Python's standard library, but is officially blessed by th",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 41573587,
      "answer_id": 41573588,
      "question_score": 2255,
      "answer_score": 2450,
      "is_accepted": true,
      "tags": [
        "python",
        "virtualenv",
        "virtualenvwrapper",
        "pyenv",
        "python-venv"
      ]
    }
  },
  {
    "prompt": "How do I pad a string with zeros?\n\nHow do I pad a numeric string with zeroes to the left, so that the string has a specific length?",
    "response": "To pad strings:\n>>> n = '4'\n>>> print(n.zfill(3))\n004\n\nTo pad numbers:\n>>> n = 4\n>>> print(f'{n:03}') # Preferred method, python >= 3.6\n004\n>>> print('%03d' % n)\n004\n>>> print(format(n, '03')) # python >= 2.6\n004\n>>> print('{0:03d}'.format(n))  # python >= 2.6 + python 3\n004\n>>> print('{foo:03d}'.format(foo=n))  # python >= 2.6 + python 3\n004\n>>> print('{:03d}'.format(n))  # python >= 2.7 + python3\n004\n\nString formatting documentation.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 339007,
      "answer_id": 339013,
      "question_score": 2249,
      "answer_score": 3497,
      "is_accepted": true,
      "tags": [
        "python",
        "string",
        "zero-padding"
      ]
    }
  },
  {
    "prompt": "How can I determine a Python variable&#39;s type?\n\nHow do I see the type of a variable? (e.g., unsigned 32 bit)",
    "response": "Use the type() built-in function:\n>>> i = 123\n>>> type(i)\n<type 'int'>\n>>> type(i) is int\nTrue\n>>> i = 123.456\n>>> type(i)\n<type 'float'>\n>>> type(i) is float\nTrue\n\nTo check if a variable is of a given type, use isinstance:\n>>> i = 123\n>>> isinstance(i, int)\nTrue\n>>> isinstance(i, (float, str, set, dict))\nFalse\n\nNote that Python doesn't have the same types as C/C++, which appears to be your question.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 402504,
      "answer_id": 402704,
      "question_score": 2245,
      "answer_score": 2109,
      "is_accepted": false,
      "tags": [
        "python",
        "types"
      ]
    }
  },
  {
    "prompt": "Delete an element from a dictionary\n\nHow do I delete an item from a dictionary in Python?\nWithout modifying the original dictionary, how do I obtain another dictionary with the item removed?\n\n\nSee also How can I remove a key from a Python dictionary? for the specific issue of removing an item (by key) that may not already be present.",
    "response": "The del statement removes an element:\ndel d[key]\n\nNote that this mutates the existing dictionary, so the contents of the dictionary changes for anybody else who has a reference to the same instance. To return a new dictionary, make a copy of the dictionary:\ndef removekey(d, key):\n    r = dict(d)\n    del r[key]\n    return r\n\nThe dict() constructor makes a shallow copy. To make a deep copy, see the copy module.\n\nNote that making a copy for every dict del/assignment/etc. means you're going from constant time to linear time, and also using linear space. For small dicts, this is not a problem. But if you're planning to make lots of copies of large dicts, you probably want a different data structure, like a HAMT (as described in this answer).",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 5844672,
      "answer_id": 5844692,
      "question_score": 2240,
      "answer_score": 2570,
      "is_accepted": true,
      "tags": [
        "python",
        "dictionary",
        "del"
      ]
    }
  },
  {
    "prompt": "Determine the type of an object?\n\nIs there a simple way to determine if a variable is a list, dictionary, or something else?",
    "response": "There are two built-in functions that help you identify the type of an object. You can use type()  if you need the exact type of an object, and isinstance() to check an object’s type against something. Usually, you want to use isinstance() most of the times since it is very robust and also supports type inheritance.\n\nTo get the actual type of an object, you use the built-in type() function. Passing an object as the only parameter will return the type object of that object:\n>>> type([]) is list\nTrue\n>>> type({}) is dict\nTrue\n>>> type('') is str\nTrue\n>>> type(0) is int\nTrue\n\nThis of course also works for custom types:\n>>> class Test1 (object):\n        pass\n>>> class Test2 (Test1):\n        pass\n>>> a = Test1()\n>>> b = Test2()\n>>> type(a) is Test1\nTrue\n>>> type(b) is Test2\nTrue\n\nNote that type() will only return the immediate type of the object, but won’t be able to tell you about type inheritance.\n>>> type(b) is Test1\nFalse\n\nTo cover that, you should use the isinstance function. This of c",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 2225038,
      "answer_id": 2225066,
      "question_score": 2225,
      "answer_score": 2397,
      "is_accepted": true,
      "tags": [
        "python",
        "dictionary",
        "types",
        "typeof"
      ]
    }
  },
  {
    "prompt": "How do I count the occurrences of a list item?\n\nGiven a single item, how do I count occurrences of it in a list, in Python?\n\nA related but different problem is counting occurrences of each different element in a collection, getting a dictionary or list as a histogram result instead of a single integer. For that problem, see Using a dictionary to count the items in a list.",
    "response": "If you only want a single item's count, use the count method:\n>>> [1, 2, 3, 4, 1, 4, 1].count(1)\n3\n\n\nImportant: this is very slow if you are counting multiple different items\nEach count call goes over the entire list of n elements. Calling count in a loop n times means n * n total checks, which can be catastrophic for performance.\nIf you want to count multiple items, use Counter, which only does n total checks.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 2600191,
      "answer_id": 2600208,
      "question_score": 2219,
      "answer_score": 2558,
      "is_accepted": true,
      "tags": [
        "python",
        "list",
        "count"
      ]
    }
  },
  {
    "prompt": "How to check if the string is empty in Python?\n\nDoes Python have something like an empty string variable where you can do:\nif myString == string.empty:\n\nRegardless, what's the most elegant way to check for empty string values? I find hard coding \"\" every time for checking an empty string not as good.",
    "response": "Empty strings are \"falsy\" (python 2 or python 3 reference), which means they are considered false in a Boolean context, so you can just do this:\nif not myString:\n\nThis is the preferred way if you know that your variable is a string.  If your variable could also be some other type then you should use:\nif myString == \"\":\n\nSee the documentation on Truth Value Testing for other values that are false in Boolean contexts.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 9573244,
      "answer_id": 9573259,
      "question_score": 2218,
      "answer_score": 3153,
      "is_accepted": true,
      "tags": [
        "python",
        "string",
        "boolean",
        "is-empty",
        "comparison-operators"
      ]
    }
  },
  {
    "prompt": "Why is reading lines from stdin much slower in C++ than Python?\n\nI wanted to compare reading lines of string input from stdin using Python and C++ and was shocked to see my C++ code run an order of magnitude slower than the equivalent Python code. Since my C++ is rusty and I'm not yet an expert Pythonista, please tell me if I'm doing something wrong or if I'm misunderstanding something.\n\n(TLDR answer: include the statement: cin.sync_with_stdio(false) or just use fgets instead.\nTLDR results: scroll all the way down to the bottom of my question and look at the ",
    "response": "tl;dr: Because of different default settings in C++ requiring more system calls.\nBy default, cin is synchronized with stdio, which causes it to avoid any input buffering.  If you add this to the top of your main, you should see much better performance:\nstd::ios_base::sync_with_stdio(false);\n\nNormally, when an input stream is buffered, instead of reading one character at a time, the stream will be read in larger chunks.  This reduces the number of system calls, which are typically relatively expensive.  However, since the FILE* based stdio and iostreams often have separate implementations and therefore separate buffers, this could lead to a problem if both were used together.  For example:\nint myvalue1;\ncin >> myvalue1;\nint myvalue2;\nscanf(\"%d\",&myvalue2);\n\nIf more input was read by cin than it actually needed, then the second integer value wouldn't be available for the scanf function, which has its own independent buffer.  This would lead to unexpected results.\nTo avoid this, by defaul",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 9371238,
      "answer_id": 9371717,
      "question_score": 2199,
      "answer_score": 1959,
      "is_accepted": true,
      "tags": [
        "python",
        "c++",
        "benchmarking",
        "iostream",
        "getline"
      ]
    }
  },
  {
    "prompt": "How do I measure elapsed time in Python?\n\nI want to measure the time it took to execute a function. I couldn't get timeit to work:\nimport timeit\nstart = timeit.timeit()\nprint(\"hello\")\nend = timeit.timeit()\nprint(end - start)",
    "response": "Use time.time() to measure the elapsed wall-clock time between two points:\nimport time\n\nstart = time.time()\nprint(\"hello\")\nend = time.time()\nprint(end - start)\n\nThis gives the execution time in seconds.\n\nAnother option since Python 3.3 might be to use perf_counter or process_time, depending on your requirements. Before 3.3 it was recommended to use time.clock (thanks Amber). However, it is currently deprecated:\n\nOn Unix, return the current processor time as a floating point number\nexpressed in seconds. The precision, and in fact the very definition\nof the meaning of “processor time”, depends on that of the C function\nof the same name.\nOn Windows, this function returns wall-clock seconds elapsed since the\nfirst call to this function, as a floating point number, based on the\nWin32 function QueryPerformanceCounter(). The resolution is typically\nbetter than one microsecond.\nDeprecated since version 3.3: The behaviour of this function depends\non the platform: use perf_counter() or process_t",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 7370801,
      "answer_id": 7370824,
      "question_score": 2187,
      "answer_score": 2620,
      "is_accepted": false,
      "tags": [
        "python",
        "performance",
        "measure",
        "timeit"
      ]
    }
  },
  {
    "prompt": "Why is it string.join(list) instead of list.join(string)?\n\nThis has always confused me. It seems like this would be nicer:\n[\"Hello\", \"world\"].join(\"-\")\n\nThan this:\n\"-\".join([\"Hello\", \"world\"])\n\nIs there a specific reason it is like this?",
    "response": "It's because any iterable can be joined (e.g, list, tuple, dict, set), but its contents and the \"joiner\" must be strings.\nFor example:\n'_'.join(['welcome', 'to', 'stack', 'overflow'])\n'_'.join(('welcome', 'to', 'stack', 'overflow'))\n\n'welcome_to_stack_overflow'\n\nUsing something other than strings will raise the following error:\n\nTypeError: sequence item 0: expected str instance, int found",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 493819,
      "answer_id": 493842,
      "question_score": 2157,
      "answer_score": 1472,
      "is_accepted": true,
      "tags": [
        "python",
        "string",
        "list"
      ]
    }
  },
  {
    "prompt": "How do I append to a file?\n\nHow do I append to a file instead of overwriting it?",
    "response": "Set the mode in open() to \"a\" (append) instead of \"w\" (write):\nwith open(\"test.txt\", \"a\") as myfile:\n    myfile.write(\"appended text\")\n\nThe documentation lists all the available modes.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 4706499,
      "answer_id": 4706520,
      "question_score": 2136,
      "answer_score": 3171,
      "is_accepted": true,
      "tags": [
        "python",
        "file",
        "append"
      ]
    }
  },
  {
    "prompt": "Is there a way to run Python on Android?\n\nWe are working on an S60 version and this platform has a nice Python API..\n\nHowever, there is nothing official about Python on Android, but since Jython exists, is there a way to let the snake and the robot work together??",
    "response": "One way is to use Kivy:\n\n\n  Open source Python library for rapid development of applications\n  that make use of innovative user interfaces, such as multi-touch apps.\n\n\n\n\n\n  Kivy runs on Linux, Windows, OS X, Android and iOS. You can run the same [python] code on all supported platforms.\n\n\nKivy Showcase app",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 101754,
      "answer_id": 8189603,
      "question_score": 2083,
      "answer_score": 1190,
      "is_accepted": true,
      "tags": [
        "android",
        "python",
        "jython",
        "ase",
        "android-scripting"
      ]
    }
  },
  {
    "prompt": "How can I remove a trailing newline?\n\nHow can I remove the last character of a string if it is a newline?\n\"abc\\n\"  -->  \"abc\"",
    "response": "Try the method rstrip() (see doc Python 2 and Python 3)\n>>> 'test string\\n'.rstrip()\n'test string'\n\nPython's rstrip() method strips all kinds of trailing whitespace by default, not just one newline as Perl does with chomp.\n>>> 'test string \\n \\r\\n\\n\\r \\n\\n'.rstrip()\n'test string'\n\nTo strip only newlines:\n>>> 'test string \\n \\r\\n\\n\\r \\n\\n'.rstrip('\\n')\n'test string \\n \\r\\n\\n\\r '\n\nIn addition to rstrip(), there are also the methods strip() and lstrip(). Here is an example with the three of them:\n>>> s = \"   \\n\\r\\n  \\n  abc   def \\n\\r\\n  \\n  \"\n>>> s.strip()\n'abc   def'\n>>> s.lstrip()\n'abc   def \\n\\r\\n  \\n  '\n>>> s.rstrip()\n'   \\n\\r\\n  \\n  abc   def'",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 275018,
      "answer_id": 275025,
      "question_score": 2074,
      "answer_score": 2334,
      "is_accepted": false,
      "tags": [
        "python",
        "newline",
        "trailing"
      ]
    }
  },
  {
    "prompt": "Static methods in Python?\n\nCan I define a static method which I can call directly on the class instance? e.g.,\nMyClass.the_static_method()",
    "response": "Yep, using the staticmethod decorator:\nclass MyClass(object):\n    @staticmethod\n    def the_static_method(x):\n        print(x)\n\nMyClass.the_static_method(2)  # outputs 2\n\nNote that some code might use the old method of defining a static method, using staticmethod as a function rather than a decorator. This should only be used if you have to support ancient versions of Python (2.2 and 2.3):\nclass MyClass(object):\n    def the_static_method(x):\n        print(x)\n    the_static_method = staticmethod(the_static_method)\n\nMyClass.the_static_method(2)  # outputs 2\n\nThis is entirely identical to the first example (using @staticmethod), just not using the nice decorator syntax.\nFinally, use staticmethod sparingly! There are very few situations where static-methods are necessary in Python, and I've seen them used many times where a separate \"top-level\" function would have been clearer.\n\nThe following is verbatim from the documentation::\n\nA static method does not receive an implicit first argument.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 735975,
      "answer_id": 735978,
      "question_score": 2069,
      "answer_score": 2340,
      "is_accepted": true,
      "tags": [
        "python",
        "static-methods"
      ]
    }
  },
  {
    "prompt": "Installing specific package version with pip\n\nI am trying to install version 1.2.2 of MySQL_python, using a fresh virtualenv created with the --no-site-packages option. The current version shown in PyPi is 1.2.3. Is there a way to install the older version? I have tried:\npip install MySQL_python==1.2.2\n\nHowever, when installed, it still shows MySQL_python-1.2.3-py2.6.egg-info in the site packages. Is this a problem specific to this package, or am I doing something wrong?",
    "response": "TL;DR:\nUpdate as of 2022-12-28:\npip install --force-reinstall -v\nFor example: pip install --force-reinstall -v \"MySQL_python==1.2.2\"\nWhat these options mean:\n\n--force-reinstall is an option to reinstall all packages even if they are already up-to-date.\n-v is for verbose. You can combine for even more verbosity (i.e. -vv) up to 3 times (e.g. --force-reinstall -vvv).\n\nThanks to @Peter for highlighting this (and it seems that the context of the question has broadened given the time when the question was first asked!), the documentation for Python discusses a caveat with using -I, in that it can break your installation if it was installed with a different package manager or if if your package is/was a different version.\n\nOriginal answer:\n\npip install -Iv (i.e. pip install -Iv MySQL_python==1.2.2)\n\n\nWhat these options mean:\n\n-I stands for --ignore-installed which will ignore the installed packages, overwriting them.\n-v is for verbose. You can combine for even more verbosity (i.e. -vv) up to",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 5226311,
      "answer_id": 5226504,
      "question_score": 2066,
      "answer_score": 1604,
      "is_accepted": true,
      "tags": [
        "python",
        "mysql",
        "pip",
        "pypi",
        "mysql-python"
      ]
    }
  },
  {
    "prompt": "How do I split the definition of a long string over multiple lines?\n\nI have a very long query. I would like to split it in several lines in Python. A way to do it in JavaScript would be using several sentences and joining them with a + operator (I know, maybe it's not the most efficient way to do it, but I'm not really concerned about performance in this stage, just code readability). Example:\nvar long_string = 'some text not important. just garbage to' +\n                      'illustrate my example';\n\nI tried doing something similar in Python, but it didn't work",
    "response": "Are you talking about multi-line strings? Easy, use triple quotes to start and end them.\n\ns = \"\"\" this is a very\n        long string if I had the\n        energy to type more and more ...\"\"\"\n\n\nYou can use single quotes too (3 of them of course at start and end) and treat the resulting string s just like any other string.\n\nNOTE: Just as with any string, anything between the starting and ending quotes becomes part of the string, so this example has a leading blank (as pointed out by @root45). This string will also contain both blanks and newlines.\n\nI.e.,:\n\n' this is a very\\n        long string if I had the\\n        energy to type more and more ...'\n\n\nFinally, one can also construct long lines in Python like this:\n\n s = (\"this is a very\"\n      \"long string too\"\n      \"for sure ...\"\n     )\n\n\nwhich will not include any extra blanks or newlines (this is a deliberate example showing what the effect of skipping blanks will result in):\n\n'this is a verylong string toofor sure ...'\n\n\nNo commas req",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 10660435,
      "answer_id": 10660443,
      "question_score": 2056,
      "answer_score": 3254,
      "is_accepted": true,
      "tags": [
        "python",
        "string",
        "multiline",
        "multilinestring"
      ]
    }
  },
  {
    "prompt": "How to prettyprint a JSON file?\n\nHow do I pretty-print a JSON file in Python?",
    "response": "Use the indent= parameter of json.dump() or json.dumps() to specify how many spaces to indent by:\n>>> import json\n>>> your_json = '[\"foo\", {\"bar\": [\"baz\", null, 1.0, 2]}]'\n>>> parsed = json.loads(your_json)\n>>> print(json.dumps(parsed, indent=4))\n[\n    \"foo\",\n    {\n        \"bar\": [\n            \"baz\",\n            null,\n            1.0,\n            2\n        ]\n    }\n]\n\nTo parse a file, use json.load():\nwith open('filename.txt', 'r') as handle:\n    parsed = json.load(handle)",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 12943819,
      "answer_id": 12944035,
      "question_score": 2046,
      "answer_score": 3082,
      "is_accepted": true,
      "tags": [
        "python",
        "json",
        "formatting",
        "pretty-print"
      ]
    }
  },
  {
    "prompt": "Getting the class name of an instance\n\nHow do I find out the name of the class used to create an instance of an object in Python?\nI'm not sure if I should use the inspect module or parse the __class__ attribute.",
    "response": "Have you tried the __name__ attribute of the class? ie type(x).__name__ will give you the name of the class, which I think is what you want.\n\n>>> import itertools\n>>> x = itertools.count(0)\n>>> type(x).__name__\n'count'\n\n\nIf you're still using Python 2, note that the above method works with new-style classes only (in Python 3+ all classes are \"new-style\" classes). Your code might use some old-style classes. The following works for both:\n\nx.__class__.__name__",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 510972,
      "answer_id": 511059,
      "question_score": 2030,
      "answer_score": 2635,
      "is_accepted": true,
      "tags": [
        "python",
        "introspection",
        "instanceof",
        "python-datamodel"
      ]
    }
  },
  {
    "prompt": "How to read a file line-by-line into a list?\n\nHow do I read every line of a file in Python and store each line as an element in a list? \n\nI want to read the file line by line and append each line to the end of the list.",
    "response": "This code will read the entire file into memory and remove all whitespace characters (newlines and spaces) from the end of each line:\nwith open(filename) as file:\n    lines = [line.rstrip() for line in file]\n\nIf you're working with a large file, then you should instead read and process it line-by-line:\nwith open(filename) as file:\n    for line in file:\n        print(line.rstrip())\n\nIn Python 3.8 and up you can use a while loop with the walrus operator like so:\nwith open(filename) as file:\n    while line := file.readline():\n        print(line.rstrip())\n\nDepending on what you plan to do with your file and how it was encoded, you may also want to manually set the access mode and character encoding:\nwith open(filename, 'r', encoding='UTF-8') as file:\n    while line := file.readline():\n        print(line.rstrip())",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 3277503,
      "answer_id": 3277516,
      "question_score": 2023,
      "answer_score": 3057,
      "is_accepted": false,
      "tags": [
        "python",
        "string",
        "file",
        "readlines"
      ]
    }
  },
  {
    "prompt": "How do I get the row count of a Pandas DataFrame?\n\nHow do I get the number of rows of a pandas dataframe df?",
    "response": "For a dataframe df, one can use any of the following:\n\nlen(df.index)\ndf.shape[0]\ndf[df.columns[0]].count() (== number of non-NaN values in first column)\n\n\n\nCode to reproduce the plot:\nimport numpy as np\nimport pandas as pd\nimport perfplot\n\nperfplot.save(\n    \"out.png\",\n    setup=lambda n: pd.DataFrame(np.arange(n * 3).reshape(n, 3)),\n    n_range=[2**k for k in range(25)],\n    kernels=[\n        lambda df: len(df.index),\n        lambda df: df.shape[0],\n        lambda df: df[df.columns[0]].count(),\n    ],\n    labels=[\"len(df.index)\", \"df.shape[0]\", \"df[df.columns[0]].count()\"],\n    xlabel=\"Number of rows\",\n)",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 15943769,
      "answer_id": 15943975,
      "question_score": 2023,
      "answer_score": 2994,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe"
      ]
    }
  },
  {
    "prompt": "How do I check if a string represents a number (float or int)?\n\nHow do I check if a string represents a numeric value in Python?\ndef is_number(s):\n    try:\n        float(s)\n        return True\n    except ValueError:\n        return False\n\nThe above works, but it seems clunky.\n\nIf what you are testing comes from user input, it is still a string even if it represents an int or a float. See How can I read inputs as numbers? for converting the input, and Asking the user for input until they give a valid response for ensuring that the input represents an int or fl",
    "response": "Which, not only is ugly and slow\n\n\nI'd dispute both.\n\nA regex or other string parsing method would be uglier and slower.  \n\nI'm not sure that anything much could be faster than the above.  It calls the function and returns.  Try/Catch doesn't introduce much overhead because the most common exception is caught without an extensive search of stack frames.\n\nThe issue is that any numeric conversion function has two kinds of results\n\n\nA number, if the number is valid\nA status code (e.g., via errno) or exception to show that no valid number could be parsed.\n\n\nC (as an example) hacks around this a number of ways.  Python lays it out clearly and explicitly.\n\nI think your code for doing this is perfect.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 354038,
      "answer_id": 354130,
      "question_score": 2019,
      "answer_score": 787,
      "is_accepted": true,
      "tags": [
        "python",
        "casting",
        "floating-point",
        "type-conversion",
        "integer"
      ]
    }
  },
  {
    "prompt": "What&#39;s the canonical way to check for type in Python?\n\nHow do I check if an object is of a given type, or if it inherits from a given type?\nHow do I check if the object o is of type str?\n\nEditor's note: Beginners often wrongly expect a string to already be \"a number\" – either expecting Python 3.x input to convert type, or expecting that a string like '1' is also simultaneously an integer. This question does not address those types of questions. Instead, see How do I check if a string represents a number (float or int)?, How can I read inputs as numb",
    "response": "Use isinstance to check if o is an instance of str or any subclass of str:\nif isinstance(o, str):\n\nTo check if the type of o is exactly str, excluding subclasses of str:\nif type(o) is str:\n\nSee Built-in Functions in the Python Library Reference for relevant information.\n\nChecking for strings in Python 2\nFor Python 2, this is a better way to check if o is a string:\nif isinstance(o, basestring):\n\nbecause this will also catch Unicode strings. unicode is not a subclass of str; both str and unicode are subclasses of basestring. In Python 3, basestring no longer exists since there's a strict separation of strings (str) and binary data (bytes).\nAlternatively, isinstance accepts a tuple of classes. This will return True if o is an instance of any subclass of any of (str, unicode):\nif isinstance(o, (str, unicode)):",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 152580,
      "answer_id": 152596,
      "question_score": 2006,
      "answer_score": 2350,
      "is_accepted": true,
      "tags": [
        "python",
        "types"
      ]
    }
  },
  {
    "prompt": "Meaning of @classmethod and @staticmethod for beginner\n\nWhat do @classmethod and @staticmethod mean in Python, and how are they different? When should I use them, why should I use them, and how should I use them?\nAs far as I understand, @classmethod tells a class that it's a method which should be inherited into subclasses, or... something. However, what's the point of that? Why not just define the class method without adding @classmethod or @staticmethod or any @ definitions?",
    "response": "Though classmethod and staticmethod are quite similar, there's a slight difference in usage for both entities: classmethod must have a reference to a class object as the first parameter, whereas staticmethod can have no parameters at all.\nExample\nclass Date(object):\n    \n    def __init__(self, day=0, month=0, year=0):\n        self.day = day\n        self.month = month\n        self.year = year\n\n    @classmethod\n    def from_string(cls, date_as_string):\n        day, month, year = map(int, date_as_string.split('-'))\n        date1 = cls(day, month, year)\n        return date1\n\n    @staticmethod\n    def is_date_valid(date_as_string):\n        day, month, year = map(int, date_as_string.split('-'))\n        return day <= 31 and month <= 12 and year <= 3999\n\ndate2 = Date.from_string('11-09-2012')\nis_date = Date.is_date_valid('11-09-2012')\n\nExplanation\nLet's assume an example of a class, dealing with date information (this will be our boilerplate):\nclass Date(object):\n    \n    def __init__(self, da",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 12179271,
      "answer_id": 12179752,
      "question_score": 2000,
      "answer_score": 3050,
      "is_accepted": true,
      "tags": [
        "python",
        "oop",
        "static-methods",
        "class-method"
      ]
    }
  },
  {
    "prompt": "How can I install packages using pip according to the requirements.txt file from a local directory?\n\nHere is the problem:\nI have a requirements.txt file that looks like:\nBeautifulSoup==3.2.0\nDjango==1.3\nFabric==1.2.0\nJinja2==2.5.5\nPyYAML==3.09\nPygments==1.4\nSQLAlchemy==0.7.1\nSouth==0.7.3\namqplib==0.6.1\nanyjson==0.3\n...\n\nI have a local archive directory containing all the packages + others.\nI have created a new virtualenv with\nbin/virtualenv testing\n\nUpon activating it, I tried to install the packages according to requirements.txt from the local archive directory.\nsource bin/activate\npip install",
    "response": "This works for me:\n$ pip install -r requirements.txt --no-index --find-links file:///tmp/packages\n\n--no-index - Ignore package index (only look at --find-links URLs instead).\n-f, --find-links <URL> - If <URL> is a URL or a path to an HTML file, then parse for links to archives. If <URL> is a local path or a file:// URL that's a directory, then look for archives in the directory listing.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 7225900,
      "answer_id": 10429168,
      "question_score": 1981,
      "answer_score": 1170,
      "is_accepted": true,
      "tags": [
        "python",
        "pip",
        "virtualenv",
        "requirements.txt"
      ]
    }
  },
  {
    "prompt": "How do I declare custom exceptions in modern Python?\n\nHow do I declare custom exception classes in modern Python? My primary goal is to follow whatever standard other exception classes have, so that (for instance) any extra string I include in the exception is printed out by whatever tool caught the exception.\nBy \"modern Python\" I mean something that will run in Python 2.5 but be 'correct' for the Python 2.6 and Python 3.* way of doing things. And by \"custom\" I mean an Exception object that can include extra data about the cause of the error: a str",
    "response": "Maybe I missed the question, but why not:\nclass MyException(Exception):\n    pass\n\nTo override something (or pass extra args), do this:\nclass ValidationError(Exception):\n    def __init__(self, message, errors):            \n        # Call the base class constructor with the parameters it needs\n        super().__init__(message)\n            \n        # Now for your custom code...\n        self.errors = errors\n\nThat way you could pass dict of error messages to the second param, and get to it later with e.errors.\nIn Python 2, you have to use this slightly more complex form of super():\nsuper(ValidationError, self).__init__(message)",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 1319615,
      "answer_id": 1319675,
      "question_score": 1959,
      "answer_score": 2055,
      "is_accepted": true,
      "tags": [
        "python",
        "python-3.x",
        "exception",
        "python-dataclasses",
        "pep352"
      ]
    }
  },
  {
    "prompt": "fatal error: Python.h: No such file or directory\n\nI am trying to build a shared library using a C extension file but first I have to generate the output file using the command below:\ngcc -Wall utilsmodule.c -o Utilc\n\nAfter executing the command, I get this error message:\n> utilsmodule.c:1:20: fatal error: Python.h: No such file or directory\ncompilation terminated.\n\nI have tried all the suggested solutions over the internet but the problem still exists. I have no problem with Python.h. I managed to locate the file on my machine.",
    "response": "I managed to solve this issue and generate the .so file in one command \n\ngcc -shared -o UtilcS.so\n-fPIC -I/usr/include/python2.7 -lpython2.7  utilsmodule.c",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 21530577,
      "answer_id": 21548557,
      "question_score": 1938,
      "answer_score": 17,
      "is_accepted": true,
      "tags": [
        "python",
        "gcc",
        "python-c-api"
      ]
    }
  },
  {
    "prompt": "Generate random integers between 0 and 9\n\nHow can I generate random integers between 0 and 9 (inclusive) in Python?\nFor example, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9",
    "response": "Try random.randrange:\nfrom random import randrange\nprint(randrange(10))",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 3996904,
      "answer_id": 3996930,
      "question_score": 1929,
      "answer_score": 2649,
      "is_accepted": true,
      "tags": [
        "python",
        "random",
        "integer"
      ]
    }
  },
  {
    "prompt": "What is the meaning of single and double underscore before an object name?\n\nWhat do single and double leading underscores before an object's name represent in Python?",
    "response": "Single Underscore\nIn a class, names with a leading underscore indicate to other programmers that the attribute or method is intended to be be used inside that class. However, privacy is not enforced in any way.\nUsing leading underscores for functions in a module indicates it should not be imported from somewhere else.\nFrom the PEP-8 style guide:\n\n_single_leading_underscore: weak \"internal use\" indicator. E.g. from M import * does not import objects whose name starts with an underscore.\n\nDouble Underscore (Name Mangling)\nFrom the Python docs:\n\nAny identifier of the form __spam (at least two leading underscores, at most one trailing underscore) is textually replaced with _classname__spam, where classname is the current class name with leading underscore(s) stripped. This mangling is done without regard to the syntactic position of the identifier, so it can be used to define class-private instance and class variables, methods, variables stored in globals, and even variables stored in inst",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 1301346,
      "answer_id": 1301369,
      "question_score": 1914,
      "answer_score": 1676,
      "is_accepted": true,
      "tags": [
        "python",
        "oop",
        "naming-conventions",
        "identifier"
      ]
    }
  },
  {
    "prompt": "Relative imports for the billionth time\n\nI've been here:\n\nPEP 328 – Imports: Multi-Line and Absolute/Relative\nModules, Packages\nPython packages: relative imports\nPython relative import example code does not work\nRelative imports in Python 2.5\nRelative imports in Python\nPython: Disabling relative import\n\nand plenty of URLs that I did not copy, some on SO, some on other sites, back when I thought I'd have the solution quickly.\nThe forever-recurring question is this: how do I solve this \"Attempted relative import in non-package\" message?\n",
    "response": "Script vs. Module\nHere's an explanation.  The short version is that there is a big difference between directly running a Python file, and importing that file from somewhere else.  Just knowing what directory a file is in does not determine what package Python thinks it is in.  That depends, additionally, on how you load the file into Python (by running or by importing).\nThere are two ways to load a Python file: as the top-level script, or as a\nmodule.  A file is loaded as the top-level script if you execute it directly, for instance by typing python myfile.py on the command line.  It is loaded as a module when an import statement is encountered inside some other file.  There can only be one top-level script at a time; the top-level script is the Python file you ran to start things off.\nNaming\nWhen a file is loaded, it is given a name (which is stored in its __name__ attribute).  If it was loaded as the top-level script, its name is __main__.  If it was loaded as a module, its name is t",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 14132789,
      "answer_id": 14132912,
      "question_score": 1889,
      "answer_score": 2414,
      "is_accepted": true,
      "tags": [
        "python",
        "python-import",
        "relative-path",
        "python-packaging",
        "relative-import"
      ]
    }
  },
  {
    "prompt": "Relative imports in Python 3\n\nI want to import a function from another file in the same directory.\nUsually, one of the following works:\nfrom .mymodule import myfunction\n\nfrom mymodule import myfunction\n\n...but the other one gives me one of these errors:\nImportError: attempted relative import with no known parent package\n\nModuleNotFoundError: No module named 'mymodule'\n\nSystemError: Parent module '' not loaded, cannot perform relative import\n\nWhy is this?",
    "response": "unfortunately, this module needs to be inside the package, and it also\nneeds to be runnable as a script, sometimes. Any idea how I could\nachieve that?\n\nIt's quite common to have a layout like this...\nmain.py\nmypackage/\n    __init__.py\n    mymodule.py\n    myothermodule.py\n\n...with a mymodule.py like this...\n#!/usr/bin/env python3\n\n# Exported function\ndef as_int(a):\n    return int(a)\n\n# Test function for module  \ndef _test():\n    assert as_int('1') == 1\n\nif __name__ == '__main__':\n    _test()\n\n...a myothermodule.py like this...\n#!/usr/bin/env python3\n\nfrom .mymodule import as_int\n\n# Exported function\ndef add(a, b):\n    return as_int(a) + as_int(b)\n\n# Test function for module  \ndef _test():\n    assert add('1', '1') == 2\n\nif __name__ == '__main__':\n    _test()\n\n...and a main.py like this...\n#!/usr/bin/env python3\n\nfrom mypackage.myothermodule import add\n\ndef main():\n    print(add('1', '1'))\n\nif __name__ == '__main__':\n    main()\n\n...which works fine when you run main.py or mypackage/mymodu",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 16981921,
      "answer_id": 16985066,
      "question_score": 1874,
      "answer_score": 1432,
      "is_accepted": true,
      "tags": [
        "python",
        "python-3.x",
        "python-import"
      ]
    }
  },
  {
    "prompt": "How do I write JSON data to a file?\n\nHow do I write JSON data stored in the dictionary data to a file?\nf = open('data.json', 'wb')\nf.write(data)\n\nThis gives the error:\n\nTypeError: must be string or buffer, not dict",
    "response": "data is a Python dictionary. It needs to be encoded as JSON before writing.\nUse this for maximum compatibility (Python 2 and 3):\nimport json\nwith open('data.json', 'w') as f:\n    json.dump(data, f)\n\nOn a modern system (i.e. Python 3 and UTF-8 support), you can write a nicer file using:\nimport json\nwith open('data.json', 'w', encoding='utf-8') as f:\n    json.dump(data, f, ensure_ascii=False, indent=4)\n\nSee json documentation.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 12309269,
      "answer_id": 12309296,
      "question_score": 1872,
      "answer_score": 3330,
      "is_accepted": true,
      "tags": [
        "python",
        "json"
      ]
    }
  },
  {
    "prompt": "Use different Python version with virtualenv\n\nHow do I create a virtual environment for a specified version of Python?",
    "response": "NOTE: For Python 3.3+, see The Aelfinn's answer below.\n\nUse the --python (or short -p) option when creating a virtualenv instance to specify the Python executable you want to use, e.g.:\nvirtualenv --python=\"/usr/bin/python2.6\" \"/path/to/new/virtualenv/\"",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 1534210,
      "answer_id": 1534343,
      "question_score": 1872,
      "answer_score": 2066,
      "is_accepted": true,
      "tags": [
        "python",
        "virtualenv",
        "virtualenvwrapper"
      ]
    }
  },
  {
    "prompt": "Extracting extension from filename\n\nIs there a function to extract the extension from a filename?",
    "response": "Use os.path.splitext:\n>>> import os\n>>> filename, file_extension = os.path.splitext('/path/to/somefile.ext')\n>>> filename\n'/path/to/somefile'\n>>> file_extension\n'.ext'\n\nUnlike most manual string-splitting attempts, os.path.splitext will correctly treat /a/b.c/d as having no extension instead of having extension .c/d, and it will treat .bashrc as having no extension instead of having extension .bashrc:\n>>> os.path.splitext('/a/b.c/d')\n('/a/b.c/d', '')\n>>> os.path.splitext('.bashrc')\n('.bashrc', '')",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 541390,
      "answer_id": 541394,
      "question_score": 1869,
      "answer_score": 2678,
      "is_accepted": true,
      "tags": [
        "python",
        "filenames",
        "file-extension"
      ]
    }
  },
  {
    "prompt": "How can I import a module dynamically given the full path?\n\nHow do I load a Python module given its full path?\nNote that the file can be anywhere in the filesystem where the user has access rights.\n\nSee also: How to import a module given its name as string?",
    "response": "Let's have MyClass in module.name module defined at /path/to/file.py. Below is how we import MyClass from this module\nFor Python 3.5+ use (docs):\nimport importlib.util\nimport sys\nspec = importlib.util.spec_from_file_location(\"module.name\", \"/path/to/file.py\")\nfoo = importlib.util.module_from_spec(spec)\nsys.modules[\"module.name\"] = foo\nspec.loader.exec_module(foo)\nfoo.MyClass()\n\nFor Python 3.3 and 3.4 use:\nfrom importlib.machinery import SourceFileLoader\n\nfoo = SourceFileLoader(\"module.name\", \"/path/to/file.py\").load_module()\nfoo.MyClass()\n\n(Although this has been deprecated in Python 3.4.)\nFor Python 2 use:\nimport imp\n\nfoo = imp.load_source('module.name', '/path/to/file.py')\nfoo.MyClass()\n\nThere are equivalent convenience functions for compiled Python files and DLLs.\nSee also http://bugs.python.org/issue21436.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 67631,
      "answer_id": 67692,
      "question_score": 1868,
      "answer_score": 1815,
      "is_accepted": true,
      "tags": [
        "python",
        "python-import",
        "python-module"
      ]
    }
  },
  {
    "prompt": "How do I get time of a Python program&#39;s execution?\n\nI have a command line program in Python that takes a while to finish. I want to know the exact time it takes to finish running.\n\nI've looked at the timeit module, but it seems it's only for small snippets of code. I want to time the whole program.",
    "response": "The simplest way in Python:\n\nimport time\nstart_time = time.time()\nmain()\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\n\nThis assumes that your program takes at least a tenth of second to run.\n\nPrints:\n\n--- 0.764891862869 seconds ---",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 1557571,
      "answer_id": 1557584,
      "question_score": 1825,
      "answer_score": 3104,
      "is_accepted": true,
      "tags": [
        "python",
        "time",
        "execution-time"
      ]
    }
  },
  {
    "prompt": "Random string generation with upper case letters and digits\n\nHow do I generate a string of size N, made of numbers and uppercase English letters such as:\n\n6U1S75\n4Z4UKK\nU911K4",
    "response": "Answer in one line:\n''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(N))\n\nor even shorter starting with Python 3.6 using random.choices():\n''.join(random.choices(string.ascii_uppercase + string.digits, k=N))\n\nA cryptographically more secure version: see this post\n''.join(random.SystemRandom().choice(string.ascii_uppercase + string.digits) for _ in range(N))\n\nIn details, with a clean function for further reuse:\n>>> import string\n>>> import random\n>>> def id_generator(size=6, chars=string.ascii_uppercase + string.digits):\n...    return ''.join(random.choice(chars) for _ in range(size))\n...\n>>> id_generator()\n'G5G74W'\n>>> id_generator(3, \"6793YUIO\")\n'Y3U'\n\nHow does it work ?\nWe import string, a module that contains sequences of common ASCII characters, and random, a module that deals with random generation.\nstring.ascii_uppercase + string.digits just concatenates the list of characters representing uppercase ASCII chars and digits:\n>>> string.ascii_uppercase\n'A",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 2257441,
      "answer_id": 2257449,
      "question_score": 1811,
      "answer_score": 3299,
      "is_accepted": true,
      "tags": [
        "python",
        "string",
        "random"
      ]
    }
  },
  {
    "prompt": "How do I print to stderr in Python?\n\nThere are several ways to write to stderr:\nprint >> sys.stderr, \"spam\"  # Python 2 only.\n\nsys.stderr.write(\"spam\\n\")\n\nos.write(2, b\"spam\\n\")\n\nfrom __future__ import print_function\nprint(\"spam\", file=sys.stderr)\n\nWhat are the differences between these methods? Which method should be preferred?",
    "response": "I found this to be the only one short, flexible, portable and readable:\nimport sys\n\ndef eprint(*args, **kwargs):\n    print(*args, file=sys.stderr, **kwargs)\n\nThe optional function eprint saves some repetition. It can be used in the same way as the standard print function:\n>>> print(\"Test\")\nTest\n>>> eprint(\"Test\")\nTest\n>>> eprint(\"foo\", \"bar\", \"baz\", sep=\"---\")\nfoo---bar---baz",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 5574702,
      "answer_id": 14981125,
      "question_score": 1810,
      "answer_score": 1638,
      "is_accepted": true,
      "tags": [
        "python",
        "printing",
        "stderr"
      ]
    }
  },
  {
    "prompt": "How do I read from stdin?\n\nHow do I read from standard input (stdin)?",
    "response": "Use the fileinput module:\nimport fileinput\n\nfor line in fileinput.input():\n    pass\n\nfileinput will loop through all the lines in the input specified as file names given in command-line arguments, or the standard input if no arguments are provided.\nNote: line will contain a trailing newline; to remove it use line.rstrip().",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 1450393,
      "answer_id": 1454400,
      "question_score": 1807,
      "answer_score": 1148,
      "is_accepted": false,
      "tags": [
        "python",
        "stdin"
      ]
    }
  },
  {
    "prompt": "Save plot to image file instead of displaying it\n\nThis displays the figure in a GUI:\nimport matplotlib.pyplot as plt\nplt.plot([1, 2, 3], [1, 4, 9])\nplt.show()\n\nBut how do I instead save the figure to a file (e.g. foo.png)?",
    "response": "When using matplotlib.pyplot.savefig, the file format can be specified by the extension:\nfrom matplotlib import pyplot as plt\n\nplt.savefig('foo.png')\nplt.savefig('foo.pdf')\n\nThat gives a rasterized or vectorized output respectively.\nIn addition, there is sometimes undesirable whitespace around the image, which can be removed with:\nplt.savefig('foo.png', bbox_inches='tight')\n\nNote that if showing the plot, plt.show() should follow plt.savefig(); otherwise, the file image will be blank.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 9622163,
      "answer_id": 9890599,
      "question_score": 1799,
      "answer_score": 2283,
      "is_accepted": true,
      "tags": [
        "python",
        "matplotlib",
        "savefig"
      ]
    }
  },
  {
    "prompt": "What is the best way of implementing a singleton in Python?\n\nI have multiple classes which would become singletons (my use case is for a logger, but this is not important). I do not wish to clutter several classes with added gumph when I can simply inherit or decorate.\nBest methods:\n\nMethod 1: A decorator\ndef singleton(class_):\n    instances = {}\n    def getinstance(*args, **kwargs):\n        if class_ not in instances:\n            instances[class_] = class_(*args, **kwargs)\n        return instances[class_]\n    return getinstance\n\n@singleton\nclass MyClass(",
    "response": "You just need a decorator, different depending on the python version.\nNotice how foo gets printed only once.\n\nPython 3.2+\nImplementation:\nfrom functools import lru_cache\n\n@lru_cache(maxsize=None)\nclass CustomClass(object):\n\n    def __init__(self, arg):\n        print(f\"CustomClass initialised with {arg}\")\n        self.arg = arg\n\nUsage\nc1 = CustomClass(\"foo\")\nc2 = CustomClass(\"foo\")\nc3 = CustomClass(\"bar\")\n\nprint(c1 == c2)\nprint(c1 == c3)\n\nOutput\n>>> CustomClass initialised with foo\n>>> CustomClass initialised with bar\n>>> True\n>>> False\n\n\nPython 3.9+\nImplementation:\nfrom functools import cache\n\n@cache\nclass CustomClass(object):\n    ...",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 6760685,
      "answer_id": 73988733,
      "question_score": 1798,
      "answer_score": 29,
      "is_accepted": true,
      "tags": [
        "python",
        "singleton",
        "decorator",
        "base-class",
        "metaclass"
      ]
    }
  },
  {
    "prompt": "Selecting multiple columns in a Pandas dataframe\n\nHow do I select columns a and b from df, and save them into a new dataframe df1?\nindex  a   b   c\n1      2   3   4\n2      3   4   5\n\nUnsuccessful attempt:\ndf1 = df['a':'b']\ndf1 = df.ix[:, 'a':'b']",
    "response": "The column names (which are strings) cannot be sliced in the manner you tried.\nHere you have a couple of options. If you know from context which variables you want to slice out, you can just return a view of only those columns by passing a list into the __getitem__ syntax (the []'s).\ndf1 = df[['a', 'b']]\n\nAlternatively, if it matters to index them numerically and not by their name (say your code should automatically do this without knowing the names of the first two columns) then you can do this instead:\ndf1 = df.iloc[:, 0:2] # Remember that Python does not slice inclusive of the ending index.\n\nAdditionally, you should familiarize yourself with the idea of a view into a Pandas object vs. a copy of that object. The first of the above methods will return a new copy in memory of the desired sub-object (the desired slices).\nSometimes, however, there are indexing conventions in Pandas that don't do this and instead give you a new variable that just refers to the same chunk of memory as the ",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 11285613,
      "answer_id": 11287278,
      "question_score": 1796,
      "answer_score": 2738,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "select",
        "indexing"
      ]
    }
  },
  {
    "prompt": "Make a dictionary (dict) from separate lists of keys and values\n\nI want to combine these:\nkeys = ['name', 'age', 'food']\nvalues = ['Monty', 42, 'spam']\n\ninto a single dictionary:\n{'name': 'Monty', 'age': 42, 'food': 'spam'}\n\nHow can I do this?",
    "response": "Like this:\nkeys = ['a', 'b', 'c']\nvalues = [1, 2, 3]\ndictionary = dict(zip(keys, values))\nprint(dictionary) # {'a': 1, 'b': 2, 'c': 3}\n\nVoila :-)  The pairwise dict constructor and zip function are awesomely useful.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 209840,
      "answer_id": 209854,
      "question_score": 1792,
      "answer_score": 2988,
      "is_accepted": true,
      "tags": [
        "python",
        "list",
        "dictionary",
        "key"
      ]
    }
  },
  {
    "prompt": "What does __all__ mean in Python?\n\nI see __all__ in __init__.py files. What does it do?",
    "response": "It's a list of public objects of that module, as interpreted by import *. It overrides the default of hiding everything that begins with an underscore.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 44834,
      "answer_id": 44842,
      "question_score": 1788,
      "answer_score": 883,
      "is_accepted": true,
      "tags": [
        "python",
        "syntax",
        "namespaces",
        "python-import"
      ]
    }
  },
  {
    "prompt": "How do I get the filename without the extension from a path in Python?\n\nHow do I get the filename without the extension from a path in Python?\n\"/path/to/some/file.txt\"  →  \"file\"",
    "response": "Python 3.4+\nUse pathlib.Path.stem\n>>> from pathlib import Path\n>>> Path(\"/path/to/file.txt\").stem\n'file'\n>>> Path(\"/path/to/file.tar.gz\").stem\n'file.tar'\n\nPython < 3.4\nUse os.path.splitext in combination with os.path.basename:\n>>> os.path.splitext(os.path.basename(\"/path/to/file.txt\"))[0]\n'file'\n>>> os.path.splitext(os.path.basename(\"/path/to/file.tar.gz\"))[0]\n'file.tar'",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 678236,
      "answer_id": 678242,
      "question_score": 1786,
      "answer_score": 1927,
      "is_accepted": true,
      "tags": [
        "python",
        "string",
        "path"
      ]
    }
  },
  {
    "prompt": "How do I profile a Python script?\n\nProject Euler and other coding contests often have a maximum time to run or people boast of how fast their particular solution runs. With Python, sometimes the approaches are somewhat kludgey - i.e., adding timing code to __main__.\n\nWhat is a good way to profile how long a Python program takes to run?",
    "response": "Python includes a profiler called cProfile. It not only gives the total running time, but also times each function separately, and tells you how many times each function was called, making it easy to determine where you should make optimizations.\nYou can call it from within your code, or from the interpreter, like this:\nimport cProfile\ncProfile.run('foo()')\n\nEven more usefully, you can invoke cProfile when running a script:\npython -m cProfile myscript.py\n\nOr when running a module:\npython -m cProfile -m mymodule\n\nTo make it even easier, I made a little batch file called 'profile.bat':\npython -m cProfile %1\n\nSo all I have to do is run:\nprofile euler048.py\n\nAnd I get this:\n1007 function calls in 0.061 CPU seconds\n\nOrdered by: standard name\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\n    1    0.000    0.000    0.061    0.061 <string>:1(<module>)\n 1000    0.051    0.000    0.051    0.000 euler048.py:2(<lambda>)\n    1    0.005    0.005    0.061    0.061 euler048.py:2",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 582336,
      "answer_id": 582337,
      "question_score": 1751,
      "answer_score": 1816,
      "is_accepted": true,
      "tags": [
        "python",
        "performance",
        "optimization",
        "time-complexity",
        "profiling"
      ]
    }
  },
  {
    "prompt": "if/else in a list comprehension\n\nHow do I convert the following for-loop containing an if/else into a list comprehension?\nresults = []\nfor x in xs:\n    results.append(f(x) if x is not None else '')\n\nIt should yield '' if x is None, and otherwise f(x).\nI tried:\n[f(x) for x in xs if x is not None else '']\n\nbut it gives a SyntaxError. What is the correct syntax?\n\nSee Does Python have a ternary conditional operator? for info on ... if ... else ....\nSee List comprehension with condition for omitting values based on a condition: [...",
    "response": "You can totally do that. It's just an ordering issue:\n[f(x) if x is not None else '' for x in xs]\n\nIn general,\n[f(x) if condition else g(x) for x in sequence]\n\nAnd, for list comprehensions with if conditions only,\n[f(x) for x in sequence if condition]\n\nNote that this actually uses a different language construct, a conditional expression, which itself is not part of the comprehension syntax, while the if after the for…in is part of list comprehensions and used to filter elements from the source iterable.\n\nConditional expressions can be used in all kinds of situations where you want to choose between two expression values based on some condition. This does the same as the ternary operator ?: that exists in other languages. For example:\nvalue = 123\nprint(value, 'is', 'even' if value % 2 == 0 else 'odd')",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 4260280,
      "answer_id": 4260304,
      "question_score": 1741,
      "answer_score": 2912,
      "is_accepted": true,
      "tags": [
        "python",
        "list",
        "if-statement",
        "list-comprehension"
      ]
    }
  },
  {
    "prompt": "How to put the legend outside the plot\n\nI have a series of 20 plots (not subplots) to be made in a single figure. I want the legend to be outside of the box. At the same time, I do not want to change the axes, as the size of the figure gets reduced.\n\nI want to keep the legend box outside the plot area (I want the legend to be outside at the right side of the plot area).\nIs there a way to reduce the font size of the text inside the legend box, so that the size of the legend box will be small?",
    "response": "You can make the legend text smaller by specifying set_size of FontProperties.\nResources:\n\nLegend guide\nmatplotlib.legend\nmatplotlib.pyplot.legend\nmatplotlib.font_manager\n\nset_size(self, size)\nValid font size are xx-small, x-small, small, medium, large, x-large, xx-large, larger, smaller, and None.\n\n\nReal Python: Python Plotting With Matplotlib (Guide)\n\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.font_manager import FontProperties\n\nfontP = FontProperties()\nfontP.set_size('xx-small')\n\np1, = plt.plot([1, 2, 3], label='Line 1')\np2, = plt.plot([3, 2, 1], label='Line 2')\nplt.legend(handles=[p1, p2], title='title', bbox_to_anchor=(1.05, 1), loc='upper left', prop=fontP)\n\n\n\nfontsize='xx-small' also works, without importing FontProperties.\n\nplt.legend(handles=[p1, p2], title='title', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='xx-small')",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 4700614,
      "answer_id": 4700674,
      "question_score": 1723,
      "answer_score": 204,
      "is_accepted": true,
      "tags": [
        "python",
        "matplotlib",
        "seaborn",
        "legend"
      ]
    }
  },
  {
    "prompt": "How to check for NaN values\n\nfloat('nan') represents NaN (not a number). But how do I check for it?",
    "response": "Use math.isnan:\n>>> import math\n>>> x = float('nan')\n>>> math.isnan(x)\nTrue",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 944700,
      "answer_id": 944733,
      "question_score": 1720,
      "answer_score": 2200,
      "is_accepted": true,
      "tags": [
        "python",
        "math",
        "nan"
      ]
    }
  },
  {
    "prompt": "Replacements for switch statement in Python?\n\nI want to write a function in Python that returns different fixed values based on the value of an input index.  \n\nIn other languages I would use a switch or case statement, but Python does not appear to have a switch statement.  What are the recommended Python solutions in this scenario?",
    "response": "Python 3.10 (2021) introduced the match-case statement, which provides a first-class implementation of a \"switch\" for Python. For example:\ndef f(x):\n    match x:\n        case 'a':\n            return 1\n        case 'b':\n            return 2\n        case _:\n            return 0   # 0 is the default case if x is not found\n\nThe match-case statement is considerably more powerful than this simple example.\nDocumentation:\n\nmatch statements (under the \"More Control Flow Tools\" page)\nThe match statement (under \"Compound statements\" page)\nPEP 634 – Structural Pattern Matching: Specification\nPEP 636 – Structural Pattern Matching: Tutorial\n\n\nIf you need to support Python ≤ 3.9, use a dictionary instead:\ndef f(x):\n    return {\n        'a': 1,\n        'b': 2,\n    }.get(x, 0)  # default case",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 60208,
      "answer_id": 60211,
      "question_score": 1716,
      "answer_score": 2239,
      "is_accepted": true,
      "tags": [
        "python",
        "switch-statement"
      ]
    }
  },
  {
    "prompt": "Why do Python classes inherit object?\n\nWhy does the following class declaration inherit from object?\nclass MyClass(object):\n    ...",
    "response": "Is there any reason for a class declaration to inherit from object?\n\n\nIn Python 3, apart from compatibility between Python 2 and 3, no reason. In Python 2, many reasons. \n\n\n\nPython 2.x story:\n\nIn Python 2.x (from 2.2 onwards) there's two styles of classes depending on the presence or absence of object as a base-class:\n\n\n\"classic\" style classes: they don't have object as a base class:\n\n>>> class ClassicSpam:      # no base class\n...     pass\n>>> ClassicSpam.__bases__\n()\n\n\"new\" style classes: they have, directly or indirectly (e.g inherit from a built-in type), object as a base class:\n\n>>> class NewSpam(object):           # directly inherit from object\n...    pass\n>>> NewSpam.__bases__\n(<type 'object'>,)\n>>> class IntSpam(int):              # indirectly inherit from object...\n...    pass\n>>> IntSpam.__bases__\n(<type 'int'>,) \n>>> IntSpam.__bases__[0].__bases__   # ... because int inherits from object  \n(<type 'object'>,)\n\n\n\nWithout a doubt, when writing a class you'll always want to go f",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 4015417,
      "answer_id": 45062077,
      "question_score": 1713,
      "answer_score": 1261,
      "is_accepted": true,
      "tags": [
        "python",
        "class",
        "oop",
        "object",
        "inheritance"
      ]
    }
  },
  {
    "prompt": "How do I reverse a string in Python?\n\nThere is no built in reverse method for Python's str object. How can I reverse a string?",
    "response": "Using slicing:\n>>> 'hello world'[::-1]\n'dlrow olleh'\n\n\nSlice notation takes the form [start:stop:step]. In this case, we omit the start and stop positions since we want the whole string. We also use step = -1, which means, \"repeatedly step from right to left by 1 character\".",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 931092,
      "answer_id": 931095,
      "question_score": 1705,
      "answer_score": 3140,
      "is_accepted": true,
      "tags": [
        "python",
        "string"
      ]
    }
  },
  {
    "prompt": "How to change the order of DataFrame columns?\n\nI have the following DataFrame (df):\nimport numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.rand(10, 5))\n\nI add more column(s) by assignment:\ndf['mean'] = df.mean(1)\n\nHow can I move the column mean to the front, i.e. set it as first column leaving the order of the other columns untouched?",
    "response": "One easy way would be to reassign the dataframe with a list of the columns, rearranged as needed. \n\nThis is what you have now: \n\nIn [6]: df\nOut[6]:\n          0         1         2         3         4      mean\n0  0.445598  0.173835  0.343415  0.682252  0.582616  0.445543\n1  0.881592  0.696942  0.702232  0.696724  0.373551  0.670208\n2  0.662527  0.955193  0.131016  0.609548  0.804694  0.632596\n3  0.260919  0.783467  0.593433  0.033426  0.512019  0.436653\n4  0.131842  0.799367  0.182828  0.683330  0.019485  0.363371\n5  0.498784  0.873495  0.383811  0.699289  0.480447  0.587165\n6  0.388771  0.395757  0.745237  0.628406  0.784473  0.588529\n7  0.147986  0.459451  0.310961  0.706435  0.100914  0.345149\n8  0.394947  0.863494  0.585030  0.565944  0.356561  0.553195\n9  0.689260  0.865243  0.136481  0.386582  0.730399  0.561593\n\nIn [7]: cols = df.columns.tolist()\n\nIn [8]: cols\nOut[8]: [0L, 1L, 2L, 3L, 4L, 'mean']\n\n\nRearrange cols in any way you want. This is how I moved the last element to the f",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 13148429,
      "answer_id": 13148611,
      "question_score": 1681,
      "answer_score": 1433,
      "is_accepted": false,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "sorting",
        "indexing"
      ]
    }
  },
  {
    "prompt": "How do I install pip on macOS or OS X?\n\nI spent most of the day yesterday searching for a clear answer for installing pip (package manager for Python). I can't find a good solution.\n\nHow do I install it?",
    "response": "On Linux or MacOS:\npython -m ensurepip --upgrade\n\nIf you want to install pip for Python 3, replace python with python3.\nSee https://pip.pypa.io/en/stable/installation/ for more details.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 17271319,
      "answer_id": 18947390,
      "question_score": 1671,
      "answer_score": 3496,
      "is_accepted": false,
      "tags": [
        "python",
        "macos",
        "pip",
        "installation"
      ]
    }
  },
  {
    "prompt": "How do I check if a directory exists in Python?\n\nHow do I check if a directory exists in Python?",
    "response": "Use os.path.isdir for directories only:\n>>> import os\n>>> os.path.isdir('new_folder')\nTrue\n\nUse os.path.exists for both files and directories:\n>>> import os\n>>> os.path.exists(os.path.join(os.getcwd(), 'new_folder', 'file.txt'))\nFalse\n\nAlternatively, you can use pathlib:\n >>> from pathlib import Path\n >>> Path('new_folder').is_dir()\n True\n >>> (Path.cwd() / 'new_folder' / 'file.txt').exists()\n False",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 8933237,
      "answer_id": 8933290,
      "question_score": 1670,
      "answer_score": 2428,
      "is_accepted": true,
      "tags": [
        "python",
        "directory"
      ]
    }
  },
  {
    "prompt": "What is the Python 3 equivalent of &quot;python -m SimpleHTTPServer&quot;\n\nWhat is the Python 3 equivalent of python -m SimpleHTTPServer?",
    "response": "From the docs:\n\n\n  The SimpleHTTPServer module has been merged into http.server in Python 3.0. The 2to3 tool will automatically adapt imports when converting your sources to 3.0.\n\n\nSo, your command is python -m http.server, or depending on your installation, it can be:\n\npython3 -m http.server",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 7943751,
      "answer_id": 7943768,
      "question_score": 1651,
      "answer_score": 2304,
      "is_accepted": true,
      "tags": [
        "python",
        "python-3.x",
        "httpserver",
        "simplehttpserver"
      ]
    }
  },
  {
    "prompt": "What is setup.py?\n\nWhat is setup.py and how can it be configured or used?",
    "response": "setup.py is a Python file, the presence of which is an indication that the module/package you are about to install has likely been packaged and distributed with Distutils, which is the standard for distributing Python Modules.\nThis allows you to easily install Python packages. Often it's enough to write:\n$ pip install . \n\npip will use setup.py to install your module. Avoid calling setup.py directly.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 1471994,
      "answer_id": 1472014,
      "question_score": 1650,
      "answer_score": 1223,
      "is_accepted": true,
      "tags": [
        "python",
        "pypi",
        "setup.py",
        "python-packaging"
      ]
    }
  },
  {
    "prompt": "How can I flush the output of the print function?\n\nHow do I force Python's print function to flush the buffered output to the screen?\n\nSee also: Disable output buffering if the goal is to change the buffering behaviour generally. This question is about explicitly flushing output after a specific print call, even though output is still being buffered.\nFor duplicate closers: if a beginner is asking a question about trying to make output appear immediately while not using a newline at the end, please instead use Why doesn't print output show up imm",
    "response": "In Python 3, print can take an optional flush argument:\nprint(\"Hello, World!\", flush=True)\n\nIn Python 2, after calling print, do:\nimport sys\nsys.stdout.flush()\n\nBy default, print prints to sys.stdout (see the documentation for more about file objects).",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 230751,
      "answer_id": 230774,
      "question_score": 1636,
      "answer_score": 1947,
      "is_accepted": true,
      "tags": [
        "python",
        "printing",
        "flush",
        "output-buffering"
      ]
    }
  },
  {
    "prompt": "Create a dictionary with comprehension\n\nCan I use list comprehension syntax to create a dictionary?\nFor example, by iterating over pairs of keys and values:\nd = {... for k, v in zip(keys, values)}",
    "response": "Use a dict comprehension (Python 2.7 and later):\n{key: value for key, value in zip(keys, values)}\n\n\nAlternatively, use the dict constructor:\npairs = [('a', 1), ('b', 2)]\ndict(pairs)                          # → {'a': 1, 'b': 2}\ndict((k, v + 10) for k, v in pairs)  # → {'a': 11, 'b': 12}\n\nGiven separate lists of keys and values, use the dict constructor with zip:\nkeys = ['a', 'b']\nvalues = [1, 2]\ndict(zip(keys, values))              # → {'a': 1, 'b': 2}",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 1747817,
      "answer_id": 1747827,
      "question_score": 1610,
      "answer_score": 2361,
      "is_accepted": true,
      "tags": [
        "python",
        "dictionary",
        "list-comprehension",
        "dictionary-comprehension"
      ]
    }
  },
  {
    "prompt": "Catch and print full Python exception traceback without halting/exiting the program\n\nI want to catch and log exceptions without exiting, e.g.,\ntry:\n    do_stuff()\nexcept Exception as err:\n    print(Exception, err)\n    # I want to print the entire traceback here,\n    # not just the exception name and details\n\nI want to print the exact same output that is printed when the exception is raised without the try/except intercepting the exception, and I do not want it to exit my program.",
    "response": "Some other answer have already pointed out the traceback module.\n\nPlease notice that with print_exc, in some corner cases, you will not obtain what you would expect. In Python 2.x:\n\nimport traceback\n\ntry:\n    raise TypeError(\"Oups!\")\nexcept Exception, err:\n    try:\n        raise TypeError(\"Again !?!\")\n    except:\n        pass\n\n    traceback.print_exc()\n\n\n...will display the traceback of the last exception:\n\nTraceback (most recent call last):\n  File \"e.py\", line 7, in <module>\n    raise TypeError(\"Again !?!\")\nTypeError: Again !?!\n\n\nIf you really need to access the original traceback one solution is to cache the exception infos as returned from exc_info in a local variable and display it using print_exception:\n\nimport traceback\nimport sys\n\ntry:\n    raise TypeError(\"Oups!\")\nexcept Exception, err:\n    try:\n        exc_info = sys.exc_info()\n\n        # do you usefull stuff here\n        # (potentially raising an exception)\n        try:\n            raise TypeError(\"Again !?!\")\n        except:\n",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 3702675,
      "answer_id": 16946886,
      "question_score": 1604,
      "answer_score": 873,
      "is_accepted": true,
      "tags": [
        "python",
        "exception",
        "try-catch",
        "traceback"
      ]
    }
  },
  {
    "prompt": "pg_config executable not found\n\nI am having trouble installing psycopg2. I get the following error when I try to pip install psycopg2:\n\nError: pg_config executable not found.\n\nPlease add the directory containing pg_config to the PATH\n\nor specify the full executable path with the option:\n\n\n\n    python setup.py build_ext --pg-config /path/to/pg_config build ...\n\n\n\nor with the pg_config option in 'setup.cfg'.\n\n----------------------------------------\nCommand python setup.py egg_info failed with error code 1 in /tmp/pip-build/psyc",
    "response": "pg_config is in postgresql-devel (libpq-dev in Debian/Ubuntu, libpq-devel on Centos/Fedora/Cygwin/Babun.)",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 11618898,
      "answer_id": 12037133,
      "question_score": 1602,
      "answer_score": 1498,
      "is_accepted": false,
      "tags": [
        "python",
        "pip",
        "psycopg2"
      ]
    }
  },
  {
    "prompt": "Convert integer to string in Python\n\nHow do I convert an integer to a string?\n42   ⟶   \"42\"\n\n\nFor the reverse, see How do I parse a string to a float or int?. Floats can be handled similarly, but handling the decimal points can be tricky because floating-point values are not precise. See Converting a float to a string without rounding it for more specific advice.",
    "response": ">>> str(42)\n'42'\n\n>>> int('42')\n42\n\nLinks to the documentation:\n\nint()\nstr()\n\nstr(x) converts any object x to a string by calling x.__str__(), or repr(x) if x doesn't have a __str__() method.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 961632,
      "answer_id": 961638,
      "question_score": 1595,
      "answer_score": 2355,
      "is_accepted": true,
      "tags": [
        "python",
        "string",
        "integer"
      ]
    }
  },
  {
    "prompt": "Use of *args and **kwargs\n\nSo I have difficulty with the concept of *args and **kwargs.\n\nSo far I have learned that:\n\n\n*args = list of arguments - as positional arguments\n**kwargs = dictionary - whose keys become separate keyword arguments and the values become values of these arguments.\n\n\nI don't understand what programming task this would be helpful for. \n\nMaybe:\n\nI think to enter lists and dictionaries as arguments of a function AND at the same time as a wildcard, so I can pass ANY argument?\n\nIs there a simple example ",
    "response": "The syntax is the * and **.  The names *args and **kwargs are only by convention but there's no hard requirement to use them.\n\nYou would use *args when you're not sure how many arguments might be passed to your function, i.e. it allows you pass an arbitrary number of arguments to your function.  For example:\n\n>>> def print_everything(*args):\n        for count, thing in enumerate(args):\n...         print( '{0}. {1}'.format(count, thing))\n...\n>>> print_everything('apple', 'banana', 'cabbage')\n0. apple\n1. banana\n2. cabbage\n\n\nSimilarly, **kwargs allows you to handle named arguments that you have not defined in advance:\n\n>>> def table_things(**kwargs):\n...     for name, value in kwargs.items():\n...         print( '{0} = {1}'.format(name, value))\n...\n>>> table_things(apple = 'fruit', cabbage = 'vegetable')\ncabbage = vegetable\napple = fruit\n\n\nYou can use these along with named arguments too.  The explicit arguments get values first and then everything else is passed to *args and **kwargs.  Th",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 3394835,
      "answer_id": 3394898,
      "question_score": 1588,
      "answer_score": 1820,
      "is_accepted": true,
      "tags": [
        "python",
        "args",
        "keyword-argument"
      ]
    }
  },
  {
    "prompt": "How do I sort a dictionary by key?\n\nHow do I sort a dictionary by its keys?\nExample input:\n{2:3, 1:89, 4:5, 3:0}\n\nDesired output:\n{1:89, 2:3, 3:0, 4:5}",
    "response": "Note: for Python 3.7+, see this answer\n\nStandard Python dictionaries are unordered (until Python 3.7). Even if you sorted the (key,value) pairs, you wouldn't be able to store them in a dict in a way that would preserve the ordering.\nThe easiest way is to use OrderedDict, which remembers the order in which the elements have been inserted:\nIn [1]: import collections\n\nIn [2]: d = {2:3, 1:89, 4:5, 3:0}\n\nIn [3]: od = collections.OrderedDict(sorted(d.items()))\n\nIn [4]: od\nOut[4]: OrderedDict([(1, 89), (2, 3), (3, 0), (4, 5)])\n\nNever mind the way od is printed out; it'll work as expected:\nIn [11]: od[1]\nOut[11]: 89\n\nIn [12]: od[3]\nOut[12]: 0\n\nIn [13]: for k, v in od.iteritems(): print k, v\n   ....: \n1 89\n2 3\n3 0\n4 5\n\nPython 3\nFor Python 3 users, one needs to use the .items() instead of .iteritems():\nIn [13]: for k, v in od.items(): print(k, v)\n   ....: \n1 89\n2 3\n3 0\n4 5",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 9001509,
      "answer_id": 9001529,
      "question_score": 1579,
      "answer_score": 1313,
      "is_accepted": true,
      "tags": [
        "python",
        "sorting",
        "dictionary"
      ]
    }
  },
  {
    "prompt": "Change column type in pandas\n\nI created a DataFrame from a list of lists:\ntable = [\n    ['a',  '1.2',  '4.2' ],\n    ['b',  '70',   '0.03'],\n    ['x',  '5',    '0'   ],\n]\n\ndf = pd.DataFrame(table)\n\nHow do I convert the columns to specific types? In this case, I want to convert columns 2 and 3 into floats.\nIs there a way to specify the types while converting the list to DataFrame? Or is it better to create the DataFrame first and then loop through the columns to change the dtype for each column? Ideally I would like to do this",
    "response": "You have four main options for converting types in pandas:\n\nto_numeric() - provides functionality to safely convert non-numeric types (e.g. strings) to a suitable numeric type. (See also to_datetime() and to_timedelta().)\n\nastype() - convert (almost) any type to (almost) any other type (even if it's not necessarily sensible to do so). Also allows you to convert to categorial types (very useful).\n\ninfer_objects() - a utility method to convert object columns holding Python objects to a pandas type if possible.\n\nconvert_dtypes() - convert DataFrame columns to the \"best possible\"  dtype that supports pd.NA (pandas' object to indicate a missing value).\n\n\nRead on for more detailed explanations and usage of each of these methods.\n\n1. to_numeric()\nThe best way to convert one or more columns of a DataFrame to numeric values is to use pandas.to_numeric().\nThis function will try to change non-numeric objects (such as strings) into integers or floating-point numbers as appropriate.\nBasic usage\nThe",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 15891038,
      "answer_id": 28648923,
      "question_score": 1578,
      "answer_score": 2624,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "types",
        "type-conversion"
      ]
    }
  },
  {
    "prompt": "What are the differences between type() and isinstance()?\n\nWhat are the differences between these two code snippets?\nUsing type:\nimport types\n\nif type(a) is types.DictType:\n    do_something()\nif type(b) in types.StringTypes:\n    do_something_else()\n\nUsing isinstance:\nif isinstance(a, dict):\n    do_something()\nif isinstance(b, str) or isinstance(b, unicode):\n    do_something_else()",
    "response": "To summarize the contents of other (already good!) answers, isinstance caters for inheritance (an instance of a derived class is an instance of a base class, too), while checking for equality of type does not (it demands identity of types and rejects instances of subtypes, AKA subclasses).\n\nNormally, in Python, you want your code to support inheritance, of course (since inheritance is so handy, it would be bad to stop code using yours from using it!), so isinstance is less bad than checking identity of types because it seamlessly supports inheritance.\n\nIt's not that isinstance is good, mind you—it's just less bad than checking equality of types.  The normal, Pythonic, preferred solution is almost invariably \"duck typing\": try using the argument as if it was of a certain desired type, do it in a try/except statement catching all exceptions that could arise if the argument was not in fact of that type (or any other type nicely duck-mimicking it;-), and in the except clause, try something",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 1549801,
      "answer_id": 1549854,
      "question_score": 1575,
      "answer_score": 1553,
      "is_accepted": true,
      "tags": [
        "python",
        "oop",
        "inheritance",
        "types"
      ]
    }
  },
  {
    "prompt": "How to copy a dictionary and only edit the copy\n\nI set dict2 = dict1. When I edit dict2, the original dict1 also changes. How can I avoid this?\n>>> dict1 = {\"key1\": \"value1\", \"key2\": \"value2\"}\n>>> dict2 = dict1\n>>> dict2[\"key2\"] = \"WHY?!\"\n>>> dict1\n{'key2': 'WHY?!', 'key1': 'value1'}",
    "response": "Python never implicitly copies objects. When you set dict2 = dict1, you are making them refer to the same exact dict object, so when you mutate it, all references to it keep referring to the object in its current state.\n\nIf you want to copy the dict (which is rare), you have to do so explicitly with\n\ndict2 = dict(dict1)\n\n\nor\n\ndict2 = dict1.copy()",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 2465921,
      "answer_id": 2465932,
      "question_score": 1545,
      "answer_score": 1496,
      "is_accepted": false,
      "tags": [
        "python",
        "dictionary",
        "reference"
      ]
    }
  },
  {
    "prompt": "Is there a built-in function to print all the current properties and values of an object?\n\nSo what I'm looking for here is something like PHP's print_r function.\nThis is so I can debug my scripts by seeing what's the state of the object in question.",
    "response": "You are really mixing together two different things.\n\nUse dir(), vars() or the inspect module to get what you are interested in (I use __builtins__ as an example; you can use any object instead).\n\n>>> l = dir(__builtins__)\n>>> d = __builtins__.__dict__\n\n\nPrint that dictionary however fancy you like:\n\n>>> print l\n['ArithmeticError', 'AssertionError', 'AttributeError',...\n\n\nor\n\n>>> from pprint import pprint\n>>> pprint(l)\n['ArithmeticError',\n 'AssertionError',\n 'AttributeError',\n 'BaseException',\n 'DeprecationWarning',\n...\n\n>>> pprint(d, indent=2)\n{ 'ArithmeticError': <type 'exceptions.ArithmeticError'>,\n  'AssertionError': <type 'exceptions.AssertionError'>,\n  'AttributeError': <type 'exceptions.AttributeError'>,\n...\n  '_': [ 'ArithmeticError',\n         'AssertionError',\n         'AttributeError',\n         'BaseException',\n         'DeprecationWarning',\n...\n\n\nPretty printing is also available in the interactive debugger as a command:\n\n(Pdb) pp vars()\n{'__builtins__': {'ArithmeticError': ",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 192109,
      "answer_id": 192365,
      "question_score": 1534,
      "answer_score": 821,
      "is_accepted": true,
      "tags": [
        "python",
        "debugging",
        "introspection",
        "pretty-print",
        "python-datamodel"
      ]
    }
  },
  {
    "prompt": "How to convert string to bytes in Python 3\n\nTypeError: 'str' does not support the buffer interface suggests two possible methods to convert a string to bytes:\nb = bytes(mystring, 'utf-8')\n\nb = mystring.encode('utf-8')\n\nWhat are the differences between them? Which one should I opt for and why?\n\nSee Convert bytes to a string in Python 3 for the other way around.",
    "response": "If you look at the docs for bytes, it points you to bytearray:\n\nbytearray([source[, encoding[, errors]]])\nReturn a new array of bytes. The bytearray type is a mutable sequence of integers in the range 0 <= x < 256. It has most of the usual methods of mutable sequences, described in Mutable Sequence Types, as well as most methods that the bytes type has, see Bytes and Byte Array Methods.\nThe optional source parameter can be used to initialize the array in a few different ways:\nIf it is a string, you must also give the encoding (and optionally, errors) parameters; bytearray() then converts the string to bytes using str.encode().\nIf it is an integer, the array will have that size and will be initialized with null bytes.\nIf it is an object conforming to the buffer interface, a read-only buffer of the object will be used to initialize the bytes array.\nIf it is an iterable, it must be an iterable of integers in the range 0 <= x < 256, which are used as the initial contents of the array.\nWith",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 7585435,
      "answer_id": 7585619,
      "question_score": 1533,
      "answer_score": 892,
      "is_accepted": true,
      "tags": [
        "python",
        "string",
        "character-encoding",
        "python-3.x"
      ]
    }
  },
  {
    "prompt": "How do I move a file in Python?\n\nHow can I do the equivalent of mv in Python?\nmv \"path/to/current/file.foo\" \"path/to/new/destination/for/file.foo\"",
    "response": "os.rename(), os.replace(), or shutil.move()\nAll employ the same syntax:\nimport os\nimport shutil\n\nos.rename(\"path/to/current/file.foo\", \"path/to/new/destination/for/file.foo\")\nos.replace(\"path/to/current/file.foo\", \"path/to/new/destination/for/file.foo\")\nshutil.move(\"path/to/current/file.foo\", \"path/to/new/destination/for/file.foo\")\n\n\nThe filename (\"file.foo\") must be included in both the source and destination arguments. If it differs between the two, the file will be renamed as well as moved.\nThe directory within which the new file is being created must already exist.\nOn Windows, a file with that name must not exist or an exception will be raised, but os.replace() will silently replace a file even in that occurrence.\nshutil.move simply calls os.rename in most cases. However, if the destination is on a different disk than the source, it will instead copy and then delete the source file.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 8858008,
      "answer_id": 8858026,
      "question_score": 1523,
      "answer_score": 2282,
      "is_accepted": true,
      "tags": [
        "python",
        "file",
        "file-handling",
        "python-os"
      ]
    }
  },
  {
    "prompt": "How do I use threading in Python?\n\nI would like a clear example showing tasks being divided across multiple threads.",
    "response": "Since this question was asked in 2010, there has been real simplification in how to do simple multithreading with Python with map and pool.\nThe code below comes from an article/blog post that you should definitely check out (no affiliation) - Parallelism in one line: A Better Model for Day to Day Threading Tasks. I'll summarize below - it ends up being just a few lines of code:\nfrom multiprocessing.dummy import Pool as ThreadPool\npool = ThreadPool(4)\nresults = pool.map(my_function, my_array)\n\nWhich is the multithreaded version of:\nresults = []\nfor item in my_array:\n    results.append(my_function(item))\n\n\nDescription\n\nMap is a cool little function, and the key to easily injecting parallelism into your Python code. For those unfamiliar, map is something lifted from functional languages like Lisp. It is a function which maps another function over a sequence.\nMap handles the iteration over the sequence for us, applies the function, and stores all of the results in a handy list at the end.\n",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 2846653,
      "answer_id": 28463266,
      "question_score": 1521,
      "answer_score": 1653,
      "is_accepted": false,
      "tags": [
        "python",
        "multithreading",
        "concurrency",
        "python-multithreading"
      ]
    }
  },
  {
    "prompt": "UnicodeEncodeError: &#39;ascii&#39; codec can&#39;t encode character u&#39;\\xa0&#39; in position 20: ordinal not in range(128)\n\nI'm having problems dealing with unicode characters from text fetched from different web pages (on different sites). I am using BeautifulSoup. \n\nThe problem is that the error is not always reproducible; it sometimes works with some pages, and sometimes, it barfs by throwing a UnicodeEncodeError. I have tried just about everything I can think of, and yet I have not found anything that works consistently without throwing some kind of Unicode-related error.\n\nOne of the sections of code that is caus",
    "response": "Read the Python Unicode HOWTO. This error is the very first example.\nDo not use str() to convert from unicode to encoded text / bytes.\nInstead, use .encode() to encode the string:\np.agent_info = u' '.join((agent_contact, agent_telno)).encode('utf-8').strip()\n\nor work entirely in unicode.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 9942594,
      "answer_id": 9942822,
      "question_score": 1518,
      "answer_score": 1531,
      "is_accepted": true,
      "tags": [
        "python",
        "unicode",
        "beautifulsoup",
        "python-2.x",
        "python-unicode"
      ]
    }
  },
  {
    "prompt": "Removing duplicates in lists\n\nHow can I check if a list has any duplicates and return a new list without duplicates?",
    "response": "The common approach to get a unique collection of items is to use a set. Sets are unordered collections of distinct objects. To create a set from any iterable, you can simply pass it to the built-in set() function. If you later need a real list again, you can similarly pass the set to the list() function.\nThe following example should cover whatever you are trying to do:\n>>> t = [1, 2, 3, 1, 2, 3, 5, 6, 7, 8]\n>>> list(set(t))\n[1, 2, 3, 5, 6, 7, 8]\n>>> s = [1, 2, 3]\n>>> list(set(t) - set(s))\n[8, 5, 6, 7]\n\nAs you can see from the example result, the original order is not maintained. As mentioned above, sets themselves are unordered collections, so the order is lost. When converting a set back to a list, an arbitrary order is created.\nMaintaining order\nIf order is important to you, then you will have to use a different mechanism. A very common solution for this is to rely on OrderedDict to keep the order of keys during insertion:\n>>> from collections import OrderedDict\n>>> list(OrderedDict",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 7961363,
      "answer_id": 7961390,
      "question_score": 1516,
      "answer_score": 2328,
      "is_accepted": true,
      "tags": [
        "python",
        "algorithm",
        "list",
        "duplicates",
        "intersection"
      ]
    }
  },
  {
    "prompt": "What is the use of &quot;assert&quot; in Python?\n\nWhat does assert mean? How is it used?",
    "response": "The assert statement exists in almost every programming language. It has two main uses:\n\nIt helps detect problems early in your program, where the cause is clear, rather than later when some other operation fails. A type error in Python, for example, can go through several layers of code before actually raising an Exception if not caught early on.\n\nIt works as documentation for other developers reading the code, who see the assert and can confidently say that its condition holds from now on.\n\n\nWhen you do...\nassert condition\n\n... you're telling the program to test that condition, and immediately trigger an error if the condition is false.\nIn Python, it's roughly equivalent to this:\nif not condition:\n    raise AssertionError()\n\nTry it in the Python shell:\n>>> assert True # nothing happens\n>>> assert False\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAssertionError\n\nAssertions can include an optional message, and you can disable them when running the interpret",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 5142418,
      "answer_id": 5142453,
      "question_score": 1516,
      "answer_score": 1757,
      "is_accepted": true,
      "tags": [
        "python",
        "exception",
        "assert",
        "assertion"
      ]
    }
  },
  {
    "prompt": "How do I get the full path of the current file&#39;s directory?\n\nHow do I get the current file's directory path?\nI tried:\n>>> os.path.abspath(__file__)\n'C:\\\\python27\\\\test.py'\n\nBut I want:\n'C:\\\\python27\\\\'",
    "response": "The special variable __file__ contains the path to the current file. From that we can get the directory using either pathlib or the os.path module.\nPython 3\nFor the directory of the script being run:\nimport pathlib\npathlib.Path(__file__).parent.resolve()\n\nFor the current working directory:\nimport pathlib\npathlib.Path().resolve()\n\nPython 2 and 3\nFor the directory of the script being run:\nimport os\nos.path.dirname(os.path.abspath(__file__))\n\nIf you mean the current working directory:\nimport os\nos.path.abspath(os.getcwd())\n\nNote that before and after file is two underscores, not just one.\nAlso note that if you are running interactively or have loaded code from something other than a file (eg: a database or online resource), __file__ may not be set since there is no notion of \"current file\". The above answer assumes the most common scenario of running a python script that is in a file.\nReferences\n\npathlib in the python documentation.\nos.path - Python 2.7, os.path - Python 3\nos.getcwd - Pyt",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 3430372,
      "answer_id": 3430395,
      "question_score": 1514,
      "answer_score": 2811,
      "is_accepted": true,
      "tags": [
        "python",
        "directory"
      ]
    }
  },
  {
    "prompt": "Null object in Python\n\nHow do I refer to the null object in Python?",
    "response": "In Python, the 'null' object is the singleton None.\nTo check if something is None, use the is identity operator:\nif foo is None:\n    ...",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 3289601,
      "answer_id": 3289606,
      "question_score": 1510,
      "answer_score": 1990,
      "is_accepted": true,
      "tags": [
        "python",
        "object",
        "null"
      ]
    }
  },
  {
    "prompt": "How do I print an exception in Python?\n\nHow do I print the error/exception in the except: block?\ntry:\n    ...\nexcept:\n    print(exception)",
    "response": "For Python 2.6 and later and Python 3.x:\n\nexcept Exception as e: print(e)\n\n\nFor Python 2.5 and earlier, use:\n\nexcept Exception,e: print str(e)",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 1483429,
      "answer_id": 1483488,
      "question_score": 1509,
      "answer_score": 2029,
      "is_accepted": true,
      "tags": [
        "python",
        "exception",
        "error-handling"
      ]
    }
  },
  {
    "prompt": "How to make a class JSON serializable\n\nHow to make a Python class serializable?\nclass FileItem:\n    def __init__(self, fname):\n        self.fname = fname\n\nAttempt to serialize to JSON:\n>>> import json\n>>> x = FileItem('/foo/bar')\n>>> json.dumps(x)\nTypeError: Object of type 'FileItem' is not JSON serializable",
    "response": "Do you have an idea about the expected output? For example, will this do?\n>>> f  = FileItem(\"/foo/bar\")\n>>> magic(f)\n'{\"fname\": \"/foo/bar\"}'\n\nIn that case you can merely call json.dumps(f.__dict__).\nIf you want more customized output then you will have to subclass JSONEncoder and implement your own custom serialization.\nFor a trivial example, see below.\n>>> from json import JSONEncoder\n>>> class MyEncoder(JSONEncoder):\n        def default(self, o):\n            return o.__dict__\n    \n>>> MyEncoder().encode(f)\n'{\"fname\": \"/foo/bar\"}'\n\nThen you pass this class into the json.dumps() method as cls kwarg:\njson.dumps(cls=MyEncoder)\n\nIf you also want to decode then you'll have to supply a custom object_hook to the JSONDecoder class. For example:\n>>> def from_json(json_object):\n        if 'fname' in json_object:\n            return FileItem(json_object['fname'])\n>>> f = JSONDecoder(object_hook = from_json).decode('{\"fname\": \"/foo/bar\"}')\n>>> f\n<__main__.FileItem object at 0x9337fac>\n>>>",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 3768895,
      "answer_id": 3768975,
      "question_score": 1504,
      "answer_score": 734,
      "is_accepted": true,
      "tags": [
        "python",
        "json",
        "serialization"
      ]
    }
  },
  {
    "prompt": "Why can&#39;t Python parse this JSON data?\n\nI have this JSON in a file:\n{\n    \"maps\": [\n        {\n            \"id\": \"blabla\",\n            \"iscategorical\": \"0\"\n        },\n        {\n            \"id\": \"blabla\",\n            \"iscategorical\": \"0\"\n        }\n    ],\n    \"masks\": [\n        \"id\": \"valore\"\n    ],\n    \"om_points\": \"value\",\n    \"parameters\": [\n        \"id\": \"valore\"\n    ]\n}\n\nI wrote this script to print all of the JSON data:\nimport json\nfrom pprint import pprint\n\nwith open('data.json') as f:\n    data = json.load(f)\n\npprint(data)\n\nThis ",
    "response": "Your data is not valid JSON format. You have [] when you should have {} for the \"masks\" and \"parameters\" elements:\n\n[] are for JSON arrays, which are called list in Python\n{} are for JSON objects, which are called dict in Python\n\nHere's how your JSON file should look:\n{\n    \"maps\": [\n        {\n            \"id\": \"blabla\",\n            \"iscategorical\": \"0\"\n        },\n        {\n            \"id\": \"blabla\",\n            \"iscategorical\": \"0\"\n        }\n    ],\n    \"masks\": {\n        \"id\": \"valore\"\n    },\n    \"om_points\": \"value\",\n    \"parameters\": {\n        \"id\": \"valore\"\n    }\n}\n\nThen you can use your code:\nimport json\nfrom pprint import pprint\n\nwith open('data.json') as f:\n    data = json.load(f)\n\npprint(data)\n\nWith data, you can now also find values like so:\ndata[\"maps\"][0][\"id\"]\ndata[\"masks\"][\"id\"]\ndata[\"om_points\"]\n\nTry those out and see if it starts to make sense.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 2835559,
      "answer_id": 2835672,
      "question_score": 1503,
      "answer_score": 2193,
      "is_accepted": true,
      "tags": [
        "python",
        "json",
        "parsing"
      ]
    }
  },
  {
    "prompt": "How do I check which version of Python is running my script?\n\nHow do I check which version of the Python interpreter is running my script?\n\n See Find full path of the Python interpreter (Python executable)? if you are looking to find exactly which interpreter is being used - for example, to debug a Pip installation problem, or to check which virtual environment (if any) is active.",
    "response": "This information is available in the sys.version string in the sys module:\n>>> import sys\n\nHuman readable:\n>>> print(sys.version)  # parentheses necessary in python 3.       \n2.5.2 (r252:60911, Jul 31 2008, 17:28:52) \n[GCC 4.2.3 (Ubuntu 4.2.3-2ubuntu7)]\n\nFor further processing, use sys.version_info or sys.hexversion:\n>>> sys.version_info\n(2, 5, 2, 'final', 0)\n# or\n>>> sys.hexversion\n34014192\n\nTo ensure a script runs with a minimal version requirement of the Python interpreter add this to your code:\nassert sys.version_info >= (2, 5)\n\nThis compares major and minor version information. Add micro (=0, 1, etc) and even releaselevel (='alpha','final', etc) to the tuple as you like. Note however, that it is almost always better to \"duck\" check if a certain feature is there, and if not, workaround (or bail out). Sometimes features go away in newer releases, being replaced by others.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 1093322,
      "answer_id": 1093331,
      "question_score": 1501,
      "answer_score": 1715,
      "is_accepted": true,
      "tags": [
        "python",
        "version"
      ]
    }
  },
  {
    "prompt": "Automatically create file &#39;requirements.txt&#39;\n\nSometimes I download the Python source code from GitHub and don't know how to install all the dependencies. If there isn't any requirements.txt file I have to create it by hand.\nGiven the Python source code directory, is it possible to create requirements.txt automatically from the import section?",
    "response": "Use Pipenv or other tools is recommended for improving your development flow.\npip3 freeze > requirements.txt  # Python3\npip freeze > requirements.txt  # Python2\n\nIf you do not use a virtual environment, pigar will be a good choice for you.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 31684375,
      "answer_id": 33468993,
      "question_score": 1497,
      "answer_score": 1473,
      "is_accepted": true,
      "tags": [
        "python",
        "dependencies",
        "python-import",
        "requirements.txt"
      ]
    }
  },
  {
    "prompt": "How can I overcome &quot;datetime.datetime not JSON serializable&quot;?\n\nsample = {}\nsample['title'] = \"String\"\nsample['somedate'] = somedatetimehere\n\njsonify(sample) on that dict gets:\n\nTypeError: datetime.datetime(2012, 8, 8, 21, 46, 24, 862000) is not JSON serializable\n\nWhat can I do to overcome that error?\nNote: the dictionaries are generated from the retrieval of records out of mongodb where when I print out str(sample['somedate']), the output is 2012-08-08 21:46:24.862000.",
    "response": "Updated for 2018\n\nThe original answer accommodated the way MongoDB \"date\" fields were represented as:\n\n{\"$date\": 1506816000000}\n\nIf you want a generic Python solution for serializing datetime to json, check out @jjmontes' answer for a quick solution which requires no dependencies.\n\n\n\nAs you are using mongoengine (per comments) and pymongo is a dependency, pymongo has built-in utilities to help with json serialization:\nhttp://api.mongodb.org/python/1.10.1/api/bson/json_util.html\n\nExample usage (serialization):\n\nfrom bson import json_util\nimport json\n\njson.dumps(anObject, default=json_util.default)\n\n\nExample usage (deserialization):\n\njson.loads(aJsonString, object_hook=json_util.object_hook)\n\n\n\n\nDjango\n\nDjango provides a native DjangoJSONEncoder serializer that deals with this kind of properly.\n\nSee https://docs.djangoproject.com/en/dev/topics/serialization/#djangojsonencoder\n\nfrom django.core.serializers.json import DjangoJSONEncoder\n\nreturn json.dumps(\n  item,\n  sort_keys=True,\n  inden",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 11875770,
      "answer_id": 11875813,
      "question_score": 1488,
      "answer_score": 559,
      "is_accepted": true,
      "tags": [
        "python",
        "json"
      ]
    }
  },
  {
    "prompt": "How to deal with SettingWithCopyWarning in Pandas\n\nI just upgraded my Pandas from 0.11 to 0.13.0rc1. Now, the application is popping out many new warnings. One of them like this:\nE:\\FinReporter\\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE\n\nWhat exactly does it mean?  Do I need to change something?\nHow should I suspend the warning if I insist on using quote_df['TVol']   = quote_df['T",
    "response": "The SettingWithCopyWarning was created to flag potentially confusing \"chained\" assignments, such as the following, which does not always work as expected, particularly when the first selection returns a copy.  [see GH5390 and GH5597 for background discussion.]\ndf[df['A'] > 2]['B'] = new_val  # new_val not set in df\n\nThe warning offers a suggestion to rewrite as follows:\ndf.loc[df['A'] > 2, 'B'] = new_val\n\nHowever, this doesn't fit your usage, which is equivalent to:\ndf = df[df['A'] > 2]\ndf['B'] = new_val\n\nWhile it's clear that you don't care about writes making it back to the original frame (since you are overwriting the reference to it), unfortunately this pattern cannot be differentiated from the first chained assignment example. Hence the (false positive) warning. The potential for false positives is addressed in the docs on indexing, if you'd like to read further.  You can safely disable this new warning with the following assignment.\nimport pandas as pd\npd.options.mode.chained_ass",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 20625582,
      "answer_id": 20627316,
      "question_score": 1487,
      "answer_score": 1723,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "chained-assignment",
        "pandas-settingwithcopy-warning"
      ]
    }
  },
  {
    "prompt": "Running shell command and capturing the output\n\nI want to write a function that will execute a shell command and return its output as a string, no matter, is it an error or success message. I just want to get the same result that I would have gotten with the command line.\n\nWhat would be a code example that would do such a thing?\n\nFor example:\n\ndef run_command(cmd):\n    # ??????\n\nprint run_command('mysqladmin create test -uroot -pmysqladmin12')\n# Should output something like:\n# mysqladmin: CREATE DATABASE failed; error: 'Can't create database ",
    "response": "In all officially maintained versions of Python, the simplest approach is to use the subprocess.check_output function:\n>>> subprocess.check_output(['ls', '-l'])\nb'total 0\\n-rw-r--r--  1 memyself  staff  0 Mar 14 11:04 files\\n'\n\ncheck_output runs a single program that takes only arguments as input.1 It returns the result exactly as printed to stdout. If you need to write input to stdin, skip ahead to the run or Popen sections. If you want to execute complex shell commands, see the note on shell=True at the end of this answer.\nThe check_output function works in all officially maintained versions of Python. But for more recent versions, a more flexible approach is available.\nModern versions of Python (3.5 or higher): run\nIf you're using Python 3.5+, and do not need backwards compatibility, the new run function is recommended by the official documentation for most tasks. It provides a very general, high-level API for the subprocess module. To capture the output of a program, pass the subpr",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 4760215,
      "answer_id": 4760517,
      "question_score": 1484,
      "answer_score": 1961,
      "is_accepted": true,
      "tags": [
        "python",
        "shell",
        "subprocess"
      ]
    }
  },
  {
    "prompt": "How to drop rows of Pandas DataFrame whose value in a certain column is NaN\n\nI have this DataFrame and want only the records whose EPS column is not NaN:\n                 STK_ID  EPS  cash\nSTK_ID RPT_Date                   \n601166 20111231  601166  NaN   NaN\n600036 20111231  600036  NaN    12\n600016 20111231  600016  4.3   NaN\n601009 20111231  601009  NaN   NaN\n601939 20111231  601939  2.5   NaN\n000001 20111231  000001  NaN   NaN\n\n...i.e. something like df.drop(....) to get this resulting dataframe:\n                  STK_ID  EPS  cash\nSTK_ID RPT_Date                   \n6",
    "response": "Don't drop, just take the rows where EPS is not NA:\n\ndf = df[df['EPS'].notna()]",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 13413590,
      "answer_id": 13413845,
      "question_score": 1477,
      "answer_score": 1649,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "indexing",
        "nan"
      ]
    }
  },
  {
    "prompt": "Display number with leading zeros\n\nHow do I display a leading zero for all numbers with less than two digits?\n1    →  01\n10   →  10\n100  →  100",
    "response": "In Python 2 (and Python 3) you can do:\nnumber = 1\nprint(\"%02d\" % (number,))\n\nBasically % is like printf or sprintf (see docs).\n\nFor Python 3.+, the same behavior can also be achieved with format:\nnumber = 1\nprint(\"{:02d}\".format(number))\n\n\nFor Python 3.6+ the same behavior can be achieved with f-strings:\nnumber = 1\nprint(f\"{number:02d}\")",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 134934,
      "answer_id": 134951,
      "question_score": 1474,
      "answer_score": 1990,
      "is_accepted": true,
      "tags": [
        "python",
        "integer",
        "string-formatting"
      ]
    }
  },
  {
    "prompt": "How do I find the location of my Python site-packages directory?\n\nHow do I find the location of my site-packages directory?",
    "response": "There are two types of site-packages directories, global and per user.\n\nGlobal site-packages (\"dist-packages\") directories are listed in sys.path when you run:\n python -m site\n\nFor a more concise list run getsitepackages from the site module in Python code:\n python -c 'import site; print(site.getsitepackages())'\n\nCaution: In virtual environments getsitepackages is not available with older versions of virtualenv, sys.path from above will list the virtualenv's site-packages directory correctly, though. In Python 3, you may use the sysconfig module instead:\n python3 -c 'import sysconfig; print(sysconfig.get_paths()[\"purelib\"])'\n\n\nThe per user site-packages directory (PEP 370) is where Python installs your local packages:\n python -m site --user-site\n\nIf this points to a non-existing directory check the exit status of Python and see python -m site --help for explanations.\nHint: Running pip list --user or pip freeze --user gives you a list of all installed per user site-packages.\n\n\n\nPractica",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 122327,
      "answer_id": 46071447,
      "question_score": 1474,
      "answer_score": 1171,
      "is_accepted": true,
      "tags": [
        "python"
      ]
    }
  },
  {
    "prompt": "How do I create multiline comments in Python?\n\nHow do I make multi-line comments? Most languages have block comment symbols like:\n/*\n\n*/",
    "response": "You can use triple-quoted strings. When they're not a docstring (the first thing in a class/function/module), they are ignored.\n'''\nThis is a multiline\ncomment.\n'''\n\n(Make sure to indent the leading ''' appropriately to avoid an IndentationError.)\nGuido van Rossum (creator of Python) tweeted this as a \"pro tip\".\nHowever, Python's style guide, PEP8, favors using consecutive single-line comments, like this:\n# This is a multiline\n# comment.\n\n...and this is also what you'll find in many projects. Text editors usually have a shortcut to do this easily.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 7696924,
      "answer_id": 7696966,
      "question_score": 1473,
      "answer_score": 2220,
      "is_accepted": true,
      "tags": [
        "python",
        "comments",
        "documentation"
      ]
    }
  },
  {
    "prompt": "String formatting: % vs. .format vs. f-string literal\n\nThere are various string formatting methods:\n\nPython <2.6: \"Hello %s\" % name\nPython 2.6+: \"Hello {}\".format(name)   (uses str.format)\nPython 3.6+: f\"{name}\"   (uses f-strings)\n\nWhich is better, and for what situations?\n\n\nThe following methods have the same outcome, so what is the difference?\nname = \"Alice\"\n\n\"Hello %s\" % name\n\"Hello {0}\".format(name)\nf\"Hello {name}\"\n\n# Using named arguments:\n\"Hello %(kwarg)s\" % {'kwarg': name}\n\"Hello {kwarg}\".format(kwarg=name)\nf\"Hello {name}\"\n\n\nWhen does string ",
    "response": "To answer your first question... .format just seems more sophisticated in many ways. An annoying thing about % is also how it can either take a variable or a tuple. You'd think the following would always work:\n\"Hello %s\" % name\n\nyet, if name happens to be (1, 2, 3), it will throw a TypeError. To guarantee that it always prints, you'd need to do\n\"Hello %s\" % (name,)   # supply the single argument as a single-item tuple\n\nwhich is just ugly. .format doesn't have those issues. Also in the second example you gave, the .format example is much cleaner looking.\nOnly use it for backwards compatibility with Python 2.5.\n\nTo answer your second question, string formatting happens at the same time as any other operation - when the string formatting expression is evaluated. And Python, not being a lazy language, evaluates expressions before calling functions, so the expression log.debug(\"some debug info: %s\" % some_info) will first evaluate the string to, e.g. \"some debug info: roflcopters are active",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 5082452,
      "answer_id": 5082482,
      "question_score": 1468,
      "answer_score": 992,
      "is_accepted": true,
      "tags": [
        "python",
        "performance",
        "string-formatting",
        "f-string"
      ]
    }
  },
  {
    "prompt": "Python: how to determine if an object is iterable?\n\nIs there a method like isiterable? The only solution I have found so far is to call:\nhasattr(myObj, '__iter__')\n\nbut I am not sure how foolproof this is.",
    "response": "Checking for __iter__ works on sequence types, but it would fail on e.g. strings in Python 2. I would like to know the right answer too, until then, here is one possibility (which would work on strings, too):\ntry:\n    some_object_iterator = iter(some_object)\nexcept TypeError as te:\n    print(some_object, 'is not iterable')\n\nThe iter built-in checks for the __iter__ method or in the case of strings the __getitem__ method.\n\nAnother general pythonic approach is to assume an iterable, then fail gracefully if it does not work on the given object. The Python glossary:\n\nPythonic programming style that determines an object's type by inspection of its method or attribute signature rather than by explicit relationship to some type object (\"If it looks like a duck and quacks like a duck, it must be a duck.\") By emphasizing interfaces rather than specific types, well-designed code improves its flexibility by allowing polymorphic substitution. Duck-typing avoids tests using type() or isinstance(). ",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 1952464,
      "answer_id": 1952481,
      "question_score": 1466,
      "answer_score": 1081,
      "is_accepted": true,
      "tags": [
        "python",
        "iterable"
      ]
    }
  },
  {
    "prompt": "Getting key with maximum value in dictionary?\n\nI have a dictionary where keys are strings, and values are integers.\nstats = {'a': 1, 'b': 3000, 'c': 0}\n\nHow do I get the key with the maximum value? In this case, it is 'b'.\n\nIs there a nicer approach than using an intermediate list with reversed key-value tuples?\ninverse = [(value, key) for key, value in stats.items()]\nprint(max(inverse)[1])",
    "response": "You can use operator.itemgetter for that:\nimport operator\nstats = {'a': 1000, 'b': 3000, 'c': 100}\nmax(stats.iteritems(), key=operator.itemgetter(1))[0]\n\nAnd instead of building a new list in memory use stats.iteritems(). The key parameter to the max() function is a function that computes a key that is used to determine how to rank items.\nPlease note that if you were to have another key-value pair 'd': 3000 that this method will only return one of the two even though they both have the maximum value.\n>>> import operator\n>>> stats = {'a': 1000, 'b': 3000, 'c': 100, 'd': 3000}\n>>> max(stats.iteritems(), key=operator.itemgetter(1))[0]\n'b' \n\nIf using Python3:\n>>> max(stats.items(), key=operator.itemgetter(1))[0]\n'b'",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 268272,
      "answer_id": 268285,
      "question_score": 1465,
      "answer_score": 776,
      "is_accepted": true,
      "tags": [
        "python",
        "dictionary",
        "max"
      ]
    }
  },
  {
    "prompt": "What does the &#39;b&#39; character do in front of a string literal?\n\nApparently, the following is the valid syntax:\nb'The string'\n\nI would like to know:\n\nWhat does this b character in front of the string mean?\nWhat are the effects of using it?\nWhat are appropriate situations to use it?\n\nI found a related question right here on SO, but that question is about PHP though, and it states the b is used to indicate the string is binary, as opposed to Unicode, which was needed for code to be compatible from version of PHP < 6, when migrating to PHP 6. I don't think this ",
    "response": "To quote the Python 2.x documentation:\n\n\n  A prefix of 'b' or 'B' is ignored in\n  Python 2; it indicates that the\n  literal should become a bytes literal\n  in Python 3 (e.g. when code is\n  automatically converted with 2to3). A\n  'u' or 'b' prefix may be followed by\n  an 'r' prefix.\n\n\nThe Python 3 documentation states:\n\n\n  Bytes literals are always prefixed with 'b' or 'B'; they produce an instance of the bytes type instead of the str type. They may only contain ASCII characters; bytes with a numeric value of 128 or greater must be expressed with escapes.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 6269765,
      "answer_id": 6269785,
      "question_score": 1460,
      "answer_score": 548,
      "is_accepted": true,
      "tags": [
        "python",
        "string",
        "unicode",
        "binary"
      ]
    }
  },
  {
    "prompt": "How do I create a constant in Python?\n\nHow do I declare a constant in Python?\nIn Java, we do:\npublic static final String CONST_NAME = \"Name\";",
    "response": "You cannot declare a variable or value as constant in Python.\n\nTo indicate to programmers that a variable is a constant, one usually writes it in upper case:\nCONST_NAME = \"Name\"\n\n\nTo raise exceptions when constants are changed, see Constants in Python by Alex Martelli. Note that this is not commonly used in practice.\n\nAs of Python 3.8, there's a typing.Final variable annotation that will tell static type checkers (like mypy) that your variable shouldn't be reassigned. This is the closest equivalent to Java's final. However, it does not actually prevent reassignment:\nfrom typing import Final\n\na: Final[int] = 1\n\n# Executes fine, but mypy will report an error if you run mypy on this:\na = 2",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 2682745,
      "answer_id": 2682752,
      "question_score": 1448,
      "answer_score": 1434,
      "is_accepted": true,
      "tags": [
        "python",
        "constants"
      ]
    }
  },
  {
    "prompt": "How do I terminate a script?\n\nHow do I exit a script early, like the die() command in PHP?",
    "response": "import sys\nsys.exit()\n\ndetails from the sys module documentation:\n\nsys.exit([arg])\nExit from Python. This is implemented by raising the\nSystemExit exception, so cleanup actions specified by finally clauses\nof try statements are honored, and it is possible to intercept the\nexit attempt at an outer level.\nThe optional argument arg can be an integer giving the exit status\n(defaulting to zero), or another type of object. If it is an integer,\nzero is considered “successful termination” and any nonzero value is\nconsidered “abnormal termination” by shells and the like. Most systems\nrequire it to be in the range 0-127, and produce undefined results\notherwise. Some systems have a convention for assigning specific\nmeanings to specific exit codes, but these are generally\nunderdeveloped; Unix programs generally use 2 for command line syntax\nerrors and 1 for all other kind of errors. If another type of object\nis passed, None is equivalent to passing zero, and any other object is\nprinted to stderr a",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 73663,
      "answer_id": 73673,
      "question_score": 1439,
      "answer_score": 1850,
      "is_accepted": true,
      "tags": [
        "python",
        "termination"
      ]
    }
  },
  {
    "prompt": "Create a Pandas Dataframe by appending one row at a time\n\nHow do I create an empty DataFrame, then add rows, one by one?\nI created an empty DataFrame:\ndf = pd.DataFrame(columns=('lib', 'qty1', 'qty2'))\n\nThen I can add a new row at the end and fill a single field with:\ndf = df._set_value(index=len(df), col='qty1', value=10.0)\n\nIt works for only one field at a time. What is a better way to add new row to df?",
    "response": "You can use df.loc[i], where the row with index i will be what you specify it to be in the dataframe.\n>>> import pandas as pd\n>>> from numpy.random import randint\n\n>>> df = pd.DataFrame(columns=['lib', 'qty1', 'qty2'])\n>>> for i in range(5):\n>>>     df.loc[i] = ['name' + str(i)] + list(randint(10, size=2))\n\n>>> df\n     lib qty1 qty2\n0  name0    3    3\n1  name1    2    4\n2  name2    2    8\n3  name3    2    1\n4  name4    9    6",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 10715965,
      "answer_id": 24888331,
      "question_score": 1428,
      "answer_score": 924,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "append"
      ]
    }
  },
  {
    "prompt": "How does the @property decorator work in Python?\n\nI would like to understand how the built-in function property works. What confuses me is that property can also be used as a decorator, but it only takes arguments when used as a built-in function and not when used as a decorator.\nThis example is from the documentation:\nclass C:\n    def __init__(self):\n        self._x = None\n\n    def getx(self):\n        return self._x\n\n    def setx(self, value):\n        self._x = value\n\n    def delx(self):\n        del self._x\n\n    x = property(getx, setx, delx, ",
    "response": "The property() function returns a special descriptor object:\n>>> property()\n<property object at 0x10ff07940>\n\nIt is this object that has extra methods:\n>>> property().getter\n<built-in method getter of property object at 0x10ff07998>\n>>> property().setter\n<built-in method setter of property object at 0x10ff07940>\n>>> property().deleter\n<built-in method deleter of property object at 0x10ff07998>\n\nThese act as decorators too. They return a new property object:\n>>> property().getter(None)\n<property object at 0x10ff079f0>\n\nthat is a copy of the old object, but with one of the functions replaced.\nRemember, that the @decorator syntax is just syntactic sugar; the syntax:\n@property\ndef foo(self): return self._foo\n\nreally means the same thing as\ndef foo(self): return self._foo\nfoo = property(foo)\n\nso foo the function is replaced by property(foo), which we saw above is a special object. Then when you use @foo.setter(), what you are doing is call that property().setter method I showed you above, w",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 17330160,
      "answer_id": 17330273,
      "question_score": 1424,
      "answer_score": 1341,
      "is_accepted": true,
      "tags": [
        "python",
        "properties",
        "decorator",
        "python-decorators",
        "python-internals"
      ]
    }
  },
  {
    "prompt": "What is the difference between null=True and blank=True in Django?\n\nWhen we add a model field in Django we generally write:\nmodels.CharField(max_length=100, null=True, blank=True)\n\nThe same is done with ForeignKey, DecimalField etc. What is the basic difference between:\n\nnull=True only\nblank=True only\nnull=True and blank=True\n\nin respect to different (CharField, ForeignKey, ManyToManyField, DateTimeField) fields? What are the advantages/disadvantages of using option 1, 2, or 3?",
    "response": "null=True sets NULL (versus NOT NULL) on the column in your DB. Blank values for Django field types such as DateTimeField or ForeignKey will be stored as NULL in the DB.\n\nblank determines whether the field will be required in forms. This includes the admin and your custom forms. If blank=True then the field will not be required, whereas if it's False the field cannot be blank.\n\nThe combo of the two is so frequent because typically if you're going to allow a field to be blank in your form, you're going to also need your database to allow NULL values for that field. The exception is CharFields and TextFields, which in Django are never saved as NULL. Blank values are stored in the DB as an empty string ('').\n\nA few examples:\n\nmodels.DateTimeField(blank=True) # raises IntegrityError if blank\n\nmodels.DateTimeField(null=True) # NULL allowed, but must be filled out in a form\n\n\nObviously, Those two options don't make logical sense to use (though there might be a use case for null=True, blank=F",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 8609192,
      "answer_id": 8609425,
      "question_score": 1424,
      "answer_score": 1654,
      "is_accepted": true,
      "tags": [
        "python",
        "django",
        "django-models",
        "django-admin"
      ]
    }
  },
  {
    "prompt": "How do I check if a variable exists?\n\nI want to check if a variable exists. Now I'm doing something like this:\ntry:\n    myVar\nexcept NameError:\n    # Do something.\n\nAre there other ways without exceptions?",
    "response": "To check the existence of a local variable:\n\nif 'myVar' in locals():\n  # myVar exists.\n\n\nTo check the existence of a global variable:\n\nif 'myVar' in globals():\n  # myVar exists.\n\n\nTo check if an object has an attribute:\n\nif hasattr(obj, 'attr_name'):\n  # obj.attr_name exists.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 843277,
      "answer_id": 843293,
      "question_score": 1419,
      "answer_score": 2366,
      "is_accepted": true,
      "tags": [
        "python",
        "exception",
        "variables"
      ]
    }
  },
  {
    "prompt": "Hidden features of Python\n\nWhat are the lesser-known but useful features of the Python programming language?\n\nTry to limit answers to Python core.\nOne feature per answer.\nGive an example and short description of the feature, not just a link to documentation.\nLabel the feature using a title as the first line.\n\nQuick links to answers:\n\nArgument Unpacking\nBraces\nChaining Comparison Operators\nDecorators\nDefault Argument Gotchas / Dangers of Mutable Default arguments\nDescriptors\nDictionary default .get value\nDocstring Tests\nEl",
    "response": "Chaining comparison operators:\n\n>>> x = 5\n>>> 1 < x < 10\nTrue\n>>> 10 < x < 20 \nFalse\n>>> x < 10 < x*10 < 100\nTrue\n>>> 10 > x <= 9\nTrue\n>>> 5 == x > 4\nTrue\n\n\nIn case you're thinking it's doing 1 < x, which comes out as True, and then comparing True < 10, which is also True, then no, that's really not what happens (see the last example.) It's really translating into 1 < x and x < 10, and x < 10 and 10 < x * 10 and x*10 < 100, but with less typing and each term is only evaluated once.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 101268,
      "answer_id": 101945,
      "question_score": 1414,
      "answer_score": 738,
      "is_accepted": false,
      "tags": [
        "python",
        "hidden-features"
      ]
    }
  },
  {
    "prompt": "How can I do a line break (line continuation) in Python (split up a long line of source code)?\n\nGiven:\ne = 'a' + 'b' + 'c' + 'd'\n\nHow do I write the above in two lines?\ne = 'a' + 'b' +\n    'c' + 'd'\n\n\nSee also: How do I split the definition of a long string over multiple lines? if there is a long string literal in the code that needs to be broken up to wrap the line nicely.",
    "response": "What is the line?  You can just have arguments on the next line without any problems:\na = dostuff(blahblah1, blahblah2, blahblah3, blahblah4, blahblah5, \n            blahblah6, blahblah7)\n\nOtherwise you can do something like this:\nif (a == True and\n    b == False):\n\nor with explicit line break:\nif a == True and \\\n   b == False:\n\nCheck the style guide for more information.\nUsing parentheses, your example can be written over multiple lines:\na = ('1' + '2' + '3' +\n    '4' + '5')\n\nThe same effect can be obtained using explicit line break:\na = '1' + '2' + '3' + \\\n    '4' + '5'\n\nNote that the style guide says that using the implicit continuation with parentheses is preferred, but in this particular case just adding parentheses around your expression is probably the wrong way to go.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 53162,
      "answer_id": 53180,
      "question_score": 1412,
      "answer_score": 1586,
      "is_accepted": true,
      "tags": [
        "python",
        "syntax",
        "line-breaks",
        "long-lines"
      ]
    }
  },
  {
    "prompt": "How can I read a text file into a string variable and strip newlines?\n\nI have a text file that looks like:\nABC\nDEF\n\nHow can I read the file into a single-line string without newlines, in this case creating a string 'ABCDEF'?\n\nFor reading the file into a list of lines, but removing the trailing newline character from each line, see How to read a file without newlines?.",
    "response": "You could use:\nwith open('data.txt', 'r') as file:\n    data = file.read().replace('\\n', '')\n\nOr if the file content is guaranteed to be one line:\nwith open('data.txt', 'r') as file:\n    data = file.read().rstrip()",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 8369219,
      "answer_id": 8369345,
      "question_score": 1409,
      "answer_score": 1975,
      "is_accepted": true,
      "tags": [
        "python",
        "string"
      ]
    }
  },
  {
    "prompt": "What is a mixin and why is it useful?\n\nIn Programming Python, Mark Lutz mentions the term mixin. I am from a C/C++/C# background and I have not heard the term before. What is a mixin?\nReading between the lines of this example (which I have linked to because it is quite long), I am presuming it is a case of using multiple inheritance to extend a class as opposed to proper subclassing. Is this right?\nWhy would I want to do that rather than put the new functionality into a subclass? For that matter, why would a mixin/multiple inheritanc",
    "response": "A mixin is a special kind of multiple inheritance.  There are two main situations where mixins are used:\n\n\nYou want to provide a lot of optional features for a class.\nYou want to use one particular feature in a lot of different classes.\n\n\nFor an example of number one, consider werkzeug's request and response system.  I can make a plain old request object by saying:\n\nfrom werkzeug import BaseRequest\n\nclass Request(BaseRequest):\n    pass\n\n\nIf I want to add accept header support, I would make that\n\nfrom werkzeug import BaseRequest, AcceptMixin\n\nclass Request(AcceptMixin, BaseRequest):\n    pass\n\n\nIf I wanted to make a request object that supports accept headers, etags, authentication, and user agent support, I could do this:\n\nfrom werkzeug import BaseRequest, AcceptMixin, ETagRequestMixin, UserAgentMixin, AuthenticationMixin\n\nclass Request(AcceptMixin, ETagRequestMixin, UserAgentMixin, AuthenticationMixin, BaseRequest):\n    pass\n\n\nThe difference is subtle, but in the above examples, the mi",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 533631,
      "answer_id": 547714,
      "question_score": 1406,
      "answer_score": 1021,
      "is_accepted": true,
      "tags": [
        "python",
        "oop",
        "multiple-inheritance",
        "mixins",
        "python-class"
      ]
    }
  },
  {
    "prompt": "How do I reverse a list or loop over it backwards?\n\nHow do I iterate over a list in reverse in Python?\n\nSee also: How can I get a reversed copy of a list (avoid a separate statement when chaining a method after .reverse)?",
    "response": "To get a new reversed list, apply the reversed function and collect the items into a list:\n>>> xs = [0, 10, 20, 40]\n>>> list(reversed(xs))\n[40, 20, 10, 0]\n\nTo iterate backwards through a list:\n>>> xs = [0, 10, 20, 40]\n>>> for x in reversed(xs):\n...     print(x)\n40\n20\n10\n0",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 3940128,
      "answer_id": 3940144,
      "question_score": 1405,
      "answer_score": 1724,
      "is_accepted": true,
      "tags": [
        "python",
        "list",
        "reverse"
      ]
    }
  },
  {
    "prompt": "How do I remove all packages installed by pip?\n\nHow do I uninstall all packages installed by pip from my currently activated virtual environment?",
    "response": "I've found this snippet as an alternative solution. It's a more graceful removal of libraries than remaking the virtualenv:\npip freeze | xargs pip uninstall -y\n\n\nIn case you have packages installed via VCS, you need to exclude those lines and remove the packages manually (elevated from the comments below):\npip freeze --exclude-editable | xargs pip uninstall -y\n\n\nIf you have packages installed directly from github/gitlab, those will have @.\nLike:\ndjango @ git+https://github.com/django.git@<sha>\nYou can add cut -d \"@\" -f1 to get just the package name that is required to uninstall it.\npip freeze | cut -d \"@\" -f1 | xargs pip uninstall -y",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 11248073,
      "answer_id": 11250821,
      "question_score": 1398,
      "answer_score": 1966,
      "is_accepted": true,
      "tags": [
        "python",
        "pip",
        "virtualenv",
        "python-packaging"
      ]
    }
  },
  {
    "prompt": "Putting a simple if-then-else statement on one line\n\nHow do I write an if-then-else statement in Python so that it fits on one line?\nFor example, I want a one line version of:\nif count == N:\n    count = 0\nelse:\n    count = N + 1\n\nIn Objective-C, I would write this as:\ncount = count == N ? 0 : count + 1;",
    "response": "That's more specifically a ternary operator expression than an if-then, here's the python syntax\nvalue_when_true if condition else value_when_false\n\nBetter Example: (thanks Mr. Burns)\n'Yes' if fruit == 'Apple' else 'No'\n\nNow with assignment and contrast with if syntax\nfruit = 'Apple'\nisApple = True if fruit == 'Apple' else False\n\nvs\nfruit = 'Apple'\nisApple = False\nif fruit == 'Apple' : isApple = True",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 2802726,
      "answer_id": 2802748,
      "question_score": 1398,
      "answer_score": 2615,
      "is_accepted": true,
      "tags": [
        "python",
        "if-statement",
        "syntax",
        "conditional-operator"
      ]
    }
  },
  {
    "prompt": "Get a list from Pandas DataFrame column headers\n\nI want to get a list of the column headers from a Pandas DataFrame.  The DataFrame will come from user input, so I won't know how many columns there will be or what they will be called.\nFor example, if I'm given a DataFrame like this:\n    y  gdp  cap\n0   1    2    5\n1   2    3    9\n2   8    7    2\n3   3    4    7\n4   6    7    7\n5   4    8    3\n6   8    2    8\n7   9    9   10\n8   6    6    4\n9  10   10    7\n\nI would get a list like this:\n['y', 'gdp', 'cap']",
    "response": "You can get the values as a list by doing:\nlist(my_dataframe.columns.values)\n\nAlso you can simply use (as shown in Ed Chum's answer):\nlist(my_dataframe)",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 19482970,
      "answer_id": 19483025,
      "question_score": 1391,
      "answer_score": 2060,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "list",
        "header"
      ]
    }
  },
  {
    "prompt": "Why do people write &quot;#!/usr/bin/env python&quot; on the first line of a Python script?\n\nI see these at the top of Python files:\n#!/usr/bin/env python\n\n#!/usr/bin/env python3\n\nIt seems to me that the files run the same without that line.",
    "response": "If you have several versions of Python installed, /usr/bin/env will ensure the interpreter used is the first one on your environment's $PATH. The alternative would be to hard code something like #!/usr/bin/python; that's ok, but less flexible.\nIn Unix, an executable file that's meant to be interpreted can indicate what interpreter to use by having a #! at the start of the first line, followed by the interpreter (and any flags it may need).\nIf you're talking about other platforms, of course, this rule does not apply (but that \"shebang line\" does no harm, and will help if you ever copy that script to a platform with a Unix base, such as Linux, Mac, etc.).",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 2429511,
      "answer_id": 2429517,
      "question_score": 1391,
      "answer_score": 1315,
      "is_accepted": false,
      "tags": [
        "python",
        "shell",
        "shebang"
      ]
    }
  },
  {
    "prompt": "Use a list of values to select rows from a Pandas dataframe\n\nLet’s say I have the following Pandas dataframe:\ndf = DataFrame({'A': [5,6,3,4], 'B': [1,2,3,5]})\ndf\n\n     A   B\n0    5   1\n1    6   2\n2    3   3\n3    4   5\n\nI can subset based on a specific value:\nx = df[df['A'] == 3]\nx\n\n     A   B\n2    3   3\n\nBut how can I subset based on a list of values? - something like this:\nlist_of_values = [3, 6]\n\ny = df[df['A'] in list_of_values]\n\nTo get:\n     A    B\n1    6    2\n2    3    3",
    "response": "You can use the isin method:\nIn [1]: df = pd.DataFrame({'A': [5,6,3,4], 'B': [1,2,3,5]})\n\nIn [2]: df\nOut[2]:\n   A  B\n0  5  1\n1  6  2\n2  3  3\n3  4  5\n\nIn [3]: df[df['A'].isin([3, 6])]\nOut[3]:\n   A  B\n1  6  2\n2  3  3\n\nAnd to get the opposite use ~:\nIn [4]: df[~df['A'].isin([3, 6])]\nOut[4]:\n   A  B\n0  5  1\n3  4  5",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 12096252,
      "answer_id": 12098586,
      "question_score": 1390,
      "answer_score": 2285,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "indexing",
        "filter"
      ]
    }
  },
  {
    "prompt": "Extract file name from path, no matter what the os/path format\n\nWhich Python library can I use to extract filenames from paths, no matter what the operating system or path format could be?\n\nFor example, I'd like all of these paths to return me c:\n\na/b/c/\na/b/c\n\\a\\b\\c\n\\a\\b\\c\\\na\\b\\c\na/b/../../a/b/c/\na/b/../../a/b/c",
    "response": "Using os.path.split or os.path.basename as others suggest won't work in all cases: if you're running the script on Linux and attempt to process a classic windows-style path, it will fail.\n\nWindows paths can use either backslash or forward slash as path separator. Therefore, the ntpath module (which is equivalent to os.path when running on windows) will work for all(1) paths on all platforms.\n\nimport ntpath\nntpath.basename(\"a/b/c\")\n\n\nOf course, if the file ends with a slash, the basename will be empty, so make your own function to deal with it:\n\ndef path_leaf(path):\n    head, tail = ntpath.split(path)\n    return tail or ntpath.basename(head)\n\n\nVerification:\n\n>>> paths = ['a/b/c/', 'a/b/c', '\\\\a\\\\b\\\\c', '\\\\a\\\\b\\\\c\\\\', 'a\\\\b\\\\c', \n...     'a/b/../../a/b/c/', 'a/b/../../a/b/c']\n>>> [path_leaf(path) for path in paths]\n['c', 'c', 'c', 'c', 'c', 'c', 'c']\n\n\n\n\n(1) There's one caveat: Linux filenames may contain backslashes. So on linux, r'a/b\\c' always refers to the file b\\c in the a folder, w",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 8384737,
      "answer_id": 8384788,
      "question_score": 1387,
      "answer_score": 1069,
      "is_accepted": true,
      "tags": [
        "python",
        "path"
      ]
    }
  },
  {
    "prompt": "Correct way to write line to file?\n\nHow do I write a line to a file in modern Python? I heard that this is deprecated:\nprint >>f, \"hi there\"\n\nAlso, does \"\\n\" work on all platforms, or should I use \"\\r\\n\" on Windows?",
    "response": "This should be as simple as:\nwith open('somefile.txt', 'a') as the_file:\n    the_file.write('Hello\\n')\n\nFrom The Documentation:\n\nDo not use os.linesep as a line terminator when writing files opened in text mode (the default); use a single '\\n' instead, on all platforms.\n\nSome useful reading:\n\nThe with statement\nopen()\n\n'a' is for append, or use\n'w' to write with truncation\n\n\nos (particularly os.linesep)",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 6159900,
      "answer_id": 6160082,
      "question_score": 1387,
      "answer_score": 1588,
      "is_accepted": true,
      "tags": [
        "python",
        "file-io"
      ]
    }
  },
  {
    "prompt": "How does Python&#39;s super() work with multiple inheritance?\n\nHow does super() work with multiple inheritance? For example, given:\nclass First(object):\n    def __init__(self):\n        print \"first\"\n\nclass Second(object):\n    def __init__(self):\n        print \"second\"\n\nclass Third(First, Second):\n    def __init__(self):\n        super(Third, self).__init__()\n        print \"that's it\"\n\nWhich parent method of Third does super().__init__ refer to? Can I choose which runs?\nI know it has something to do with method resolution order (MRO).",
    "response": "This is detailed with a reasonable amount of detail by Guido himself in his blog post Method Resolution Order (including two earlier attempts).\nIn your example, Third() will call First.__init__. Python looks for each attribute in the class's parents as they are listed left to right. In this case, we are looking for __init__. So, if you define\nclass Third(First, Second):\n    ...\n\nPython will start by looking at First, and, if First doesn't have the attribute, then it will look at Second.\nThis situation becomes more complex when inheritance starts crossing paths (for example if First inherited from Second). Read the link above for more details, but, in a nutshell, Python will try to maintain the order in which each class appears on the inheritance list, starting with the child class itself.\nSo, for instance, if you had:\nclass First(object):\n    def __init__(self):\n        print \"first\"\n\nclass Second(First):\n    def __init__(self):\n        print \"second\"\n\nclass Third(First):\n    def __ini",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 3277367,
      "answer_id": 3277407,
      "question_score": 1386,
      "answer_score": 941,
      "is_accepted": true,
      "tags": [
        "python",
        "multiple-inheritance"
      ]
    }
  },
  {
    "prompt": "How do I trim whitespace from a string?\n\nHow do I remove leading and trailing whitespace from a string in Python?\n\" Hello world \" --> \"Hello world\"\n\" Hello world\"  --> \"Hello world\"\n\"Hello world \"  --> \"Hello world\"\n\"Hello world\"   --> \"Hello world\"",
    "response": "To remove all whitespace surrounding a string, use .strip(). Examples:\n>>> ' Hello '.strip()\n'Hello'\n>>> ' Hello'.strip()\n'Hello'\n>>> 'Bob has a cat'.strip()\n'Bob has a cat'\n>>> '   Hello   '.strip()  # ALL consecutive spaces at both ends removed\n'Hello'\n\nNote that str.strip() removes all whitespace characters, including tabs and newlines. To remove only spaces, specify the specific character to remove as an argument to strip:\n>>> \"  Hello\\n  \".strip(\" \")\n'Hello\\n'\n\n\nTo remove only one space at most:\ndef strip_one_space(s):\n    if s.endswith(\" \"): s = s[:-1]\n    if s.startswith(\" \"): s = s[1:]\n    return s\n\n>>> strip_one_space(\"   Hello \")\n'  Hello'",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 761804,
      "answer_id": 761825,
      "question_score": 1379,
      "answer_score": 2008,
      "is_accepted": true,
      "tags": [
        "python",
        "string",
        "trim"
      ]
    }
  },
  {
    "prompt": "How do I return dictionary keys as a list in Python?\n\nWith Python 2.7, I can get dictionary keys, values, or items as a list:\n>>> newdict = {1:0, 2:0, 3:0}\n>>> newdict.keys()\n[1, 2, 3]\n\nWith Python >= 3.3, I get:\n>>> newdict.keys()\ndict_keys([1, 2, 3])\n\nHow do I get a plain list of keys with Python 3?",
    "response": "This will convert the dict_keys object to a list:\nlist(newdict.keys())\n\n\nOn the other hand, you should ask yourself whether or not it matters. It is Pythonic to assume duck typing -- if it looks like a duck and it quacks like a duck, it is a duck. The dict_keys object can be iterated over just like a list. For instance:\nfor key in newdict.keys():\n    print(key)\n\nNote that dict_keys doesn't support insertion newdict[k] = v, though you may not need it.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 16819222,
      "answer_id": 16819250,
      "question_score": 1372,
      "answer_score": 1705,
      "is_accepted": true,
      "tags": [
        "python",
        "python-3.x",
        "list",
        "dictionary"
      ]
    }
  },
  {
    "prompt": "How do I type hint a method with the type of the enclosing class?\n\nI have the following code in Python 3:\nclass Position:\n\n    def __init__(self, x: int, y: int):\n        self.x = x\n        self.y = y\n\n    def __add__(self, other: Position) -> Position:\n        return Position(self.x + other.x, self.y + other.y)\n\nBut my editor (PyCharm) says that the reference Position can not be resolved (in the __add__ method). How should I specify that I expect the return type to be of type Position?\nI think this is actually a PyCharm issue. It actually uses the information ",
    "response": "I guess you got this exception:\nNameError: name 'Position' is not defined\n\nThis is because in the original implementation of annotations, Position must be defined before you can use it in an annotation.\nPython 3.14+: It'll just work\nPython 3.14 has a new, lazily evaluated annotation implementation specified by PEP 749 and 649. Annotations will be compiled to special __annotate__ functions, executed when an object's __annotations__ dict is first accessed instead of at the point where the annotation itself occurs.\nThus, annotating your function as def __add__(self, other: Position) -> Position: no longer requires Position to already exist:\nclass Position:\n    def __add__(self, other: Position) -> Position:\n        ...\n\nPython 3.7+, deprecated: from __future__ import annotations\nfrom __future__ import annotations turns on an older solution to this problem, PEP 563, where all annotations are saved as strings instead of as __annotate__ functions or evaluated values. This was originally plan",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 33533148,
      "answer_id": 33533514,
      "question_score": 1370,
      "answer_score": 1774,
      "is_accepted": true,
      "tags": [
        "python",
        "pycharm",
        "python-typing"
      ]
    }
  },
  {
    "prompt": "How do I get file creation and modification date/times?\n\nWhat's the best cross-platform way to get file creation and modification dates/times, that works on both Linux and Windows?",
    "response": "In Python 3.4 and above, you can use the object oriented pathlib module interface which includes wrappers for much of the os module.  Here is an example of getting the file stats.\n>>> import pathlib\n>>> fname = pathlib.Path('test.py')\n>>> assert fname.exists(), f'No such file: {fname}'  # check that the file exists\n>>> print(fname.stat())\nos.stat_result(st_mode=33206, st_ino=5066549581564298, st_dev=573948050, st_nlink=1, st_uid=0, st_gid=0, st_size=413, st_atime=1523480272, st_mtime=1539787740, st_ctime=1523480272)\n\nFor more information about what os.stat_result contains, refer to the documentation. For the modification time you want fname.stat().st_mtime:\n>>> import datetime\n>>> mtime = datetime.datetime.fromtimestamp(fname.stat().st_mtime, tz=datetime.timezone.utc)\n>>> print(mtime)\ndatetime.datetime(2018, 10, 17, 10, 49, 0, 249980)\n\nIf you want the creation time on Windows, or the most recent metadata change on Unix, you would use fname.stat().st_ctime:\n>>> ctime = datetime.datetime",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 237079,
      "answer_id": 52858040,
      "question_score": 1365,
      "answer_score": 202,
      "is_accepted": true,
      "tags": [
        "python",
        "file"
      ]
    }
  },
  {
    "prompt": "How to get the ASCII value of a character\n\nHow do I get the ASCII value of a character as an int in Python?",
    "response": "From here:\n\nThe function ord() gets the int value\nof the char. And in case you want to\nconvert back after playing with the\nnumber, function chr() does the trick.\n\n>>> ord('a')\n97\n>>> chr(97)\n'a'\n>>> chr(ord('a') + 3)\n'd'\n>>>\n\nIn Python 2, there was also the unichr function, returning the Unicode character whose ordinal is the unichr argument:\n>>> unichr(97)\nu'a'\n>>> unichr(1234)\nu'\\u04d2'\n\nIn Python 3 you can use chr instead of unichr.\n\nord() - Python 3.6.5rc1 documentation\nord() - Python 2.7.14 documentation",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 227459,
      "answer_id": 227472,
      "question_score": 1362,
      "answer_score": 1812,
      "is_accepted": true,
      "tags": [
        "python",
        "ascii"
      ]
    }
  },
  {
    "prompt": "What is the purpose of the `self` parameter? Why is it needed?\n\nConsider this example:\nclass MyClass:\n    def func(self, name):\n        self.name = name\n\nI know that self refers to the specific instance of MyClass. But why must func explicitly include self as a parameter? Why do we need to use self in the method's code? Some other languages make this implicit, or use special syntax instead.\n\nFor a language-agnostic consideration of the design decision, see What is the advantage of having this/self pointer mandatory explicit?.\nTo close debugging questions whe",
    "response": "The reason you need to use self is because Python does not use special syntax to refer to instance attributes. Python decided to do methods in a way that makes the instance to which the method belongs be passed automatically but not received automatically, the first parameter of methods is the instance the method is called on. That makes methods entirely the same as functions and leaves the actual name to use up to you (although self is the convention, and people will generally frown at you when you use something else.) self is not special to the code, it's just another object.\nPython could have done something else to distinguish normal names from attributes -- special syntax like Ruby has, or requiring declarations like C++ and Java do, or perhaps something yet more different -- but it didn't. Python's all for making things explicit, making it obvious what's what, and although it doesn't do it entirely everywhere, it does do it for instance attributes. That's why assigning to an insta",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 2709821,
      "answer_id": 2709832,
      "question_score": 1357,
      "answer_score": 810,
      "is_accepted": true,
      "tags": [
        "python",
        "class",
        "oop",
        "self"
      ]
    }
  },
  {
    "prompt": "Count the number of occurrences of a character in a string\n\nHow do I count the number of occurrences of a character in a string?\ne.g. 'a' appears in 'Mary had a little lamb' 4 times.",
    "response": "str.count(sub[, start[, end]])\nReturn the number of non-overlapping occurrences of substring sub in the range [start, end]. Optional arguments start and end are interpreted as in slice notation.\n\n>>> sentence = 'Mary had a little lamb'\n>>> sentence.count('a')\n4",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 1155617,
      "answer_id": 1155647,
      "question_score": 1357,
      "answer_score": 1861,
      "is_accepted": true,
      "tags": [
        "python",
        "string",
        "count"
      ]
    }
  },
  {
    "prompt": "If Python is interpreted, what are .pyc files?\n\nPython is an interpreted language. But why does my source directory contain .pyc files, which are identified by Windows as \"Compiled Python Files\"?",
    "response": "They contain byte code, which is what the Python interpreter compiles the source to. This code is then executed by Python's virtual machine.\nPython's documentation explains the definition like this:\n\nPython is an interpreted language, as\nopposed to a compiled one, though the\ndistinction can be blurry because of\nthe presence of the bytecode compiler.\nThis means that source files can be\nrun directly without explicitly\ncreating an executable which is then\nrun.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 2998215,
      "answer_id": 2998228,
      "question_score": 1356,
      "answer_score": 781,
      "is_accepted": true,
      "tags": [
        "python",
        "compiled",
        "interpreted-language",
        "pyc"
      ]
    }
  },
  {
    "prompt": "Converting from a string to boolean in Python\n\nHow do I convert a string into a boolean in Python? This attempt returns True:\n>>> bool(\"False\")\nTrue",
    "response": "Really, you just compare the string to whatever you expect to accept as representing true, so you can do this:\ns == 'True'\n\nOr to checks against a whole bunch of values:\ns.lower() in ['true', '1', 't', 'y', 'yes', 'yeah', 'yup', 'certainly', 'uh-huh']\n\nBe cautious when using the following:\n>>> bool(\"foo\")\nTrue\n>>> bool(\"False\") # beware!\nTrue\n>>> bool(\"\")\nFalse\n\nEmpty strings evaluate to False, but everything else evaluates to True. So this should not be used for any kind of parsing purposes.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 715417,
      "answer_id": 715455,
      "question_score": 1353,
      "answer_score": 1412,
      "is_accepted": true,
      "tags": [
        "python",
        "string",
        "boolean"
      ]
    }
  },
  {
    "prompt": "How to add a new column to an existing DataFrame\n\nI have the following indexed DataFrame with named columns and rows not- continuous numbers:\n          a         b         c         d\n2  0.671399  0.101208 -0.181532  0.241273\n3  0.446172 -0.243316  0.051767  1.577318\n5  0.614758  0.075793 -0.451460 -0.012493\n\nI would like to add a new column, 'e', to the existing data frame and do not want to change anything in the data frame (i.e., the new column always has the same length as the DataFrame).\n0   -0.335485\n1   -1.166658\n2   -0.385571\ndtype: flo",
    "response": "Edit 2017\nAs indicated in the comments and by @Alexander, currently the best method to add the values of a Series as a new column of a DataFrame could be using assign:\ndf1 = df1.assign(e=pd.Series(np.random.randn(sLength)).values)\n\n\nEdit 2015\nSome reported getting the SettingWithCopyWarning with this code.\nHowever, the code still runs perfectly with the current pandas version 0.16.1.\n>>> sLength = len(df1['a'])\n>>> df1\n          a         b         c         d\n6 -0.269221 -0.026476  0.997517  1.294385\n8  0.917438  0.847941  0.034235 -0.448948\n\n>>> df1['e'] = pd.Series(np.random.randn(sLength), index=df1.index)\n>>> df1\n          a         b         c         d         e\n6 -0.269221 -0.026476  0.997517  1.294385  1.757167\n8  0.917438  0.847941  0.034235 -0.448948  2.228131\n\n>>> pd.version.short_version\n'0.16.1'\n\nThe SettingWithCopyWarning aims to inform of a possibly invalid assignment on a copy of the Dataframe. It doesn't necessarily say you did it wrong (it can trigger false positives",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 12555323,
      "answer_id": 12555510,
      "question_score": 1342,
      "answer_score": 1322,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "chained-assignment"
      ]
    }
  },
  {
    "prompt": "How to create a GUID/UUID in Python\n\nHow do I create a GUID/UUID in Python that is platform independent? I hear there is a method using ActivePython on Windows but it's Windows only because it uses COM. Is there a method using plain Python?",
    "response": "The uuid module provides immutable UUID objects (the UUID class) and the functions uuid1(), uuid3(), uuid4(), uuid5() for generating version 1, 3, 4, and 5 UUIDs as specified in RFC 4122.\n\n\nIf all you want is a unique ID, you should probably call uuid1() or uuid4().\nNote that uuid1() may compromise privacy since it creates a UUID containing the computer’s network address.\nuuid4() creates a random UUID.\n\nUUID versions 6, 7 and 8 - new Universally Unique Identifier (UUID) formats for use in modern applications and as database keys -  (draft) rfc - are available from https://pypi.org/project/uuid6/\nDocs:\n\nPython 2\nPython 3\n\nExamples (for both Python 2 and 3):\n>>> import uuid\n\n>>> # make a random UUID\n>>> uuid.uuid4()\nUUID('bd65600d-8669-4903-8a14-af88203add38')\n\n>>> # Convert a UUID to a string of hex digits in standard form\n>>> str(uuid.uuid4())\n'f50ec0b7-f960-400d-91f0-c42a6d44e3d0'\n\n>>> # Convert a UUID to a 32-character hexadecimal string\n>>> uuid.uuid4().hex\n'9fe2c4e93f654fdbb24c02b1",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 534839,
      "answer_id": 534847,
      "question_score": 1335,
      "answer_score": 1640,
      "is_accepted": true,
      "tags": [
        "python",
        "uuid",
        "guid",
        "uniqueidentifier"
      ]
    }
  },
  {
    "prompt": "Why does comparing strings using either &#39;==&#39; or &#39;is&#39; sometimes produce a different result?\n\nTwo string variables are set to the same value. s1 == s2 always returns True, but s1 is s2 sometimes returns False.\nIf I open my Python interpreter and do the same is comparison, it succeeds:\n>>> s1 = 'text'\n>>> s2 = 'text'\n>>> s1 is s2\nTrue\n\nWhy is this?",
    "response": "is is identity testing, and == is equality testing. What happens in your code would be emulated in the interpreter like this:\n>>> a = 'pub'\n>>> b = ''.join(['p', 'u', 'b'])\n>>> a == b\nTrue\n>>> a is b\nFalse\n\nSo, no wonder they're not the same, right?\nIn other words: a is b is the equivalent of id(a) == id(b)",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 1504717,
      "answer_id": 1504742,
      "question_score": 1332,
      "answer_score": 1693,
      "is_accepted": true,
      "tags": [
        "python",
        "string",
        "comparison",
        "identity",
        "equality"
      ]
    }
  },
  {
    "prompt": "How to get the line count of a large file cheaply in Python\n\nHow do I get a line count of a large file in the most memory- and time-efficient manner?\ndef file_len(filename):\n    with open(filename) as f:\n        for i, _ in enumerate(f):\n            pass\n    return i + 1",
    "response": "You can't get any better than that.\nAfter all, any solution will have to read the entire file, figure out how many \\n you have, and return that result.\nDo you have a better way of doing that without reading the entire file? Not sure... The best solution will always be I/O-bound, best you can do is make sure you don't use unnecessary memory, but it looks like you have that covered.\n[Edit May 2023]\nAs commented in many other answers, in Python 3 there are better alternatives. The for loop is not the most efficient. For example, using mmap or buffers is more efficient.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 845058,
      "answer_id": 845081,
      "question_score": 1320,
      "answer_score": 431,
      "is_accepted": true,
      "tags": [
        "python",
        "text-files",
        "line-count"
      ]
    }
  },
  {
    "prompt": "How do I get a list of locally installed Python modules?\n\nHow do I get a list of Python modules installed on my computer?",
    "response": "Solution\nDo not use with pip > 10.0!\nMy 50 cents for getting a pip freeze-like list from a Python script:\nimport pip\ninstalled_packages = pip.get_installed_distributions()\ninstalled_packages_list = sorted([\"%s==%s\" % (i.key, i.version)\n     for i in installed_packages])\nprint(installed_packages_list)\n\nAs a (too long) one liner:\nsorted([\"%s==%s\" % (i.key, i.version) for i in pip.get_installed_distributions()])\n\nGiving:\n['behave==1.2.4', 'enum34==1.0', 'flask==0.10.1', 'itsdangerous==0.24',\n 'jinja2==2.7.2', 'jsonschema==2.3.0', 'markupsafe==0.23', 'nose==1.3.3',\n 'parse-type==0.3.4', 'parse==1.6.4', 'prettytable==0.7.2', 'requests==2.3.0',\n 'six==1.6.1', 'vioozer-metadata==0.1', 'vioozer-users-server==0.1',\n 'werkzeug==0.9.4']\n\nScope\nThis solution applies to the system scope or to a virtual environment scope, and covers packages installed by setuptools, pip and (god forbid) easy_install.\nMy use case\nI added the result of this call to my Flask server, so when I call it with http://exampl",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 739993,
      "answer_id": 23885252,
      "question_score": 1315,
      "answer_score": 670,
      "is_accepted": true,
      "tags": [
        "python",
        "module",
        "pip"
      ]
    }
  },
  {
    "prompt": "How to set environment variables in Python?\n\nI need to set some environment variables in the Python script and I want all the other scripts that are called from Python to see the environment variables' set.\nIf I do,\nos.environ[\"DEBUSSY\"] = 1\n\nit complains saying that 1 has to be a string.\nI also want to know how to read the environment variables in Python (in the latter part of the script) once I set it.",
    "response": "Environment variables must be strings, so use\nimport os\nos.environ[\"DEBUSSY\"] = \"1\"\n\nto set the variable DEBUSSY to the string 1.\nTo access this variable later, simply use\nprint(os.environ[\"DEBUSSY\"])\n\nChild processes automatically inherit the environment of the parent process -- no special action on your part is required.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 5971312,
      "answer_id": 5971326,
      "question_score": 1312,
      "answer_score": 1651,
      "is_accepted": false,
      "tags": [
        "python",
        "environment-variables"
      ]
    }
  },
  {
    "prompt": "How do I import other Python files?\n\nHow do I import files in Python? I want to import:\n\na file (e.g. file.py)\na folder\na file dynamically at runtime, based on user input\none specific part of a file (e.g. a single function)",
    "response": "importlib was added to Python 3 to programmatically import a module.\nimport importlib\n\nmoduleName = input('Enter module name:')\nimportlib.import_module(moduleName)\n\nThe .py extension should be removed from moduleName. The function also defines a package argument for relative imports.\nIn python 2.x:\n\nJust import file without the .py extension\nA folder can be marked as a package, by adding an empty __init__.py file\nYou can use the __import__ function, which takes the module name (without extension) as a string extension\n\npmName = input('Enter module name:')\npm = __import__(pmName)\nprint(dir(pm))\n\nType help(__import__) for more details.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 2349991,
      "answer_id": 2349998,
      "question_score": 1310,
      "answer_score": 559,
      "is_accepted": true,
      "tags": [
        "python",
        "import",
        "python-import",
        "python-module",
        "python-packaging"
      ]
    }
  },
  {
    "prompt": "Maximum and Minimum values for ints\n\nHow do I represent minimum and maximum values for integers in Python? In Java, we have Integer.MIN_VALUE and Integer.MAX_VALUE.\n\nSee also: What is the maximum float in Python?.",
    "response": "Python 3\nIn Python 3, this question doesn't apply. The plain int type is unbounded.\nHowever, you might actually be looking for information about the current interpreter's word size, which will be the same as the machine's word size in most cases. That information is still available in Python 3 as sys.maxsize, which is the maximum value representable by a signed word. Equivalently, it's the size of the largest possible list or in-memory sequence.\nGenerally, the maximum value representable by an unsigned word will be sys.maxsize * 2 + 1, and the number of bits in a word will be math.log2(sys.maxsize * 2 + 2). See this answer for more information.\nPython 2\nIn Python 2, the maximum value for plain int values is available as sys.maxint:\n>>> sys.maxint  # on my system, 2**63-1\n9223372036854775807\n\nYou can calculate the minimum value with -sys.maxint - 1 as shown in the docs.\nPython seamlessly switches from plain to long integers once you exceed this value. So most of the time, you won't need",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 7604966,
      "answer_id": 7604981,
      "question_score": 1310,
      "answer_score": 1340,
      "is_accepted": true,
      "tags": [
        "python",
        "integer"
      ]
    }
  },
  {
    "prompt": "How to print a number using commas as thousands separators\n\nHow do I print an integer with commas as thousands separators?\n1234567   ⟶   1,234,567\n\nIt does not need to be locale-specific to decide between periods and commas.",
    "response": "Locale-agnostic: use _ as the thousand separator\nf'{value:_}'          # For Python ≥3.6\n\nNote that this will NOT format in the user's current locale and will always use _ as the thousand separator, so for example:\n1234567 ⟶ 1_234_567\n\nEnglish style: use , as the thousand separator\n'{:,}'.format(value)  # For Python ≥2.7\nf'{value:,}'          # For Python ≥3.6\n\nLocale-aware\nimport locale\nlocale.setlocale(locale.LC_ALL, '')  # Use '' for auto, or force e.g. to 'en_US.UTF-8'\n\n'{:n}'.format(value)  # For Python ≥2.7\nf'{value:n}'          # For Python ≥3.6\n\nReference\nPer Format Specification Mini-Language,\n\nThe ',' option signals the use of a comma for a thousands separator. For a locale aware separator, use the 'n' integer presentation type instead.\n\nand:\n\nThe '_' option signals the use of an underscore for a thousands separator for floating point presentation types and for integer presentation type 'd'. For integer presentation types 'b', 'o', 'x', and 'X', underscores will be inserted e",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 1823058,
      "answer_id": 10742904,
      "question_score": 1303,
      "answer_score": 2559,
      "is_accepted": true,
      "tags": [
        "python",
        "number-formatting"
      ]
    }
  },
  {
    "prompt": "How do I iterate through two lists in parallel?\n\nI have two iterables, and I want to go over them in pairs:\nfoo = [1, 2, 3]\nbar = [4, 5, 6]\n\nfor (f, b) in iterate_together(foo, bar):\n    print(\"f:\", f, \" |  b:\", b)\n\nThat should result in:\nf: 1  |  b: 4\nf: 2  |  b: 5\nf: 3  |  b: 6\n\nOne way to do it is to iterate over the indices:\nfor i in range(len(foo)):\n    print(\"f:\", foo[i], \" |  b:\", bar[i])\n\nBut that seems somewhat unpythonic to me. Is there a better way to do it?\n\nRelated tasks:\n* How to merge lists into a list of tuples? - given the abo",
    "response": "Python 3\nfor f, b in zip(foo, bar):\n    print(f, b)\n\nzip stops when the shorter of foo or bar stops.\nIn Python 3, zip\nreturns an iterator of tuples, like itertools.izip in Python2.  To get a list\nof tuples, use list(zip(foo, bar)). And to zip until both iterators are\nexhausted, you would use\nitertools.zip_longest.\nPython 2\nIn Python 2, zip\nreturns a list of tuples. This is fine when foo and bar are not massive. If they are both massive then forming zip(foo,bar) is an unnecessarily massive\ntemporary variable, and should be replaced by itertools.izip or\nitertools.izip_longest, which returns an iterator instead of a list.\nimport itertools\nfor f,b in itertools.izip(foo,bar):\n    print(f,b)\nfor f,b in itertools.izip_longest(foo,bar):\n    print(f,b)\n\nizip stops when either foo or bar is exhausted.\nizip_longest stops when both foo and bar are exhausted.\nWhen the shorter iterator(s) are exhausted, izip_longest yields a tuple with None in the position corresponding to that iterator. You can als",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 1663807,
      "answer_id": 1663826,
      "question_score": 1303,
      "answer_score": 2015,
      "is_accepted": true,
      "tags": [
        "python",
        "list",
        "for-loop",
        "iterator"
      ]
    }
  },
  {
    "prompt": "How do I install a Python package with a .whl file?\n\nI'm trying to install a Python package on Windows, using Christoph Gohlke's Windows binaries here:\nhttp://www.lfd.uci.edu/~gohlke/pythonlibs/#jpype\nThe package is made available here as a .whl file. How do I install that?",
    "response": "I just used the following which was quite simple. First open a console then cd to where you've downloaded your file like some-package.whl and use\n\npip install some-package.whl\n\n\nNote: if pip.exe is not recognized, you may find it in the \"Scripts\" directory from where python has been installed. If pip is not installed, this page can help:\nHow do I install pip on Windows?\n\nNote: for clarification\nIf you copy the *.whl file to your local drive (ex. C:\\some-dir\\some-file.whl) use the following command line parameters --  \n\npip install C:/some-dir/some-file.whl",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 27885397,
      "answer_id": 27909082,
      "question_score": 1302,
      "answer_score": 1538,
      "is_accepted": true,
      "tags": [
        "python",
        "pip",
        "python-wheel"
      ]
    }
  },
  {
    "prompt": "Get the data received in a Flask request\n\nI want to be able to get the data sent to my Flask app. I've tried accessing request.data but it is an empty string. How do you access request data?\n\nfrom flask import request\n\n@app.route('/', methods=['GET', 'POST'])\ndef parse_request():\n    data = request.data  # data is empty\n    # need posted data here\n\n\n\n\nThe answer to this question led me to ask Get raw POST body in Python Flask regardless of Content-Type header next, which is about getting the raw data rather than the parsed data.",
    "response": "The docs describe the attributes available on the request object (from flask import request) during a request. In most common cases request.data will be empty because it's used as a fallback:\n\nrequest.data Contains the incoming request data as string in case it came with a mimetype Flask does not handle.\n\n\nrequest.args: the key/value pairs in the URL query string\nrequest.form: the key/value pairs in the body, from a HTML post form, or JavaScript request that isn't JSON encoded\nrequest.files: the files in the body, which Flask keeps separate from form. HTML forms must use enctype=multipart/form-data or files will not be uploaded.\nrequest.values: combined args and form, preferring args if keys overlap\nrequest.json: parsed JSON data. The request must have the application/json content type, or use request.get_json(force=True) to ignore the content type.\n\nAll of these are MultiDict instances (except for json). You can access values using:\n\nrequest.form['name']: use indexing if you know the ",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 10434599,
      "answer_id": 16664376,
      "question_score": 1300,
      "answer_score": 2240,
      "is_accepted": true,
      "tags": [
        "python",
        "flask",
        "werkzeug"
      ]
    }
  },
  {
    "prompt": "Usage of __slots__?\n\nWhat is the purpose of __slots__ in Python — especially with respect to when I would want to use it, and when not?",
    "response": "In Python, what is the purpose of __slots__ and what are the cases one should avoid this?\n\nTLDR:\nThe special attribute __slots__ allows you to explicitly state which instance attributes you expect your object instances to have, with the expected results:\n\nfaster attribute access.\nspace savings in memory.\n\nThe space savings is from\n\nStoring value references in slots instead of __dict__.\nDenying __dict__ and __weakref__ creation if parent classes deny them and you declare __slots__.\n\nQuick Caveats\nSmall caveat, you should only declare a particular slot one time in an inheritance tree. For example:\nclass Base:\n    __slots__ = 'foo', 'bar'\n\nclass Right(Base):\n    __slots__ = 'baz', \n\nclass Wrong(Base):\n    __slots__ = 'foo', 'bar', 'baz'        # redundant foo and bar\n\nPython doesn't object when you get this wrong (it probably should), problems might not otherwise manifest, but your objects will take up more space than they otherwise should. Python 3.8:\n>>> from sys import getsizeof\n>>> ge",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 472000,
      "answer_id": 28059785,
      "question_score": 1289,
      "answer_score": 1877,
      "is_accepted": true,
      "tags": [
        "python",
        "oop",
        "python-internals",
        "slots"
      ]
    }
  },
  {
    "prompt": "How can I iterate over rows in a Pandas DataFrame?\n\nI have a pandas dataframe, df:\n   c1   c2\n0  10  100\n1  11  110\n2  12  120\n\nHow do I iterate over the rows of this dataframe? For every row, I want to access its elements (values in cells) by the name of the columns. For example:\nfor row in df.rows:\n    print(row['c1'], row['c2'])\n\n\nI found a similar question, which suggests using either of these:\n\n\nfor date, row in df.T.iteritems():\n\n\n\nfor row in df.iterrows():\n\n\n\nBut I do not understand what the row object is and how I can work with it.",
    "response": "DataFrame.iterrows is a generator which yields both the index and row (as a Series):\nimport pandas as pd\n\ndf = pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 120]})\ndf = df.reset_index()  # make sure indexes pair with number of rows\n\nfor index, row in df.iterrows():\n    print(row['c1'], row['c2'])\n\n\n10 100\n11 110\n12 120\n\n\nObligatory disclaimer from the documentation\n\nIterating through pandas objects is generally slow. In many cases, iterating manually over the rows is not needed and can be avoided with one of the following approaches:\n\nLook for a vectorized solution: many operations can be performed using built-in methods or NumPy functions, (boolean) indexing, …\nWhen you have a function that cannot work on the full DataFrame/Series at once, it is better to use apply() instead of iterating over the values. See the docs on function application.\nIf you need to do iterative manipulations on the values but performance is important, consider writing the inner loop with cython or numba. ",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 16476924,
      "answer_id": 16476974,
      "question_score": 4238,
      "answer_score": 5504,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "loops"
      ]
    }
  },
  {
    "prompt": "How do I select rows from a DataFrame based on column values?\n\nHow can I select rows from a DataFrame based on values in some column in Pandas?\nIn SQL, I would use:\nSELECT *\nFROM table\nWHERE column_name = some_value",
    "response": "To select rows whose column value equals a scalar, some_value, use ==:\ndf.loc[df['column_name'] == some_value]\n\nTo select rows whose column value is in an iterable, some_values, use isin:\ndf.loc[df['column_name'].isin(some_values)]\n\nCombine multiple conditions with &:\ndf.loc[(df['column_name'] >= A) & (df['column_name'] <= B)]\n\nNote the parentheses. Due to Python's operator precedence rules, & binds more tightly than <= and >=. Thus, the parentheses in the last example are necessary. Without the parentheses\ndf['column_name'] >= A & df['column_name'] <= B\n\nis parsed as\ndf['column_name'] >= (A & df['column_name']) <= B\n\nwhich results in a Truth value of a Series is ambiguous error.\n\nTo select rows whose column value does not equal some_value, use !=:\ndf.loc[df['column_name'] != some_value]\n\nThe isin returns a boolean Series, so to select rows whose value is not in some_values, negate the boolean Series using ~:\ndf = df.loc[~df['column_name'].isin(some_values)] # .loc is not in-place repl",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 17071871,
      "answer_id": 17071908,
      "question_score": 3625,
      "answer_score": 6625,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "indexing",
        "filter"
      ]
    }
  },
  {
    "prompt": "How do I change the size of figures drawn with Matplotlib?\n\nHow do I change the size of figure drawn with Matplotlib?",
    "response": "figure tells you the call signature:\nfrom matplotlib.pyplot import figure\n\nfigure(figsize=(8, 6), dpi=80)\n\nfigure(figsize=(1,1)) would create an inch-by-inch image, which would be 80-by-80 pixels unless you also give a different dpi argument.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 332289,
      "answer_id": 638443,
      "question_score": 3263,
      "answer_score": 1931,
      "is_accepted": false,
      "tags": [
        "python",
        "pandas",
        "matplotlib",
        "seaborn",
        "figsize"
      ]
    }
  },
  {
    "prompt": "Renaming column names in Pandas\n\nI want to change the column labels of a Pandas DataFrame from\n['$a', '$b', '$c', '$d', '$e']\n\nto\n['a', 'b', 'c', 'd', 'e']",
    "response": "Just assign it to the .columns attribute:\n>>> df = pd.DataFrame({'$a':[1,2], '$b': [10,20]})\n>>> df\n   $a  $b\n0   1  10\n1   2  20\n\n>>> df.columns = ['a', 'b']\n>>> df\n   a   b\n0  1  10\n1  2  20",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 11346283,
      "answer_id": 11346337,
      "question_score": 3068,
      "answer_score": 2571,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "replace",
        "dataframe",
        "rename"
      ]
    }
  },
  {
    "prompt": "Delete a column from a Pandas DataFrame\n\nTo delete a column in a DataFrame, I can successfully use:\ndel df['column_name']\n\nBut why can't I use the following?\ndel df.column_name\n\nSince it is possible to access the Series via df.column_name, I expected this to work.",
    "response": "As you've guessed, the right syntax is \n\ndel df['column_name']\n\n\nIt's difficult to make del df.column_name work simply as the result of syntactic limitations in Python. del df[name] gets translated to df.__delitem__(name) under the covers by Python.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 13411544,
      "answer_id": 13485766,
      "question_score": 2330,
      "answer_score": 1335,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "del"
      ]
    }
  },
  {
    "prompt": "How do I get the row count of a Pandas DataFrame?\n\nHow do I get the number of rows of a pandas dataframe df?",
    "response": "For a dataframe df, one can use any of the following:\n\nlen(df.index)\ndf.shape[0]\ndf[df.columns[0]].count() (== number of non-NaN values in first column)\n\n\n\nCode to reproduce the plot:\nimport numpy as np\nimport pandas as pd\nimport perfplot\n\nperfplot.save(\n    \"out.png\",\n    setup=lambda n: pd.DataFrame(np.arange(n * 3).reshape(n, 3)),\n    n_range=[2**k for k in range(25)],\n    kernels=[\n        lambda df: len(df.index),\n        lambda df: df.shape[0],\n        lambda df: df[df.columns[0]].count(),\n    ],\n    labels=[\"len(df.index)\", \"df.shape[0]\", \"df[df.columns[0]].count()\"],\n    xlabel=\"Number of rows\",\n)",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 15943769,
      "answer_id": 15943975,
      "question_score": 2023,
      "answer_score": 2994,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe"
      ]
    }
  },
  {
    "prompt": "Selecting multiple columns in a Pandas dataframe\n\nHow do I select columns a and b from df, and save them into a new dataframe df1?\nindex  a   b   c\n1      2   3   4\n2      3   4   5\n\nUnsuccessful attempt:\ndf1 = df['a':'b']\ndf1 = df.ix[:, 'a':'b']",
    "response": "The column names (which are strings) cannot be sliced in the manner you tried.\nHere you have a couple of options. If you know from context which variables you want to slice out, you can just return a view of only those columns by passing a list into the __getitem__ syntax (the []'s).\ndf1 = df[['a', 'b']]\n\nAlternatively, if it matters to index them numerically and not by their name (say your code should automatically do this without knowing the names of the first two columns) then you can do this instead:\ndf1 = df.iloc[:, 0:2] # Remember that Python does not slice inclusive of the ending index.\n\nAdditionally, you should familiarize yourself with the idea of a view into a Pandas object vs. a copy of that object. The first of the above methods will return a new copy in memory of the desired sub-object (the desired slices).\nSometimes, however, there are indexing conventions in Pandas that don't do this and instead give you a new variable that just refers to the same chunk of memory as the ",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 11285613,
      "answer_id": 11287278,
      "question_score": 1796,
      "answer_score": 2738,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "select",
        "indexing"
      ]
    }
  },
  {
    "prompt": "How to change the order of DataFrame columns?\n\nI have the following DataFrame (df):\nimport numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.rand(10, 5))\n\nI add more column(s) by assignment:\ndf['mean'] = df.mean(1)\n\nHow can I move the column mean to the front, i.e. set it as first column leaving the order of the other columns untouched?",
    "response": "One easy way would be to reassign the dataframe with a list of the columns, rearranged as needed. \n\nThis is what you have now: \n\nIn [6]: df\nOut[6]:\n          0         1         2         3         4      mean\n0  0.445598  0.173835  0.343415  0.682252  0.582616  0.445543\n1  0.881592  0.696942  0.702232  0.696724  0.373551  0.670208\n2  0.662527  0.955193  0.131016  0.609548  0.804694  0.632596\n3  0.260919  0.783467  0.593433  0.033426  0.512019  0.436653\n4  0.131842  0.799367  0.182828  0.683330  0.019485  0.363371\n5  0.498784  0.873495  0.383811  0.699289  0.480447  0.587165\n6  0.388771  0.395757  0.745237  0.628406  0.784473  0.588529\n7  0.147986  0.459451  0.310961  0.706435  0.100914  0.345149\n8  0.394947  0.863494  0.585030  0.565944  0.356561  0.553195\n9  0.689260  0.865243  0.136481  0.386582  0.730399  0.561593\n\nIn [7]: cols = df.columns.tolist()\n\nIn [8]: cols\nOut[8]: [0L, 1L, 2L, 3L, 4L, 'mean']\n\n\nRearrange cols in any way you want. This is how I moved the last element to the f",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 13148429,
      "answer_id": 13148611,
      "question_score": 1681,
      "answer_score": 1433,
      "is_accepted": false,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "sorting",
        "indexing"
      ]
    }
  },
  {
    "prompt": "Change column type in pandas\n\nI created a DataFrame from a list of lists:\ntable = [\n    ['a',  '1.2',  '4.2' ],\n    ['b',  '70',   '0.03'],\n    ['x',  '5',    '0'   ],\n]\n\ndf = pd.DataFrame(table)\n\nHow do I convert the columns to specific types? In this case, I want to convert columns 2 and 3 into floats.\nIs there a way to specify the types while converting the list to DataFrame? Or is it better to create the DataFrame first and then loop through the columns to change the dtype for each column? Ideally I would like to do this",
    "response": "You have four main options for converting types in pandas:\n\nto_numeric() - provides functionality to safely convert non-numeric types (e.g. strings) to a suitable numeric type. (See also to_datetime() and to_timedelta().)\n\nastype() - convert (almost) any type to (almost) any other type (even if it's not necessarily sensible to do so). Also allows you to convert to categorial types (very useful).\n\ninfer_objects() - a utility method to convert object columns holding Python objects to a pandas type if possible.\n\nconvert_dtypes() - convert DataFrame columns to the \"best possible\"  dtype that supports pd.NA (pandas' object to indicate a missing value).\n\n\nRead on for more detailed explanations and usage of each of these methods.\n\n1. to_numeric()\nThe best way to convert one or more columns of a DataFrame to numeric values is to use pandas.to_numeric().\nThis function will try to change non-numeric objects (such as strings) into integers or floating-point numbers as appropriate.\nBasic usage\nThe",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 15891038,
      "answer_id": 28648923,
      "question_score": 1578,
      "answer_score": 2624,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "types",
        "type-conversion"
      ]
    }
  },
  {
    "prompt": "How to deal with SettingWithCopyWarning in Pandas\n\nI just upgraded my Pandas from 0.11 to 0.13.0rc1. Now, the application is popping out many new warnings. One of them like this:\nE:\\FinReporter\\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE\n\nWhat exactly does it mean?  Do I need to change something?\nHow should I suspend the warning if I insist on using quote_df['TVol']   = quote_df['T",
    "response": "The SettingWithCopyWarning was created to flag potentially confusing \"chained\" assignments, such as the following, which does not always work as expected, particularly when the first selection returns a copy.  [see GH5390 and GH5597 for background discussion.]\ndf[df['A'] > 2]['B'] = new_val  # new_val not set in df\n\nThe warning offers a suggestion to rewrite as follows:\ndf.loc[df['A'] > 2, 'B'] = new_val\n\nHowever, this doesn't fit your usage, which is equivalent to:\ndf = df[df['A'] > 2]\ndf['B'] = new_val\n\nWhile it's clear that you don't care about writes making it back to the original frame (since you are overwriting the reference to it), unfortunately this pattern cannot be differentiated from the first chained assignment example. Hence the (false positive) warning. The potential for false positives is addressed in the docs on indexing, if you'd like to read further.  You can safely disable this new warning with the following assignment.\nimport pandas as pd\npd.options.mode.chained_ass",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 20625582,
      "answer_id": 20627316,
      "question_score": 1487,
      "answer_score": 1723,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "chained-assignment",
        "pandas-settingwithcopy-warning"
      ]
    }
  },
  {
    "prompt": "How to drop rows of Pandas DataFrame whose value in a certain column is NaN\n\nI have this DataFrame and want only the records whose EPS column is not NaN:\n                 STK_ID  EPS  cash\nSTK_ID RPT_Date                   \n601166 20111231  601166  NaN   NaN\n600036 20111231  600036  NaN    12\n600016 20111231  600016  4.3   NaN\n601009 20111231  601009  NaN   NaN\n601939 20111231  601939  2.5   NaN\n000001 20111231  000001  NaN   NaN\n\n...i.e. something like df.drop(....) to get this resulting dataframe:\n                  STK_ID  EPS  cash\nSTK_ID RPT_Date                   \n6",
    "response": "Don't drop, just take the rows where EPS is not NA:\n\ndf = df[df['EPS'].notna()]",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 13413590,
      "answer_id": 13413845,
      "question_score": 1477,
      "answer_score": 1649,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "indexing",
        "nan"
      ]
    }
  },
  {
    "prompt": "Create a Pandas Dataframe by appending one row at a time\n\nHow do I create an empty DataFrame, then add rows, one by one?\nI created an empty DataFrame:\ndf = pd.DataFrame(columns=('lib', 'qty1', 'qty2'))\n\nThen I can add a new row at the end and fill a single field with:\ndf = df._set_value(index=len(df), col='qty1', value=10.0)\n\nIt works for only one field at a time. What is a better way to add new row to df?",
    "response": "You can use df.loc[i], where the row with index i will be what you specify it to be in the dataframe.\n>>> import pandas as pd\n>>> from numpy.random import randint\n\n>>> df = pd.DataFrame(columns=['lib', 'qty1', 'qty2'])\n>>> for i in range(5):\n>>>     df.loc[i] = ['name' + str(i)] + list(randint(10, size=2))\n\n>>> df\n     lib qty1 qty2\n0  name0    3    3\n1  name1    2    4\n2  name2    2    8\n3  name3    2    1\n4  name4    9    6",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 10715965,
      "answer_id": 24888331,
      "question_score": 1428,
      "answer_score": 924,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "append"
      ]
    }
  },
  {
    "prompt": "Get a list from Pandas DataFrame column headers\n\nI want to get a list of the column headers from a Pandas DataFrame.  The DataFrame will come from user input, so I won't know how many columns there will be or what they will be called.\nFor example, if I'm given a DataFrame like this:\n    y  gdp  cap\n0   1    2    5\n1   2    3    9\n2   8    7    2\n3   3    4    7\n4   6    7    7\n5   4    8    3\n6   8    2    8\n7   9    9   10\n8   6    6    4\n9  10   10    7\n\nI would get a list like this:\n['y', 'gdp', 'cap']",
    "response": "You can get the values as a list by doing:\nlist(my_dataframe.columns.values)\n\nAlso you can simply use (as shown in Ed Chum's answer):\nlist(my_dataframe)",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 19482970,
      "answer_id": 19483025,
      "question_score": 1391,
      "answer_score": 2060,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "list",
        "header"
      ]
    }
  },
  {
    "prompt": "Use a list of values to select rows from a Pandas dataframe\n\nLet’s say I have the following Pandas dataframe:\ndf = DataFrame({'A': [5,6,3,4], 'B': [1,2,3,5]})\ndf\n\n     A   B\n0    5   1\n1    6   2\n2    3   3\n3    4   5\n\nI can subset based on a specific value:\nx = df[df['A'] == 3]\nx\n\n     A   B\n2    3   3\n\nBut how can I subset based on a list of values? - something like this:\nlist_of_values = [3, 6]\n\ny = df[df['A'] in list_of_values]\n\nTo get:\n     A    B\n1    6    2\n2    3    3",
    "response": "You can use the isin method:\nIn [1]: df = pd.DataFrame({'A': [5,6,3,4], 'B': [1,2,3,5]})\n\nIn [2]: df\nOut[2]:\n   A  B\n0  5  1\n1  6  2\n2  3  3\n3  4  5\n\nIn [3]: df[df['A'].isin([3, 6])]\nOut[3]:\n   A  B\n1  6  2\n2  3  3\n\nAnd to get the opposite use ~:\nIn [4]: df[~df['A'].isin([3, 6])]\nOut[4]:\n   A  B\n0  5  1\n3  4  5",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 12096252,
      "answer_id": 12098586,
      "question_score": 1390,
      "answer_score": 2285,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "indexing",
        "filter"
      ]
    }
  },
  {
    "prompt": "How to add a new column to an existing DataFrame\n\nI have the following indexed DataFrame with named columns and rows not- continuous numbers:\n          a         b         c         d\n2  0.671399  0.101208 -0.181532  0.241273\n3  0.446172 -0.243316  0.051767  1.577318\n5  0.614758  0.075793 -0.451460 -0.012493\n\nI would like to add a new column, 'e', to the existing data frame and do not want to change anything in the data frame (i.e., the new column always has the same length as the DataFrame).\n0   -0.335485\n1   -1.166658\n2   -0.385571\ndtype: flo",
    "response": "Edit 2017\nAs indicated in the comments and by @Alexander, currently the best method to add the values of a Series as a new column of a DataFrame could be using assign:\ndf1 = df1.assign(e=pd.Series(np.random.randn(sLength)).values)\n\n\nEdit 2015\nSome reported getting the SettingWithCopyWarning with this code.\nHowever, the code still runs perfectly with the current pandas version 0.16.1.\n>>> sLength = len(df1['a'])\n>>> df1\n          a         b         c         d\n6 -0.269221 -0.026476  0.997517  1.294385\n8  0.917438  0.847941  0.034235 -0.448948\n\n>>> df1['e'] = pd.Series(np.random.randn(sLength), index=df1.index)\n>>> df1\n          a         b         c         d         e\n6 -0.269221 -0.026476  0.997517  1.294385  1.757167\n8  0.917438  0.847941  0.034235 -0.448948  2.228131\n\n>>> pd.version.short_version\n'0.16.1'\n\nThe SettingWithCopyWarning aims to inform of a possibly invalid assignment on a copy of the Dataframe. It doesn't necessarily say you did it wrong (it can trigger false positives",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 12555323,
      "answer_id": 12555510,
      "question_score": 1342,
      "answer_score": 1322,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "chained-assignment"
      ]
    }
  },
  {
    "prompt": "Pretty-print an entire Pandas Series / DataFrame\n\nI work with Series and DataFrames on the terminal a lot. The default __repr__ for a Series returns a reduced sample, with some head and tail values, but the rest missing.\n\nIs there a builtin way to pretty-print the entire Series / DataFrame?  Ideally, it would support proper alignment, perhaps borders between columns, and maybe even color-coding for the different columns.",
    "response": "You can also use the option_context, with one or more options:\n\nwith pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n    print(df)\n\n\nThis will automatically return the options to their previous values.\n\nIf you are working on jupyter-notebook, using display(df) instead of print(df) will use jupyter rich display logic (like so).",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 19124601,
      "answer_id": 30691921,
      "question_score": 1262,
      "answer_score": 1590,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe"
      ]
    }
  },
  {
    "prompt": "Convert list of dictionaries to a pandas DataFrame\n\nHow can I convert a list of dictionaries into a DataFrame?\nI want to turn\n[{'points': 50, 'time': '5:00', 'year': 2010}, \n {'points': 25, 'time': '6:00', 'month': \"february\"}, \n {'points':90, 'time': '9:00', 'month': 'january'}, \n {'points_h1':20, 'month': 'june'}]\n\ninto\n      month  points  points_h1  time  year\n0       NaN      50        NaN  5:00  2010\n1  february      25        NaN  6:00   NaN\n2   january      90        NaN  9:00   NaN\n3      june     NaN         20   NaN   NaN",
    "response": "If ds is a list of dicts:\ndf = pd.DataFrame(ds)\n\nNote: this does not work with nested data.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 20638006,
      "answer_id": 20638258,
      "question_score": 1212,
      "answer_score": 1692,
      "is_accepted": true,
      "tags": [
        "python",
        "dictionary",
        "pandas",
        "dataframe"
      ]
    }
  },
  {
    "prompt": "&quot;Large data&quot; workflows using pandas\n\nI have tried to puzzle out an answer to this question for many months while learning pandas.  I use SAS for my day-to-day work and it is great for it's out-of-core support.  However, SAS is horrible as a piece of software for numerous other reasons.\n\nOne day I hope to replace my use of SAS with python and pandas, but I currently lack an out-of-core workflow for large datasets.  I'm not talking about \"big data\" that requires a distributed network, but rather files too large to fit in memory but s",
    "response": "I routinely use tens of gigabytes of data in just this fashion\ne.g. I have tables on disk that I read via queries, create data and append back.\n\nIt's worth reading the docs and late in this thread for several suggestions for how to store your data.\n\nDetails which will affect how you store your data, like:\nGive as much detail as you can; and I can help you develop a structure.\n\n\nSize of data, # of rows, columns, types of columns; are you appending\nrows, or just columns? \nWhat will typical operations look like. E.g. do a query on columns to select a bunch of rows and specific columns, then do an operation (in-memory), create new columns, save these.\n(Giving a toy example could enable us to offer more specific recommendations.)\nAfter that processing, then what do you do? Is step 2 ad hoc, or repeatable?\nInput flat files: how many, rough total size in Gb. How are these organized e.g. by records? Does each one contains different fields, or do they have some records per file with all of the ",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 14262433,
      "answer_id": 14268804,
      "question_score": 1188,
      "answer_score": 719,
      "is_accepted": true,
      "tags": [
        "python",
        "mongodb",
        "pandas",
        "hdf5",
        "large-data"
      ]
    }
  },
  {
    "prompt": "Writing a pandas DataFrame to CSV file\n\nI have a dataframe in pandas which I would like to write to a CSV file.\nI am doing this using:\ndf.to_csv('out.csv')\n\nAnd getting the following error:\nUnicodeEncodeError: 'ascii' codec can't encode character u'\\u03b1' in position 20: ordinal not in range(128)\n\n\nIs there any way to get around this easily (i.e. I have unicode characters in my data frame)?\nAnd is there a way to write to a tab delimited file instead of a CSV using e.g. a 'to-tab' method (that I don't think exists)?",
    "response": "To delimit by a tab you can use the sep argument of to_csv:\ndf.to_csv(file_name, sep='\\t')\n\nTo use a specific encoding (e.g. 'utf-8') use the encoding argument:\ndf.to_csv(file_name, sep='\\t', encoding='utf-8')\n\nIn many cases you will want to remove the index and add a header:\ndf.to_csv(file_name, sep='\\t', encoding='utf-8', index=False, header=True)",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 16923281,
      "answer_id": 16923367,
      "question_score": 1164,
      "answer_score": 1551,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "csv",
        "file-io"
      ]
    }
  },
  {
    "prompt": "Deleting DataFrame row in Pandas based on column value\n\nI have the following DataFrame:\n             daysago  line_race rating        rw    wrating\n line_date                                                 \n2007-03-31       62         11     56  1.000000  56.000000\n2007-03-10       83         11     67  1.000000  67.000000\n2007-02-10      111          9     66  1.000000  66.000000\n2007-01-13      139         10     83  0.880678  73.096278\n2006-12-23      160         10     88  0.793033  69.786942\n2006-11-09      204          9     52  0.636655  33.1",
    "response": "If I'm understanding correctly, it should be as simple as:\n\ndf = df[df.line_race != 0]",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 18172851,
      "answer_id": 18173074,
      "question_score": 1059,
      "answer_score": 1610,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "performance",
        "delete-row"
      ]
    }
  },
  {
    "prompt": "How do I expand the output display to see more columns of a Pandas DataFrame?\n\nIs there a way to widen the display of output in either interactive or script-execution mode?\nSpecifically, I am using the describe() function on a Pandas DataFrame.  When the DataFrame is five columns (labels) wide, I get the descriptive statistics that I want.  However, if the DataFrame has any more columns, the statistics are suppressed and something like this is returned:\n>> Index: 8 entries, count to max\n>> Data columns:\n>> x1          8  non-null values\n>> x2          8  non-null values\n>>",
    "response": "(For Pandas versions before 0.23.4, see at bottom.)\nUse pandas.set_option(optname, val), or equivalently pd.options.<opt.hierarchical.name> = val. Like:\nimport pandas as pd\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nPandas will try to autodetect the size of your terminal window if you set pd.options.display.width = 0.\nHere is the help for set_option:\n\nset_option(pat,value) - Sets the value of the specified option\n\nAvailable options:\ndisplay.[chop_threshold, colheader_justify, column_space, date_dayfirst,\n         date_yearfirst, encoding, expand_frame_repr, float_format, height,\n         line_width, max_columns, max_colwidth, max_info_columns, max_info_rows,\n         max_rows, max_seq_items, mpl_style, multi_sparse, notebook_repr_html,\n         pprint_nest_depth, precision, width]\nmode.[sim_interactive, use_inf_as_null]\n\nParameters\n----------\npat - str/regexp which should match a single option.\n\nNote: partial ",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 11707586,
      "answer_id": 11711637,
      "question_score": 1044,
      "answer_score": 1512,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "printing",
        "column-width"
      ]
    }
  },
  {
    "prompt": "Combine two columns of text in pandas dataframe\n\nI have a dataframe that looks like\nYear  quarter\n2000       q2\n2001       q3\n\nHow do I add a new column by combining these columns to get the following dataframe?\nYear  quarter  period\n2000       q2  2000q2\n2001       q3  2001q3",
    "response": "If both columns are strings, you can concatenate them directly:\ndf[\"period\"] = df[\"Year\"] + df[\"quarter\"]\n\nIf one (or both) of the columns are not string typed, you should convert it (them) first,\ndf[\"period\"] = df[\"Year\"].astype(str) + df[\"quarter\"]\n\nBeware of NaNs when doing this!\n\nIf you need to join multiple string columns, you can use agg:\ndf['period'] = df[['Year', 'quarter', ...]].agg('-'.join, axis=1)\n\nWhere \"-\" is the separator.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 19377969,
      "answer_id": 19378497,
      "question_score": 1006,
      "answer_score": 1285,
      "is_accepted": false,
      "tags": [
        "python",
        "pandas",
        "string",
        "dataframe",
        "string-concatenation"
      ]
    }
  },
  {
    "prompt": "How are iloc and loc different?\n\nCan someone explain how these two methods of slicing are different? I've seen the docs\nand I've seen previous similar questions (1, 2), but I still find myself unable to understand how they are different. To me, they seem interchangeable in large part, because they are at the lower levels of slicing.\nFor example, say we want to get the first five rows of a DataFrame.  How is it that these two work?\ndf.loc[:5]\ndf.iloc[:5]\n\nCan someone present cases where the distinction in uses are clearer?\n\nOnce",
    "response": "Label vs. Location\nThe main distinction between the two methods is:\n\nloc gets rows (and/or columns) with particular labels.\n\niloc gets rows (and/or columns) at integer locations.\n\n\nTo demonstrate, consider a series s of characters with a non-monotonic integer index:\n>>> s = pd.Series(list(\"abcdef\"), index=[49, 48, 47, 0, 1, 2]) \n49    a\n48    b\n47    c\n0     d\n1     e\n2     f\n\n>>> s.loc[0]    # value at index label 0\n'd'\n\n>>> s.iloc[0]   # value at index location 0\n'a'\n\n>>> s.loc[0:1]  # rows at index labels between 0 and 1 (inclusive)\n0    d\n1    e\n\n>>> s.iloc[0:1] # rows at index location between 0 and 1 (exclusive)\n49    a\n\nHere are some of the differences/similarities between s.loc and s.iloc when passed various objects:\n\n\n\n\n<object>\ndescription\ns.loc[<object>]\ns.iloc[<object>]\n\n\n\n\n0\nsingle item\nValue at index label 0 (the string 'd')\nValue at index location 0 (the string 'a')\n\n\n0:1\nslice\nTwo rows (labels 0 and 1)\nOne row (first row at location 0)\n\n\n1:47\nslice with out-of-bounds en",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 31593201,
      "answer_id": 31593712,
      "question_score": 1001,
      "answer_score": 1604,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "indexing",
        "pandas-loc"
      ]
    }
  },
  {
    "prompt": "Pandas Merging 101\n\nHow can I perform a (INNER| (LEFT|RIGHT|FULL) OUTER) JOIN with pandas?\nHow do I add NaNs for missing rows after a merge?\nHow do I get rid of NaNs after merging?\nCan I merge on the index?\nHow do I merge multiple DataFrames?\nCross join with pandas\nmerge? join? concat? update? Who? What? Why?!\n\n... and more. I've seen these recurring questions asking about various facets of the pandas merge functionality. Most of the information regarding merge and its various use cases today is fragmented across d",
    "response": "This post aims to give readers a primer on SQL-flavored merging with Pandas, how to use it, and when not to use it.\nIn particular, here's what this post will go through:\n\nThe basics - types of joins (LEFT, RIGHT, OUTER, INNER)\n\nmerging with different column names\nmerging with multiple columns\navoiding duplicate merge key column in output\n\n\n\nWhat this post (and other posts by me on this thread) will not go through:\n\nPerformance-related discussions and timings (for now). Mostly notable mentions of better alternatives, wherever appropriate.\nHandling suffixes, removing extra columns, renaming outputs, and other specific use cases. There are other (read: better) posts that deal with that, so figure it out!\n\n\nNote\nMost examples default to INNER JOIN operations while demonstrating various features, unless otherwise specified.\nFurthermore, all the DataFrames here can be copied and replicated so\nyou can play with them. Also, see this\npost\non how to read DataFrames from your clipboard.\nLastly, a",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 53645882,
      "answer_id": 53645883,
      "question_score": 947,
      "answer_score": 1301,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "join",
        "merge",
        "concatenation"
      ]
    }
  },
  {
    "prompt": "Filter pandas DataFrame by substring criteria\n\nI have a pandas DataFrame with a column of string values. I need to select rows based on partial string matches.\nSomething like this idiom:\nre.search(pattern, cell_in_question) \n\nreturning a boolean. I am familiar with the syntax of df[df['A'] == \"hello world\"] but can't seem to find a way to do the same with a partial string match, say 'hello'.",
    "response": "Vectorized string methods (i.e. Series.str) let you do the following:\ndf[df['A'].str.contains(\"hello\")]\n\nThis is available in pandas 0.8.1 and up.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 11350770,
      "answer_id": 11531402,
      "question_score": 923,
      "answer_score": 1445,
      "is_accepted": false,
      "tags": [
        "python",
        "pandas",
        "regex",
        "string",
        "dataframe"
      ]
    }
  },
  {
    "prompt": "Creating an empty Pandas DataFrame, and then filling it\n\nI'm starting from the pandas DataFrame documentation here: Introduction to data structures\nI'd like to iteratively fill the DataFrame with values in a time series kind of calculation. I'd like to initialize the DataFrame with columns A, B, and timestamp rows, all 0 or all NaN.\nI'd then add initial values and go over this data calculating the new row from the row before, say row[A][t] = row[A][t-1]+1 or so.\nI'm currently using the code as below, but I feel it's kind of ugly and there must be a  w",
    "response": "Here's a couple of suggestions:\nUse date_range for the index:\nimport datetime\nimport pandas as pd\nimport numpy as np\n\ntodays_date = datetime.datetime.now().date()\nindex = pd.date_range(todays_date-datetime.timedelta(10), periods=10, freq='D')\n\ncolumns = ['A','B', 'C']\n\nNote: we could create an empty DataFrame (with NaNs) simply by writing:\ndf_ = pd.DataFrame(index=index, columns=columns)\ndf_ = df_.fillna(0) # With 0s rather than NaNs\n\nTo do these type of calculations for the data, use a NumPy array:\ndata = np.array([np.arange(10)]*3).T\n\nHence we can create the DataFrame:\nIn [10]: df = pd.DataFrame(data, index=index, columns=columns)\n\nIn [11]: df\nOut[11]:\n            A  B  C\n2012-11-29  0  0  0\n2012-11-30  1  1  1\n2012-12-01  2  2  2\n2012-12-02  3  3  3\n2012-12-03  4  4  4\n2012-12-04  5  5  5\n2012-12-05  6  6  6\n2012-12-06  7  7  7\n2012-12-07  8  8  8\n2012-12-08  9  9  9",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 13784192,
      "answer_id": 13786327,
      "question_score": 920,
      "answer_score": 427,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe"
      ]
    }
  },
  {
    "prompt": "Shuffle DataFrame rows\n\nI have the following DataFrame:\n    Col1  Col2  Col3  Type\n0      1     2     3     1\n1      4     5     6     1\n...\n20     7     8     9     2\n21    10    11    12     2\n...\n45    13    14    15     3\n46    16    17    18     3\n...\n\nThe DataFrame is read from a CSV file. All rows which have Type 1 are on top, followed by the rows with Type 2, followed by the rows with Type 3, etc.\nI would like to shuffle the order of the DataFrame's rows so that all Type's are mixed. A possible result could be:",
    "response": "The idiomatic way to do this with Pandas is to use the .sample method of your data frame to sample all rows without replacement:\ndf.sample(frac=1)\n\nThe frac keyword argument specifies the fraction of rows to return in the random sample, so frac=1 means to return all rows (in random order).\n\nNote:\nIf you wish to shuffle your dataframe in-place and reset the index, you could do e.g.\ndf = df.sample(frac=1).reset_index(drop=True)\n\nHere, specifying drop=True prevents .reset_index from creating a column containing the old index entries.\nFollow-up note: Although it may not look like the above operation is in-place, python/pandas is smart enough not to do another malloc for the shuffled object. That is, even though the reference object has changed (by which I mean id(df_old) is not the same as id(df_new)), the underlying C object is still the same. To show that this is indeed the case, you could run a simple memory profiler:\n$ python3 -m memory_profiler .\\test.py\nFilename: .\\test.py\n\nLine #   ",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 29576430,
      "answer_id": 34879805,
      "question_score": 901,
      "answer_score": 1617,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "permutation",
        "shuffle"
      ]
    }
  },
  {
    "prompt": "Truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()\n\nI want to filter my dataframe with an or condition to keep rows with a particular column's values that are outside the range [-0.25, 0.25]. I tried:\ndf = df[(df['col'] < -0.25) or (df['col'] > 0.25)]\n\nBut I get the error:\n\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
    "response": "The or and and Python statements require truth-values. For pandas, these are considered ambiguous, so you should use \"bitwise\" | (or) or & (and) operations:\ndf = df[(df['col'] < -0.25) | (df['col'] > 0.25)]\n\nThese are overloaded for these kinds of data structures to yield the element-wise or or and.\n\nJust to add some more explanation to this statement:\nThe exception is thrown when you want to get the bool of a pandas.Series:\n>>> import pandas as pd\n>>> x = pd.Series([1])\n>>> bool(x)\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n\nYou hit a place where the operator implicitly converted the operands to bool (you used or but it also happens for and, if and while):\n>>> x or x\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n>>> x and x\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n>>> if x:\n...     print('fun')\nVa",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 36921951,
      "answer_id": 36922103,
      "question_score": 899,
      "answer_score": 1161,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "filter",
        "conditional-statements",
        "valueerror"
      ]
    }
  },
  {
    "prompt": "How to filter Pandas dataframe using &#39;in&#39; and &#39;not in&#39; like in SQL\n\nHow can I achieve the equivalents of SQL's IN and NOT IN?\nI have a list with the required values. Here's the scenario:\ndf = pd.DataFrame({'country': ['US', 'UK', 'Germany', 'China']})\ncountries_to_keep = ['UK', 'China']\n\n# pseudo-code:\ndf[df['country'] not in countries_to_keep]\n\nMy current way of doing this is as follows:\ndf = pd.DataFrame({'country': ['US', 'UK', 'Germany', 'China']})\ndf2 = pd.DataFrame({'country': ['UK', 'China'], 'matched': True})\n\n# IN\ndf.merge(df2, how='inner', on='country'",
    "response": "You can use pd.Series.isin.\nFor \"IN\" use: something.isin(somewhere)\nOr for \"NOT IN\": ~something.isin(somewhere)\nAs a worked example:\n>>> df\n    country\n0        US\n1        UK\n2   Germany\n3     China\n>>> countries_to_keep\n['UK', 'China']\n>>> df.country.isin(countries_to_keep)\n0    False\n1     True\n2    False\n3     True\nName: country, dtype: bool\n>>> df[df.country.isin(countries_to_keep)]\n    country\n1        UK\n3     China\n>>> df[~df.country.isin(countries_to_keep)]\n    country\n0        US\n2   Germany",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 19960077,
      "answer_id": 19960116,
      "question_score": 897,
      "answer_score": 1550,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "indexing",
        "filter"
      ]
    }
  },
  {
    "prompt": "Constructing DataFrame from values in variables yields &quot;ValueError: If using all scalar values, you must pass an index&quot;\n\nI have two variables as follows.\na = 2\nb = 3\n\nI want to construct a DataFrame from this:\ndf2 = pd.DataFrame({'A':a, 'B':b})\n\nThis generates an error:\nValueError: If using all scalar values, you must pass an index\n\nI tried this also:\ndf2 = (pd.DataFrame({'a':a, 'b':b})).reset_index()\n\nThis gives the same error message. How do I do what I want?",
    "response": "The error message says that if you're passing scalar values, you have to pass an index.  So you can either not use scalar values for the columns -- e.g. use a list:\n>>> df = pd.DataFrame({'A': [a], 'B': [b]})\n>>> df\n   A  B\n0  2  3\n\nor use scalar values and pass an index:\n>>> df = pd.DataFrame({'A': a, 'B': b}, index=[0, 3])\n>>> df\n   A  B\n0  2  3\n3  2  3",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 17839973,
      "answer_id": 17840195,
      "question_score": 868,
      "answer_score": 1254,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "constructor",
        "valueerror"
      ]
    }
  },
  {
    "prompt": "How to convert index of a pandas dataframe into a column\n\nHow to convert an index of a dataframe into a column?\nFor example:\n        gi       ptt_loc\n 0  384444683      593  \n 1  384444684      594 \n 2  384444686      596  \n\nto\n    index1    gi       ptt_loc\n 0  0     384444683      593  \n 1  1     384444684      594 \n 2  2     384444686      596",
    "response": "either:\ndf['index1'] = df.index\n\nor .reset_index:\ndf = df.reset_index()\n\n\nIf you have a multi-index frame with 3 levels of index, like:\n>>> df\n                       val\ntick       tag obs        \n2016-02-26 C   2    0.0139\n2016-02-27 A   2    0.5577\n2016-02-28 C   6    0.0303\n\nand you want to convert the 1st (tick) and 3rd (obs) levels in the index into columns, you could do:\n>>> df.reset_index(level=['tick', 'obs'])\n          tick  obs     val\ntag                        \nC   2016-02-26    2  0.0139\nA   2016-02-27    2  0.5577\nC   2016-02-28    6  0.0303",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 20461165,
      "answer_id": 20461206,
      "question_score": 860,
      "answer_score": 1396,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "join",
        "indexing"
      ]
    }
  },
  {
    "prompt": "How do I count the NaN values in a column in pandas DataFrame?\n\nI want to find the number of NaN in each column of my data.",
    "response": "Use the isna() method (or it's alias isnull() which is also compatible with older pandas versions < 0.21.0) and then sum to count the NaN values. For one column:\n>>> s = pd.Series([1,2,3, np.nan, np.nan])\n\n>>> s.isna().sum()   # or s.isnull().sum() for older pandas versions\n2\n\nFor several columns, this also works:\n>>> df = pd.DataFrame({'a':[1,2,np.nan], 'b':[np.nan,1,np.nan]})\n\n>>> df.isna().sum()\na    1\nb    2\ndtype: int64",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 26266362,
      "answer_id": 26266451,
      "question_score": 822,
      "answer_score": 1152,
      "is_accepted": false,
      "tags": [
        "python",
        "pandas",
        "dataframe"
      ]
    }
  },
  {
    "prompt": "Get statistics for each group (such as count, mean, etc) using pandas GroupBy?\n\nI have a dataframe df and I use several columns from it to groupby:\ndf[['col1','col2','col3','col4']].groupby(['col1','col2']).mean()\n\nIn the above way, I almost get the table (dataframe) that I need. What is missing is an additional column that contains number of rows in each group. In other words, I have mean but I also would like to know how many were used to get these means. For example in the first group there are 8 values and in the second one 10 and so on.\nIn short: How do I get group-wis",
    "response": "On groupby object, the agg function can take a list to apply several aggregation methods at once. This should give you the result you need:\n\ndf[['col1', 'col2', 'col3', 'col4']].groupby(['col1', 'col2']).agg(['mean', 'count'])",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 19384532,
      "answer_id": 19385591,
      "question_score": 818,
      "answer_score": 636,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "group-by",
        "statistics"
      ]
    }
  },
  {
    "prompt": "Set value for particular cell in pandas DataFrame using index\n\nI have created a Pandas DataFrame\ndf = DataFrame(index=['A','B','C'], columns=['x','y'])\n\nNow, I would like to assign a value to particular cell, for example to row C and column x. In other words, I would like to perform the following transformation:\n     x    y             x    y\nA  NaN  NaN        A  NaN  NaN\nB  NaN  NaN   ⟶   B  NaN  NaN\nC  NaN  NaN        C   10  NaN\n\nwith this code:\ndf.xs('C')['x'] = 10\n\nHowever, the contents of df has not changed. The dataframe contains yet again only NaNs",
    "response": "RukTech's answer, df.set_value('C', 'x', 10), is far and away faster than the options I've suggested below. However, it has been slated for deprecation.\n\nGoing forward, the recommended method is .iat/.at.\n\n\n\nWhy df.xs('C')['x']=10 does not work:\n\ndf.xs('C') by default, returns a new dataframe with a copy of the data, so \n\ndf.xs('C')['x']=10\n\n\nmodifies this new dataframe only.\n\ndf['x'] returns a view of the df dataframe, so \n\ndf['x']['C'] = 10\n\n\nmodifies df itself.\n\nWarning: It is sometimes difficult to predict if an operation returns a copy or a view. For this reason the docs recommend avoiding assignments with \"chained indexing\".  \n\n\n\nSo the recommended alternative is\n\ndf.at['C', 'x'] = 10\n\n\nwhich does modify df.\n\n\n\nIn [18]: %timeit df.set_value('C', 'x', 10)\n100000 loops, best of 3: 2.9 µs per loop\n\nIn [20]: %timeit df['x']['C'] = 10\n100000 loops, best of 3: 6.31 µs per loop\n\nIn [81]: %timeit df.at['C', 'x'] = 10\n100000 loops, best of 3: 9.2 µs per loop",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 13842088,
      "answer_id": 13842286,
      "question_score": 804,
      "answer_score": 955,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "cell",
        "nan"
      ]
    }
  },
  {
    "prompt": "Import multiple CSV files into pandas and concatenate into one DataFrame\n\nI would like to read several CSV files from a directory into pandas and concatenate them into one big DataFrame. I have not been able to figure it out though. Here is what I have so far:\nimport glob\nimport pandas as pd\n\n# Get data file names\npath = r'C:\\DRO\\DCL_rawdata_files'\nfilenames = glob.glob(path + \"/*.csv\")\n\ndfs = []\nfor filename in filenames:\n    dfs.append(pd.read_csv(filename))\n\n# Concatenate all data into one DataFrame\nbig_frame = pd.concat(dfs, ignore_index=True)\n\nI guess I need some",
    "response": "See pandas: IO tools for all of the available .read_ methods.\nTry the following code if all of the CSV files have the same columns.\nI have added header=0, so that after reading the CSV file's first row, it can be assigned as the column names.\nimport pandas as pd\nimport glob\nimport os\n\npath = r'C:\\DRO\\DCL_rawdata_files' # use your path\nall_files = glob.glob(os.path.join(path , \"/*.csv\"))\n\nli = []\n\nfor filename in all_files:\n    df = pd.read_csv(filename, index_col=None, header=0)\n    li.append(df)\n\nframe = pd.concat(li, axis=0, ignore_index=True)\n\nOr, with attribution to a comment from Sid.\nall_files = glob.glob(os.path.join(path, \"*.csv\"))\n\ndf = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)\n\n\n\nIt's often necessary to identify each sample of data, which can be accomplished by adding a new column to the dataframe.\npathlib from the standard library will be used for this example. It treats paths as objects with methods, instead of strings to be sliced.\n\nImports and Setu",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 20906474,
      "answer_id": 21232849,
      "question_score": 781,
      "answer_score": 906,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "csv",
        "dataframe",
        "concatenation"
      ]
    }
  },
  {
    "prompt": "How to avoid pandas creating an index in a saved csv\n\nI am trying to save a csv to a folder after making some edits to the file. \n\nEvery time I use pd.to_csv('C:/Path of file.csv') the csv file has a separate column of indexes. I want to avoid printing the index to csv.\n\nI tried: \n\npd.read_csv('C:/Path to file to edit.csv', index_col = False)\n\n\nAnd to save the file...\n\npd.to_csv('C:/Path to save edited file.csv', index_col = False)\n\n\nHowever, I still got the unwanted index column. How can I avoid this when I save my files?",
    "response": "Use index=False.\n\ndf.to_csv('your.csv', index=False)",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 20845213,
      "answer_id": 25230582,
      "question_score": 780,
      "answer_score": 1173,
      "is_accepted": false,
      "tags": [
        "python",
        "csv",
        "indexing",
        "pandas"
      ]
    }
  },
  {
    "prompt": "How to apply a function to two columns of Pandas dataframe\n\nSuppose I have a function and a dataframe defined as below:\ndef get_sublist(sta, end):\n    return mylist[sta:end+1]\n\ndf = pd.DataFrame({'ID':['1','2','3'], 'col_1': [0,2,3], 'col_2':[1,4,5]})\nmylist = ['a','b','c','d','e','f']\n\nNow I want to apply get_sublist to df's two columns 'col_1', 'col_2' to element-wise calculate a new column 'col_3' to get an output that looks like:\n  ID  col_1  col_2            col_3\n0  1      0      1       ['a', 'b']\n1  2      2      4  ['c', 'd', 'e']\n2  3      3   ",
    "response": "There is a clean, one-line way of doing this in Pandas:\n\ndf['col_3'] = df.apply(lambda x: f(x.col_1, x.col_2), axis=1)\n\n\nThis allows f to be a user-defined function with multiple input values, and uses (safe) column names rather than (unsafe) numeric indices to access the columns.\n\nExample with data (based on original question):\n\nimport pandas as pd\n\ndf = pd.DataFrame({'ID':['1', '2', '3'], 'col_1': [0, 2, 3], 'col_2':[1, 4, 5]})\nmylist = ['a', 'b', 'c', 'd', 'e', 'f']\n\ndef get_sublist(sta,end):\n    return mylist[sta:end+1]\n\ndf['col_3'] = df.apply(lambda x: get_sublist(x.col_1, x.col_2), axis=1)\n\n\nOutput of print(df):\n\n  ID  col_1  col_2      col_3\n0  1      0      1     [a, b]\n1  2      2      4  [c, d, e]\n2  3      3      5  [d, e, f]\n\n\nIf your column names contain spaces or share a name with an existing dataframe attribute, you can index with square brackets:\n\ndf['col_3'] = df.apply(lambda x: f(x['col 1'], x['col 2']), axis=1)",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 13331698,
      "answer_id": 52854800,
      "question_score": 768,
      "answer_score": 715,
      "is_accepted": false,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "function",
        "typeerror"
      ]
    }
  },
  {
    "prompt": "Difference between map, applymap and apply methods in Pandas\n\nCan you tell me when to use these vectorization methods with basic examples? \n\nI see that map is a Series method whereas the rest are DataFrame methods. I got confused about apply and applymap methods though. Why do we have two methods for applying a function to a DataFrame? Again, simple examples which illustrate the usage would be great!",
    "response": "Comparing map, applymap and apply: Context Matters\nThe major differences are:\nDefinition\n\nmap is defined on Series only\napplymap is defined on DataFrames only\napply is defined on both\n\nInput argument\n\nmap accepts dict, Series, or callable\napplymap and apply accept callable only\n\nBehavior\n\nmap is elementwise for Series\napplymap is elementwise for DataFrames\napply also works elementwise but is suited to more complex operations and aggregation. The behaviour and return value depends on the function.\n\nUse case (the most important difference)\n\nmap is meant for mapping values from one domain to another, so is optimised for performance, e.g.,\ndf['A'].map({1:'a', 2:'b', 3:'c'})\n\n\napplymap is good for elementwise transformations across multiple rows/columns, e.g.,\ndf[['A', 'B', 'C']].applymap(str.strip)\n\n\napply is for applying any function that cannot be vectorised, e.g.,\ndf['sentences'].apply(nltk.sent_tokenize)\n\n\n\nAlso see When should I (not) want to use pandas apply() in my code? for a write",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 19798153,
      "answer_id": 56300992,
      "question_score": 756,
      "answer_score": 372,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "vectorization"
      ]
    }
  },
  {
    "prompt": "How to check if any value is NaN in a Pandas DataFrame\n\nHow do I check whether a pandas DataFrame has NaN values?\nI know about pd.isnan but it returns a DataFrame of booleans. I also found this post but it doesn't exactly answer my question either.",
    "response": "jwilner's response is spot on. I was exploring to see if there's a faster option, since in my experience, summing flat arrays is (strangely) faster than counting. This code seems faster:\ndf.isnull().values.any()\n\n\nimport numpy as np\nimport pandas as pd\nimport perfplot\n\n\ndef setup(n):\n    df = pd.DataFrame(np.random.randn(n))\n    df[df > 0.9] = np.nan\n    return df\n\n\ndef isnull_any(df):\n    return df.isnull().any()\n\n\ndef isnull_values_sum(df):\n    return df.isnull().values.sum() > 0\n\n\ndef isnull_sum(df):\n    return df.isnull().sum() > 0\n\n\ndef isnull_values_any(df):\n    return df.isnull().values.any()\n\n\nperfplot.save(\n    \"out.png\",\n    setup=setup,\n    kernels=[isnull_any, isnull_values_sum, isnull_sum, isnull_values_any],\n    n_range=[2 ** k for k in range(25)],\n)\n\ndf.isnull().sum().sum() is a bit slower, but of course, has additional information -- the number of NaNs.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 29530232,
      "answer_id": 29530601,
      "question_score": 752,
      "answer_score": 889,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "nan"
      ]
    }
  },
  {
    "prompt": "How can I get a value from a cell of a dataframe?\n\nI have constructed a condition that extracts exactly one row from my dataframe:\nd2 = df[(df['l_ext']==l_ext) & (df['item']==item) & (df['wn']==wn) & (df['wd']==1)]\n\nNow I would like to take a value from a particular column:\nval = d2['col_name']\n\nBut as a result, I get a dataframe that contains one row and one column (i.e., one cell). It is not what I need. I need one value (one float number). How can I do it in pandas?",
    "response": "If you have a DataFrame with only one row, then access the first (only) row as a Series using iloc, and then the value using the column name:\nIn [3]: sub_df\nOut[3]:\n          A         B\n2 -0.133653 -0.030854\n\nIn [4]: sub_df.iloc[0]\nOut[4]:\nA   -0.133653\nB   -0.030854\nName: 2, dtype: float64\n\nIn [5]: sub_df.iloc[0]['A']\nOut[5]: -0.13365288513107493",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 16729574,
      "answer_id": 16729808,
      "question_score": 751,
      "answer_score": 836,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "indexing",
        "filter"
      ]
    }
  },
  {
    "prompt": "UnicodeDecodeError when reading CSV file in Pandas\n\nI'm running a program which is processing 30,000 similar files. A random number of them are stopping and producing this error...\n  File \"C:\\Importer\\src\\dfman\\importer.py\", line 26, in import_chr\n    data = pd.read_csv(filepath, names=fields)\n  File \"C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py\", line 400, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File \"C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py\", line 205, in _read\n    return parser.read()\n  File \"C:\\Python33\\lib\\si",
    "response": "read_csv takes an encoding option to deal with files in different formats. I mostly use read_csv('file', encoding = \"ISO-8859-1\"), or alternatively encoding = \"utf-8\" for reading, and generally utf-8 for to_csv.\nYou can also use one of several alias options like 'latin' or 'cp1252' (Windows) instead of 'ISO-8859-1' (see python docs, also for numerous other encodings you may encounter).\nSee relevant Pandas documentation,\npython docs examples on csv files, and plenty of related questions here on SO. A good background resource is What every developer should know about unicode and character sets.\nTo detect the encoding (assuming the file contains non-ascii characters), you can use enca (see man page) or file -i (linux) or file -I (osx) (see man page).",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 18171739,
      "answer_id": 18172249,
      "question_score": 718,
      "answer_score": 1239,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "csv",
        "dataframe",
        "unicode"
      ]
    }
  },
  {
    "prompt": "How do I check if a pandas DataFrame is empty?\n\nHow do I check if a pandas DataFrame is empty? I'd like to print some message in the terminal if the DataFrame is empty.",
    "response": "You can use the attribute df.empty to check whether it's empty or not:\nif df.empty:\n    print('DataFrame is empty!')\n\nSource: Pandas Documentation",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 19828822,
      "answer_id": 19828967,
      "question_score": 718,
      "answer_score": 1164,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "membership",
        "any"
      ]
    }
  },
  {
    "prompt": "Convert Pandas dataframe to NumPy array\n\nHow do I convert a Pandas dataframe into a NumPy array?\nimport numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(\n    {\n        'A': [np.nan, np.nan, np.nan, 0.1, 0.1, 0.1, 0.1],\n        'B': [0.2, np.nan, 0.2, 0.2, 0.2, np.nan, np.nan],\n        'C': [np.nan, 0.5, 0.5, np.nan, 0.5, 0.5, np.nan],\n    },\n    index=[1, 2, 3, 4, 5, 6, 7],\n).rename_axis('ID')\n\nThat gives this DataFrame:\n      A    B    C\nID                                 \n1   NaN  0.2  NaN\n2   NaN  NaN  0.5\n3   NaN  0.2  0.5\n4   0.",
    "response": "Use df.to_numpy()\nIt's better than df.values, here's why.*\nIt's time to deprecate your usage of values and as_matrix().\npandas v0.24.0 introduced two new methods for obtaining NumPy arrays from pandas objects:\n\nto_numpy(), which is defined on Index, Series, and DataFrame objects, and\narray, which is defined on Index and Series objects only.\n\nIf you visit the v0.24 docs for .values, you will see a big red warning that says:\n\nWarning: We recommend using DataFrame.to_numpy() instead.\n\nSee this section of the v0.24.0 release notes, and this answer for more information.\n* - to_numpy() is my recommended method for any production code that needs to run reliably for many versions into the future. However if you're just making a scratchpad in jupyter or the terminal, using .values to save a few milliseconds of typing is a permissable exception. You can always add the fit n finish later.\n\n\nTowards Better Consistency: to_numpy()\nIn the spirit of better consistency throughout the API, a new method",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 13187778,
      "answer_id": 54508052,
      "question_score": 717,
      "answer_score": 633,
      "is_accepted": false,
      "tags": [
        "python",
        "arrays",
        "pandas",
        "numpy",
        "dataframe"
      ]
    }
  },
  {
    "prompt": "pandas.parser.CParserError: Error tokenizing data\n\nI'm trying to use pandas to manipulate a .csv file but I get this error:\n\npandas.parser.CParserError: Error tokenizing data. C error: Expected 2 fields in line 3,  saw 12\n\nI have tried to read the pandas docs, but found nothing.\nMy code is simple:\npath = 'GOOG Key Ratios.csv'\n#print(open(path).read())\ndata = pd.read_csv(path)\n\nHow can I resolve this? Should I use the csv module or another language?",
    "response": "you could also try;\ndata = pd.read_csv('file1.csv', on_bad_lines='skip')\n\nDo note that this will cause the offending lines to be skipped. If you don't expect many bad lines and want to (at least) know their amount and IDs, use on_bad_lines='warn'. For advanced handling of bads, you can pass a callable.\nEdit\nFor Pandas < 1.3.0 try\ndata = pd.read_csv(\"file1.csv\", error_bad_lines=False)\n\nas per pandas API reference.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 18039057,
      "answer_id": 18129082,
      "question_score": 715,
      "answer_score": 1069,
      "is_accepted": true,
      "tags": [
        "python",
        "csv",
        "pandas"
      ]
    }
  },
  {
    "prompt": "How to sort pandas dataframe by one column\n\nI have a dataframe like this:\n        0          1     2\n0   354.7      April   4.0\n1    55.4     August   8.0\n2   176.5   December  12.0\n3    95.5   February   2.0\n4    85.6    January   1.0\n5     152       July   7.0\n6   238.7       June   6.0\n7   104.8      March   3.0\n8   283.5        May   5.0\n9   278.8   November  11.0\n10  249.6    October  10.0\n11  212.7  September   9.0\n\nAs you can see, months are not in calendar order. So I created a second column to get the month number corresponding t",
    "response": "Use sort_values to sort the df by a specific column's values:\n\nIn [18]:\ndf.sort_values('2')\n\nOut[18]:\n        0          1     2\n4    85.6    January   1.0\n3    95.5   February   2.0\n7   104.8      March   3.0\n0   354.7      April   4.0\n8   283.5        May   5.0\n6   238.7       June   6.0\n5   152.0       July   7.0\n1    55.4     August   8.0\n11  212.7  September   9.0\n10  249.6    October  10.0\n9   278.8   November  11.0\n2   176.5   December  12.0\n\n\nIf you want to sort by two columns, pass a list of column labels to sort_values with the column labels ordered according to sort priority.  If you use df.sort_values(['2', '0']), the result would be sorted by column 2 then column 0. Granted, this does not really make sense for this example because each value in df['2'] is unique.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 37787698,
      "answer_id": 37787724,
      "question_score": 710,
      "answer_score": 861,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "sorting",
        "datetime"
      ]
    }
  },
  {
    "prompt": "How to delete rows from a pandas DataFrame based on a conditional expression\n\nI have a pandas DataFrame and I want to delete rows from it where the length of the string in a particular column is greater than 2.\n\nI expect to be able to do this (per this answer):\n\ndf[(len(df['column name']) < 2)]\n\n\nbut I just get the error:\n\nKeyError: u'no item named False'\n\n\nWhat am I doing wrong?\n\n(Note: I know I can use df.dropna() to get rid of rows that contain any NaN, but I didn't see how to remove rows based on a conditional expression.)",
    "response": "When you do len(df['column name']) you are just getting one number, namely the number of rows in the DataFrame (i.e., the length of the column itself).  If you want to apply len to each element in the column, use df['column name'].map(len).  So try\n\ndf[df['column name'].map(len) < 2]",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 13851535,
      "answer_id": 13851602,
      "question_score": 703,
      "answer_score": 280,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas"
      ]
    }
  },
  {
    "prompt": "Converting a Pandas GroupBy multiindex output from Series back to DataFrame\n\nI have a dataframe:\n   City     Name\n0   Seattle    Alice\n1   Seattle      Bob\n2  Portland  Mallory\n3   Seattle  Mallory\n4   Seattle      Bob\n5  Portland  Mallory\n\nI perform the following grouping:\ng1 = df1.groupby([\"Name\", \"City\"]).count()\n\nwhich when printed looks like:\n                  City  Name\nName    City\nAlice   Seattle      1     1\nBob     Seattle      2     2\nMallory Portland     2     2\n        Seattle      1     1\n\nBut what I want eventually is another DataFrame object that contains",
    "response": "g1 here is a DataFrame. It has a hierarchical index, though:\n\nIn [19]: type(g1)\nOut[19]: pandas.core.frame.DataFrame\n\nIn [20]: g1.index\nOut[20]: \nMultiIndex([('Alice', 'Seattle'), ('Bob', 'Seattle'), ('Mallory', 'Portland'),\n       ('Mallory', 'Seattle')], dtype=object)\n\n\nPerhaps you want something like this?\n\nIn [21]: g1.add_suffix('_Count').reset_index()\nOut[21]: \n      Name      City  City_Count  Name_Count\n0    Alice   Seattle           1           1\n1      Bob   Seattle           2           2\n2  Mallory  Portland           2           2\n3  Mallory   Seattle           1           1\n\n\nOr something like:\n\nIn [36]: DataFrame({'count' : df1.groupby( [ \"Name\", \"City\"] ).size()}).reset_index()\nOut[36]: \n      Name      City  count\n0    Alice   Seattle      1\n1      Bob   Seattle      2\n2  Mallory  Portland      2\n3  Mallory   Seattle      1",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 10373660,
      "answer_id": 10374456,
      "question_score": 695,
      "answer_score": 708,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "group-by",
        "multi-index"
      ]
    }
  },
  {
    "prompt": "How to replace NaN values in a dataframe column\n\nI have a Pandas Dataframe as below:\n      itm Date                  Amount \n67    420 2012-09-30 00:00:00   65211\n68    421 2012-09-09 00:00:00   29424\n69    421 2012-09-16 00:00:00   29877\n70    421 2012-09-23 00:00:00   30990\n71    421 2012-09-30 00:00:00   61303\n72    485 2012-09-09 00:00:00   71781\n73    485 2012-09-16 00:00:00     NaN\n74    485 2012-09-23 00:00:00   11072\n75    485 2012-09-30 00:00:00  113702\n76    489 2012-09-09 00:00:00   64731\n77    489 2012-09-16 00:00:00     NaN\n\nWhen ",
    "response": "DataFrame.fillna() or Series.fillna() will do this for you.\nExample:\nIn [7]: df\nOut[7]: \n          0         1\n0       NaN       NaN\n1 -0.494375  0.570994\n2       NaN       NaN\n3  1.876360 -0.229738\n4       NaN       NaN\n\nIn [8]: df.fillna(0)\nOut[8]: \n          0         1\n0  0.000000  0.000000\n1 -0.494375  0.570994\n2  0.000000  0.000000\n3  1.876360 -0.229738\n4  0.000000  0.000000\n\nTo fill the NaNs in only one column, select just that column.\nIn [12]: df[1] = df[1].fillna(0)\n\nIn [13]: df\nOut[13]: \n          0         1\n0       NaN  0.000000\n1 -0.494375  0.570994\n2       NaN  0.000000\n3  1.876360 -0.229738\n4       NaN  0.000000\n\nOr you can use the built in column-specific functionality:\ndf = df.fillna({1: 0})",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 13295735,
      "answer_id": 13295801,
      "question_score": 678,
      "answer_score": 1019,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "nan",
        "fillna"
      ]
    }
  },
  {
    "prompt": "How to check if a column exists in Pandas\n\nHow do I check if a column exists in a Pandas DataFrame df?\n   A   B    C\n0  3  40  100\n1  6  30  200\n\nHow would I check if the column \"A\" exists in the above DataFrame so that I can compute:\ndf['sum'] = df['A'] + df['C']\n\nAnd if \"A\" doesn't exist:\ndf['sum'] = df['B'] + df['C']",
    "response": "This will work:\n\nif 'A' in df:\n\n\nBut for clarity, I'd probably write it as:\n\nif 'A' in df.columns:",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 24870306,
      "answer_id": 24870404,
      "question_score": 663,
      "answer_score": 1282,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "membership",
        "any"
      ]
    }
  },
  {
    "prompt": "Create new column based on values from other columns / apply a function of multiple columns, row-wise in Pandas\n\nI want to apply my custom function (it uses an if-else ladder) to these six columns (ERI_Hispanic, ERI_AmerInd_AKNatv, ERI_Asian, ERI_Black_Afr.Amer, ERI_HI_PacIsl, ERI_White) in each row of my dataframe.\nI've tried different methods from other questions but still can't seem to find the right answer for my problem.  The critical piece of this is that if the person is counted as Hispanic they can't be counted as anything else.  Even if they have a \"1\" in another ethnicity column they still are co",
    "response": "OK, two steps to this - first is to write a function that does the translation you want - I've put an example together based on your pseudo-code:\ndef label_race(row):\n   if row['eri_hispanic'] == 1:\n      return 'Hispanic'\n   if row['eri_afr_amer'] + row['eri_asian'] + row['eri_hawaiian'] + row['eri_nat_amer'] + row['eri_white'] > 1:\n      return 'Two Or More'\n   if row['eri_nat_amer'] == 1:\n      return 'A/I AK Native'\n   if row['eri_asian'] == 1:\n      return 'Asian'\n   if row['eri_afr_amer'] == 1:\n      return 'Black/AA'\n   if row['eri_hawaiian'] == 1:\n      return 'Haw/Pac Isl.'\n   if row['eri_white'] == 1:\n      return 'White'\n   return 'Other'\n\nYou may want to go over this, but it seems to do the trick - notice that the parameter going into the function is considered to be a Series object labelled \"row\".\nNext, use the apply function in pandas to apply the function - e.g.\ndf.apply(label_race, axis=1)\n\nNote the axis=1 specifier, that means that the application is done at a row, rat",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 26886653,
      "answer_id": 26887820,
      "question_score": 647,
      "answer_score": 795,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "conditional-statements",
        "switch-statement"
      ]
    }
  },
  {
    "prompt": "How can I pivot a dataframe?\n\nHow do I pivot the pandas dataframe below such that the col values become columns, row values become the index, and mean of val0 becomes the values? (In some cases this is called transforming from long-format to wide-format.)\nConsider a dataframe df with columns 'key', 'row', 'item', 'col', and random float values 'val0', 'val1'. I conspicuously named the columns and relevant column values to correspond with how I want to pivot them. (Setup code at bottom.)\n     key   row   item   col  val0  val",
    "response": "Here is a list of idioms we can use to pivot\n\npd.DataFrame.pivot_table\n\nA glorified version of groupby with more intuitive API.  For many people, this is the preferred approach.  And it is the intended approach by the developers.\nSpecify row level, column levels, values to be aggregated, and function(s) to perform aggregations.\n\n\npd.DataFrame.groupby + pd.DataFrame.unstack\n\nGood general approach for doing just about any type of pivot\nYou specify all columns that will constitute the pivoted row levels and column levels in one group by.  You follow that by selecting the remaining columns you want to aggregate and the function(s) you want to perform the aggregation.  Finally, you unstack the levels that you want to be in the column index.\n\n\npd.DataFrame.set_index + pd.DataFrame.unstack\n\nConvenient and intuitive for some (myself included).  Cannot handle duplicate grouped keys.\nSimilar to the groupby paradigm, we specify all columns that will eventually be either row or column levels and s",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 47152691,
      "answer_id": 47152692,
      "question_score": 630,
      "answer_score": 481,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "group-by",
        "pivot",
        "pivot-table"
      ]
    }
  },
  {
    "prompt": "Remap values in pandas column with a dict, preserve NaNs\n\nI have a dictionary which looks like this: di = {1: \"A\", 2: \"B\"}\nI would like to apply it to the col1 column of a dataframe similar to:\n     col1   col2\n0       w      a\n1       1      2\n2       2    NaN\n\nto get:\n     col1   col2\n0       w      a\n1       A      2\n2       B    NaN\n\nHow can I best do this?",
    "response": "You can use .replace.  For example:\n\n>>> df = pd.DataFrame({'col2': {0: 'a', 1: 2, 2: np.nan}, 'col1': {0: 'w', 1: 1, 2: 2}})\n>>> di = {1: \"A\", 2: \"B\"}\n>>> df\n  col1 col2\n0    w    a\n1    1    2\n2    2  NaN\n>>> df.replace({\"col1\": di})\n  col1 col2\n0    w    a\n1    A    2\n2    B  NaN\n\n\nor directly on the Series, i.e. df[\"col1\"].replace(di, inplace=True).",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 20250771,
      "answer_id": 20250996,
      "question_score": 616,
      "answer_score": 626,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "dictionary",
        "replace"
      ]
    }
  },
  {
    "prompt": "Convert Python dict into a dataframe\n\nI have a Python dictionary:\n{u'2012-07-01': 391,\n u'2012-07-02': 392,\n u'2012-07-03': 392,\n u'2012-07-04': 392,\n u'2012-07-05': 392,\n u'2012-07-06': 392}\n\nI would like to convert this into a pandas dataframe by having the dates and their corresponding values as two separate columns; the expected result looks like:\n     Date         DateValue\n0    2012-07-01    391\n1    2012-07-02    392\n2    2012-07-03    392\n.    2012-07-04    392\n.    ...           ...\n\nIs there a direct way to do this?",
    "response": "The error here, is since calling the DataFrame constructor with scalar values (where it expects values to be a list/dict/... i.e. have multiple columns):\npd.DataFrame(d)\nValueError: If using all scalar values, you must must pass an index\n\nYou could take the items from the dictionary (i.e. the key-value pairs):\nIn [11]: pd.DataFrame(d.items())  # or list(d.items()) in python 3\nOut[11]:\n            0    1\n0  2012-07-01  391\n1  2012-07-02  392\n2  2012-07-03  392\n3  2012-07-04  392\n4  2012-07-05  392\n5  2012-07-06  392\n\nIn [12]: pd.DataFrame(d.items(), columns=['Date', 'DateValue'])\nOut[12]:\n         Date  DateValue\n0  2012-07-01        391\n1  2012-07-02        392\n2  2012-07-03        392\n3  2012-07-04        392\n4  2012-07-05        392\n5  2012-07-06        392\n\nBut I think it makes more sense to pass the Series constructor:\nIn [20]: s = pd.Series(d, name='DateValue')\n\nIn [21]: s\nOut[21]:\n2012-07-01    391\n2012-07-02    392\n2012-07-03    392\n2012-07-04    392\n2012-07-05    392\n2012-07-06",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 18837262,
      "answer_id": 18837389,
      "question_score": 612,
      "answer_score": 897,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "dictionary"
      ]
    }
  },
  {
    "prompt": "Pandas read_csv: low_memory and dtype options\n\ndf = pd.read_csv('somefile.csv')\n\n...gives an error:\n\n.../site-packages/pandas/io/parsers.py:1130:\nDtypeWarning: Columns (4,5,7,16) have mixed types.  Specify dtype\noption on import or set low_memory=False.\n\nWhy is the dtype option related to low_memory, and why might low_memory=False help?",
    "response": "The deprecated low_memory option\nThe low_memory option is not properly deprecated, but it should be, since it does not actually do anything differently[source]\nThe reason you get this low_memory warning is because guessing dtypes for each column is very memory demanding. Pandas tries to determine what dtype to set by analyzing the data in each column.\nDtype Guessing (very bad)\nPandas can only determine what dtype a column should have once the whole file is read. This means nothing can really be parsed before the whole file is read unless you risk having to change the dtype of that column when you read the last value.\nConsider the example of one file which has a column called user_id.\nIt contains 10 million rows where the user_id is always numbers.\nSince pandas cannot know it is only numbers, it will probably keep it as the original strings until it has read the whole file.\nSpecifying dtypes (should always be done)\nadding\ndtype={'user_id': int}\n\nto the pd.read_csv() call will make panda",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 24251219,
      "answer_id": 27232309,
      "question_score": 599,
      "answer_score": 751,
      "is_accepted": true,
      "tags": [
        "python",
        "parsing",
        "numpy",
        "pandas",
        "dataframe"
      ]
    }
  },
  {
    "prompt": "Improve subplot size/spacing with many subplots\n\nI need to generate a whole bunch of vertically-stacked plots in matplotlib. The result will be saved using savefig and viewed on a webpage, so I don't care how tall the final image is, as long as the subplots are spaced so they don't overlap.\nNo matter how big I allow the figure to be, the subplots always seem to overlap.\nMy code currently looks like\nimport matplotlib.pyplot as plt\nimport my_other_module\n\ntitles, x_lists, y_lists = my_other_module.get_data()\n\nfig = plt.figure(figsize=(10,60))\nfo",
    "response": "Please review matplotlib: Tight Layout guide and try using matplotlib.pyplot.tight_layout, or matplotlib.figure.Figure.tight_layout\nAs a quick example:\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(nrows=4, ncols=4, figsize=(8, 8))\nfig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n\nplt.show()\n\n\nWithout Tight Layout\n\n\nWith Tight Layout",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 6541123,
      "answer_id": 9827848,
      "question_score": 596,
      "answer_score": 787,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "matplotlib",
        "seaborn",
        "subplot"
      ]
    }
  },
  {
    "prompt": "Filter dataframe rows if value in column is in a set list of values\n\nI have a Python pandas DataFrame rpt:\n\nrpt\n<class 'pandas.core.frame.DataFrame'>\nMultiIndex: 47518 entries, ('000002', '20120331') to ('603366', '20091231')\nData columns:\nSTK_ID                    47518  non-null values\nSTK_Name                  47518  non-null values\nRPT_Date                  47518  non-null values\nsales                     47518  non-null values\n\n\nI can filter the rows whose stock id is '600809' like this: rpt[rpt['STK_ID'] == '600809']\n\n<class 'pandas.core.frame.DataFrame'>\nM",
    "response": "Use the isin method: \n\nrpt[rpt['STK_ID'].isin(stk_list)]",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 12065885,
      "answer_id": 12065904,
      "question_score": 596,
      "answer_score": 882,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe"
      ]
    }
  },
  {
    "prompt": "How to group dataframe rows into list in pandas groupby\n\nGiven a dataframe, I want to groupby the first column and get second column as lists in rows, so that a dataframe like:\na b\nA 1\nA 2\nB 5\nB 5\nB 4\nC 6\n\nbecomes\nA [1,2]\nB [5,5,4]\nC [6]\n\nHow do I do this?",
    "response": "You can do this using groupby to group on the column of interest and then apply list to every group:\n\nIn [1]: df = pd.DataFrame( {'a':['A','A','B','B','B','C'], 'b':[1,2,5,5,4,6]})\n        df\n\nOut[1]: \n   a  b\n0  A  1\n1  A  2\n2  B  5\n3  B  5\n4  B  4\n5  C  6\n\nIn [2]: df.groupby('a')['b'].apply(list)\nOut[2]: \na\nA       [1, 2]\nB    [5, 5, 4]\nC          [6]\nName: b, dtype: object\n\nIn [3]: df1 = df.groupby('a')['b'].apply(list).reset_index(name='new')\n        df1\nOut[3]: \n   a        new\n0  A     [1, 2]\n1  B  [5, 5, 4]\n2  C        [6]",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 22219004,
      "answer_id": 22221675,
      "question_score": 594,
      "answer_score": 793,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "list",
        "group-by",
        "aggregate"
      ]
    }
  },
  {
    "prompt": "Get list from pandas dataframe column or row?\n\nI have a dataframe df imported from an Excel document like this:\ncluster load_date   budget  actual  fixed_price\nA   1/1/2014    1000    4000    Y\nA   2/1/2014    12000   10000   Y\nA   3/1/2014    36000   2000    Y\nB   4/1/2014    15000   10000   N\nB   4/1/2014    12000   11500   N\nB   4/1/2014    90000   11000   N\nC   7/1/2014    22000   18000   N\nC   8/1/2014    30000   28960   N\nC   9/1/2014    53000   51200   N\n\nI want to be able to return the contents of column 1 df['cluster'] as a list, so",
    "response": "Pandas DataFrame columns are Pandas Series when you pull them out, which you can then call x.tolist() on to turn them into a Python list. Alternatively you cast it with list(x).\nimport pandas as pd\n\ndata_dict = {'one': pd.Series([1, 2, 3], index=['a', 'b', 'c']),\n             'two': pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])}\n\ndf = pd.DataFrame(data_dict)\n\nprint(f\"DataFrame:\\n{df}\\n\")\nprint(f\"column types:\\n{df.dtypes}\")\n\ncol_one_list = df['one'].tolist()\n\ncol_one_arr = df['one'].to_numpy()\n\nprint(f\"\\ncol_one_list:\\n{col_one_list}\\ntype:{type(col_one_list)}\")\nprint(f\"\\ncol_one_arr:\\n{col_one_arr}\\ntype:{type(col_one_arr)}\")\n\nOutput:\nDataFrame:\n   one  two\na  1.0    1\nb  2.0    2\nc  3.0    3\nd  NaN    4\n\ncolumn types:\none    float64\ntwo      int64\ndtype: object\n\ncol_one_list:\n[1.0, 2.0, 3.0, nan]\ntype:<class 'list'>\n\ncol_one_arr:\n[ 1.  2.  3. nan]\ntype:<class 'numpy.ndarray'>",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 22341271,
      "answer_id": 22341390,
      "question_score": 594,
      "answer_score": 942,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "list",
        "type-conversion"
      ]
    }
  },
  {
    "prompt": "How to reset index in a pandas dataframe?\n\nI have a dataframe from which I remove some rows. As a result, I get a dataframe in which index is something like [1,5,6,10,11] and I would like to reset it to [0,1,2,3,4]. How can I do it?\n\nThe following seems to work:\ndf = df.reset_index()\ndel df['index']\n\nThe following does not work:\ndf = df.reindex()",
    "response": "DataFrame.reset_index is what you're looking for. If you don't want it saved as a column, then do:\ndf = df.reset_index(drop=True)\n\nIf you don't want to reassign:\ndf.reset_index(drop=True, inplace=True)",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 20490274,
      "answer_id": 20491748,
      "question_score": 589,
      "answer_score": 1167,
      "is_accepted": true,
      "tags": [
        "python",
        "indexing",
        "pandas",
        "dataframe"
      ]
    }
  },
  {
    "prompt": "Selecting a row of pandas series/dataframe by integer index\n\nI am curious as to why df[2] is not supported, while df.ix[2] and df[2:3] both work. \n\nIn [26]: df.ix[2]\nOut[26]: \nA    1.027680\nB    1.514210\nC   -1.466963\nD   -0.162339\nName: 2000-01-03 00:00:00\n\nIn [27]: df[2:3]\nOut[27]: \n                  A        B         C         D\n2000-01-03  1.02768  1.51421 -1.466963 -0.162339\n\n\nI would expect df[2] to work the same way as df[2:3] to be consistent with Python indexing convention. Is there a design reason for not supporting indexing row by single integ",
    "response": "echoing @HYRY, see the new docs in 0.11\n\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html\n\nHere we have new operators, .iloc to explicity support only integer indexing, and .loc to explicity support only label indexing\n\ne.g. imagine this scenario\n\nIn [1]: df = pd.DataFrame(np.random.rand(5,2),index=range(0,10,2),columns=list('AB'))\n\nIn [2]: df\nOut[2]: \n          A         B\n0  1.068932 -0.794307\n2 -0.470056  1.192211\n4 -0.284561  0.756029\n6  1.037563 -0.267820\n8 -0.538478 -0.800654\n\nIn [5]: df.iloc[[2]]\nOut[5]: \n          A         B\n4 -0.284561  0.756029\n\nIn [6]: df.loc[[2]]\nOut[6]: \n          A         B\n2 -0.470056  1.192211\n\n\n[] slices the rows (by label location) only",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 16096627,
      "answer_id": 16104482,
      "question_score": 589,
      "answer_score": 814,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "indexing",
        "filter"
      ]
    }
  },
  {
    "prompt": "How can I use the apply() function for a single column?\n\nI have a pandas dataframe with multiple columns. I want to change the values of the only the first column without affecting the other columns. How can I do that using apply() in pandas?",
    "response": "Given a sample dataframe df as:\n   a  b\n0  1  2\n1  2  3\n2  3  4\n3  4  5\n\nwhat you want is:\ndf['a'] = df['a'].apply(lambda x: x + 1)\n\nthat returns:\n   a  b\n0  2  2\n1  3  3\n2  4  4\n3  5  5",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 34962104,
      "answer_id": 34962199,
      "question_score": 587,
      "answer_score": 709,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "numpy",
        "apply"
      ]
    }
  },
  {
    "prompt": "How to select all columns except one in pandas?\n\nI have a dataframe that look like this:\n          a         b         c         d\n0  0.418762  0.042369  0.869203  0.972314\n1  0.991058  0.510228  0.594784  0.534366\n2  0.407472  0.259811  0.396664  0.894202\n3  0.726168  0.139531  0.324932  0.906575\n\nHow I can get all columns except b?",
    "response": "When the columns are not a MultiIndex, df.columns is just an array of column names so you can do:\n\ndf.loc[:, df.columns != 'b']\n\n          a         c         d\n0  0.561196  0.013768  0.772827\n1  0.882641  0.615396  0.075381\n2  0.368824  0.651378  0.397203\n3  0.788730  0.568099  0.869127",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 29763620,
      "answer_id": 29763653,
      "question_score": 586,
      "answer_score": 804,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "select",
        "filter"
      ]
    }
  },
  {
    "prompt": "How to flatten a hierarchical index in columns\n\nI have a data frame with a hierarchical index in axis 1 (columns) (from a groupby.agg operation):\n\n     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf       \n                                     sum   sum   sum    sum   amax   amin\n0  702730  26451  1993      1    1     1     0    12     13  30.92  24.98\n1  702730  26451  1993      1    2     0     0    13     13  32.00  24.98\n2  702730  26451  1993      1    3     1    10     2     13  23.00   6.98\n3  702730  26451  1993      1  ",
    "response": "I think the easiest way to do this would be to set the columns to the top level:\n\ndf.columns = df.columns.get_level_values(0)\n\n\nNote: if the to level has a name you can also access it by this, rather than 0.\n\n.\n\nIf you want to combine/join your MultiIndex into one Index (assuming you have just string entries in your columns) you could:\n\ndf.columns = [' '.join(col).strip() for col in df.columns.values]\n\n\nNote: we must strip the whitespace for when there is no second index.\n\nIn [11]: [' '.join(col).strip() for col in df.columns.values]\nOut[11]: \n['USAF',\n 'WBAN',\n 'day',\n 'month',\n 's_CD sum',\n 's_CL sum',\n 's_CNT sum',\n 's_PC sum',\n 'tempf amax',\n 'tempf amin',\n 'year']",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 14507794,
      "answer_id": 14508355,
      "question_score": 577,
      "answer_score": 768,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "multi-index"
      ]
    }
  },
  {
    "prompt": "How do I create test and train samples from one dataframe with pandas?\n\nI have a fairly large dataset in the form of a dataframe and I was wondering how I would be able to split the dataframe into two random samples (80% and 20%) for training and testing.\n\nThanks!",
    "response": "I would just use numpy's randn:\n\nIn [11]: df = pd.DataFrame(np.random.randn(100, 2))\n\nIn [12]: msk = np.random.rand(len(df)) < 0.8\n\nIn [13]: train = df[msk]\n\nIn [14]: test = df[~msk]\n\n\nAnd just to see this has worked:\n\nIn [15]: len(test)\nOut[15]: 21\n\nIn [16]: len(train)\nOut[16]: 79",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 24147278,
      "answer_id": 24147363,
      "question_score": 563,
      "answer_score": 486,
      "is_accepted": true,
      "tags": [
        "python",
        "python-2.7",
        "pandas",
        "dataframe"
      ]
    }
  },
  {
    "prompt": "Python Pandas: Get index of rows where column matches certain value\n\nGiven a DataFrame with a column \"BoolCol\", we want to find the indexes of the DataFrame in which the values for \"BoolCol\" == True\nI currently have the iterating way to do it, which works perfectly:\nfor i in range(100,3000):\n    if df.iloc[i]['BoolCol']== True:\n         print i,df.iloc[i]['BoolCol']\n\nBut this is not the correct pandas way to do it. After some research, I am currently using this code:\ndf[df['BoolCol'] == True].index.tolist()\n\nThis one gives me a list of indexes, but they don't mat",
    "response": "df.iloc[i] returns the ith row of df. i does not refer to the index label, i is a 0-based index.\n\nIn contrast, the attribute index returns actual index labels, not numeric row-indices:\n\ndf.index[df['BoolCol'] == True].tolist()\n\n\nor equivalently,\n\ndf.index[df['BoolCol']].tolist()\n\n\nYou can see the difference quite clearly by playing with a DataFrame with\na non-default index that does not equal to the row's numerical position:\n\ndf = pd.DataFrame({'BoolCol': [True, False, False, True, True]},\n       index=[10,20,30,40,50])\n\nIn [53]: df\nOut[53]: \n   BoolCol\n10    True\n20   False\n30   False\n40    True\n50    True\n\n[5 rows x 1 columns]\n\nIn [54]: df.index[df['BoolCol']].tolist()\nOut[54]: [10, 40, 50]\n\n\n\n\nIf you want to use the index, \n\nIn [56]: idx = df.index[df['BoolCol']]\n\nIn [57]: idx\nOut[57]: Int64Index([10, 40, 50], dtype='int64')\n\n\nthen you can select the rows using loc instead of iloc:\n\nIn [58]: df.loc[idx]\nOut[58]: \n   BoolCol\n10    True\n40    True\n50    True\n\n[3 rows x 1 columns]\n\n\n\n\n",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 21800169,
      "answer_id": 21800319,
      "question_score": 561,
      "answer_score": 814,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "indexing"
      ]
    }
  },
  {
    "prompt": "Pandas: Get first row value of a given column\n\nThis seems like a ridiculously easy question... but I'm not seeing the easy answer I was expecting.\nSo, how do I get the value at an nth row of a given column in Pandas? (I am particularly interested in the first row, but would be interested in a more general practice as well).\nFor example, let's say I want to pull the 1.2 value in Btime as a variable.\nWhats the right way to do this?\n>>> df_test\n    ATime   X   Y   Z   Btime  C   D   E\n0    1.2  2  15   2    1.2  12  25  12\n1    1.4  3  12   1  ",
    "response": "To select the ith row, use iloc:\n\nIn [31]: df_test.iloc[0]\nOut[31]: \nATime     1.2\nX         2.0\nY        15.0\nZ         2.0\nBtime     1.2\nC        12.0\nD        25.0\nE        12.0\nName: 0, dtype: float64\n\n\nTo select the ith value in the Btime column you could use:\n\nIn [30]: df_test['Btime'].iloc[0]\nOut[30]: 1.2\n\n\n\n\nThere is a difference between df_test['Btime'].iloc[0] (recommended) and df_test.iloc[0]['Btime']:\n\nDataFrames store data in column-based blocks (where each block has a single\ndtype). If you select by column first, a view can be returned (which is\nquicker than returning a copy) and the original dtype is preserved. In contrast,\nif you select by row first, and if the DataFrame has columns of different\ndtypes, then Pandas copies the data into a new Series of object dtype. So\nselecting columns is a bit faster than selecting rows. Thus, although\ndf_test.iloc[0]['Btime'] works, df_test['Btime'].iloc[0] is a little bit\nmore efficient.\n\nThere is a big difference between the two whe",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 25254016,
      "answer_id": 25254087,
      "question_score": 554,
      "answer_score": 852,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "indexing"
      ]
    }
  },
  {
    "prompt": "Selecting/excluding sets of columns in pandas\n\nI would like to create views or dataframes from an existing dataframe based on column selections.\n\nFor example, I would like to create a dataframe df2 from a dataframe df1 that holds all columns from it except two of them. I tried doing the following, but it didn't work:\n\nimport numpy as np\nimport pandas as pd\n\n# Create a dataframe with columns A,B,C and D\ndf = pd.DataFrame(np.random.randn(100, 4), columns=list('ABCD'))\n\n# Try to create a second dataframe df2 from df with all columns except 'B' ",
    "response": "You can either Drop the columns you do not need OR Select the ones you need\n\n# Using DataFrame.drop\ndf.drop(df.columns[[1, 2]], axis=1, inplace=True)\n\n# drop by Name\ndf1 = df1.drop(['B', 'C'], axis=1)\n\n# Select the ones you want\ndf1 = df[['a','d']]",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 14940743,
      "answer_id": 29319200,
      "question_score": 536,
      "answer_score": 745,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe"
      ]
    }
  },
  {
    "prompt": "How to add an empty column to a dataframe?\n\nWhat's the easiest way to add an empty column to a pandas DataFrame object?  The best I've stumbled upon is something like\ndf['foo'] = df.apply(lambda _: '', axis=1)\n\nIs there a less perverse method?",
    "response": "If I understand correctly, assignment should fill:\n\n>>> import numpy as np\n>>> import pandas as pd\n>>> df = pd.DataFrame({\"A\": [1,2,3], \"B\": [2,3,4]})\n>>> df\n   A  B\n0  1  2\n1  2  3\n2  3  4\n>>> df[\"C\"] = \"\"\n>>> df[\"D\"] = np.nan\n>>> df\n   A  B C   D\n0  1  2   NaN\n1  2  3   NaN\n2  3  4   NaN",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 16327055,
      "answer_id": 16327135,
      "question_score": 530,
      "answer_score": 775,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "variable-assignment"
      ]
    }
  },
  {
    "prompt": "How to sort a pandas dataFrame by two or more columns?\n\nSuppose I have a dataframe with columns a, b and c. I want to sort the dataframe by column b in ascending order, and by column c in descending order. How do I do this?",
    "response": "As of the 0.17.0 release, the sort method was deprecated in favor of sort_values.  sort was completely removed in the 0.20.0 release. The arguments (and results) remain the same:\n\ndf.sort_values(['a', 'b'], ascending=[True, False])\n\n\n\n\nYou can use the ascending argument of sort:\n\ndf.sort(['a', 'b'], ascending=[True, False])\n\n\nFor example:\n\nIn [11]: df1 = pd.DataFrame(np.random.randint(1, 5, (10,2)), columns=['a','b'])\n\nIn [12]: df1.sort(['a', 'b'], ascending=[True, False])\nOut[12]:\n   a  b\n2  1  4\n7  1  3\n1  1  2\n3  1  2\n4  3  2\n6  4  4\n0  4  3\n9  4  3\n5  4  1\n8  4  1\n\n\n\n\nAs commented by @renadeen\n\n\n  Sort isn't in place by default! So you should assign result of the sort method to a variable or add inplace=True to method call.\n\n\nthat is, if you want to reuse df1 as a sorted DataFrame:\n\ndf1 = df1.sort(['a', 'b'], ascending=[True, False])\n\n\nor\n\ndf1.sort(['a', 'b'], ascending=[True, False], inplace=True)",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 17141558,
      "answer_id": 17141755,
      "question_score": 529,
      "answer_score": 922,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "python-2.7",
        "sorting",
        "data-analysis"
      ]
    }
  },
  {
    "prompt": "Sorting columns in pandas dataframe based on column name\n\nI have a dataframe with over 200 columns. The issue is as they were generated the order is\n['Q1.3','Q6.1','Q1.2','Q1.1',......]\n\nI need to sort the columns as follows:\n['Q1.1','Q1.2','Q1.3',.....'Q6.1',......]\n\nIs there some way for me to do this within Python?",
    "response": "df = df.reindex(sorted(df.columns), axis=1)\n\n\nThis assumes that sorting the column names will give the order you want.  If your column names won't sort lexicographically (e.g., if you want column Q10.3 to appear after Q9.1), you'll need to sort differently, but that has nothing to do with pandas.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 11067027,
      "answer_id": 11067072,
      "question_score": 525,
      "answer_score": 668,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe"
      ]
    }
  },
  {
    "prompt": "Count the frequency that a value occurs in a dataframe column\n\nI have a dataset\ncategory\ncat a\ncat b\ncat a\n\nI'd like to return something like the following which shows the unique values and their frequencies\ncategory   freq \ncat a       2\ncat b       1",
    "response": "Use value_counts() as @DSM commented.\nIn [37]:\ndf = pd.DataFrame({'a':list('abssbab')})\ndf['a'].value_counts()\n\nOut[37]:\n\nb    3\na    2\ns    2\ndtype: int64\n\nAlso groupby and count. Many ways to skin a cat here.\nIn [38]:\ndf.groupby('a').count()\n\nOut[38]:\n\n   a\na   \na  2\nb  3\ns  2\n\n[3 rows x 1 columns]\n\nSee the online docs.\nIf you wanted to add frequency back to the original dataframe use transform to return an aligned index:\nIn [41]:\ndf['freq'] = df.groupby('a')['a'].transform('count')\ndf\n\nOut[41]:\n\n   a freq\n0  a    2\n1  b    3\n2  s    2\n3  s    2\n4  b    3\n5  a    2\n6  b    3\n\n[7 rows x 2 columns]",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 22391433,
      "answer_id": 22391554,
      "question_score": 522,
      "answer_score": 698,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "group-by",
        "count",
        "frequency"
      ]
    }
  },
  {
    "prompt": "What does `ValueError: cannot reindex from a duplicate axis` mean?\n\nI am getting a ValueError: cannot reindex from a duplicate axis when I am trying to set an index to a certain value. I tried to reproduce this with a simple example, but I could not do it.\n\nHere is my session inside of ipdb trace. I have a DataFrame with string index, and integer columns, float values. However when I try to create sum index for sum of all columns I am getting ValueError: cannot reindex from a duplicate axis error. I created a small DataFrame with the same characteristics, but wa",
    "response": "This error usually rises when you join / assign to a column when the index has duplicate values. Since you are assigning to a row, I suspect that there is a duplicate value in affinity_matrix.columns, perhaps not shown in your question.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 27236275,
      "answer_id": 27242735,
      "question_score": 520,
      "answer_score": 334,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas"
      ]
    }
  },
  {
    "prompt": "How can I display full (non-truncated) dataframe information in HTML when converting from Pandas dataframe to HTML?\n\nI converted a Pandas dataframe to an HTML output using the DataFrame.to_html function. When I save this to a separate HTML file, the file shows truncated output.\nFor example, in my TEXT column,\ndf.head(1) will show\nThe film was an excellent effort...\ninstead of\nThe film was an excellent effort in deconstructing the complex social sentiments that prevailed during this period.\nThis rendition is fine in the case of a screen-friendly format of a massive Pandas dataframe, but I need an HTML file that",
    "response": "Set the display.max_colwidth option to None (or -1 before version 1.0):\npd.set_option('display.max_colwidth', None)\n\nset_option documentation\nFor example, in IPython, we see that the information is truncated to 50 characters. Anything in excess is ellipsized:\n\nIf you set the display.max_colwidth option, the information will be displayed fully:",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 25351968,
      "answer_id": 25352191,
      "question_score": 514,
      "answer_score": 839,
      "is_accepted": true,
      "tags": [
        "python",
        "html",
        "pandas"
      ]
    }
  },
  {
    "prompt": "Get the row(s) which have the max value in groups using groupby\n\nHow do I find all rows in a pandas DataFrame which have the max value for count column, after grouping by ['Sp','Mt'] columns?\nExample 1: the following DataFrame:\n   Sp   Mt Value   count\n0  MM1  S1   a     **3**\n1  MM1  S1   n       2\n2  MM1  S3   cb    **5**\n3  MM2  S3   mk    **8**\n4  MM2  S4   bg    **10**\n5  MM2  S4   dgd     1\n6  MM4  S2   rd      2\n7  MM4  S2   cb      2\n8  MM4  S2   uyi   **7**\n\nExpected output is to get the result rows whose count is max in each group, like this:\n   Sp ",
    "response": "Firstly, we can get the max count for each group like this:\nIn [1]: df\nOut[1]:\n    Sp  Mt Value  count\n0  MM1  S1     a      3\n1  MM1  S1     n      2\n2  MM1  S3    cb      5\n3  MM2  S3    mk      8\n4  MM2  S4    bg     10\n5  MM2  S4   dgd      1\n6  MM4  S2    rd      2\n7  MM4  S2    cb      2\n8  MM4  S2   uyi      7\n\nIn [2]: df.groupby(['Sp', 'Mt'])['count'].max()\nOut[2]:\nSp   Mt\nMM1  S1     3\n     S3     5\nMM2  S3     8\n     S4    10\nMM4  S2     7\nName: count, dtype: int64\n\nTo get the indices of the original DF you can do:\nIn [3]: idx = df.groupby(['Sp', 'Mt'])['count'].transform(max) == df['count']\n\nIn [4]: df[idx]\nOut[4]:\n    Sp  Mt Value  count\n0  MM1  S1     a      3\n2  MM1  S3    cb      5\n3  MM2  S3    mk      8\n4  MM2  S4    bg     10\n8  MM4  S2   uyi      7\n\nNote that if you have multiple max values per group, all will be returned.\n\nUpdate\nOn a Hail Mary chance that this is what the OP is requesting:\nIn [5]: df['count_max'] = df.groupby(['Sp', 'Mt'])['count'].transform(max)\n\n",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 15705630,
      "answer_id": 15705958,
      "question_score": 511,
      "answer_score": 608,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "group-by",
        "max"
      ]
    }
  },
  {
    "prompt": "How do I create a new column where the values are selected based on an existing column?\n\nHow do I add a color column to the following dataframe so that color='green' if Set == 'Z', and color='red' otherwise?\n   Type  Set\n1     A    Z\n2     B    Z           \n3     B    X\n4     C    Y",
    "response": "If you only have two choices to select from then use np.where:\ndf['color'] = np.where(df['Set']=='Z', 'green', 'red')\n\nFor example,\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Type':list('ABBC'), 'Set':list('ZZXY')})\ndf['color'] = np.where(df['Set']=='Z', 'green', 'red')\nprint(df)\n\nyields\n  Set Type  color\n0   Z    A  green\n1   Z    B  green\n2   X    B    red\n3   Y    C    red\n\n\nIf you have more than two conditions then use np.select. For example, if you want color to be\n\nyellow when (df['Set'] == 'Z') & (df['Type'] == 'A')\notherwise blue when (df['Set'] == 'Z') & (df['Type'] == 'B')\notherwise purple when (df['Type'] == 'B')\notherwise black,\n\nthen use\ndf = pd.DataFrame({'Type':list('ABBC'), 'Set':list('ZZXY')})\nconditions = [\n    (df['Set'] == 'Z') & (df['Type'] == 'A'),\n    (df['Set'] == 'Z') & (df['Type'] == 'B'),\n    (df['Type'] == 'B')]\nchoices = ['yellow', 'blue', 'purple']\ndf['color'] = np.select(conditions, choices, default='black')\nprint(df)\n\nwhich yields\n  Set ",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 19913659,
      "answer_id": 19913845,
      "question_score": 510,
      "answer_score": 1060,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "numpy"
      ]
    }
  },
  {
    "prompt": "Create Pandas DataFrame from a string\n\nIn order to test some functionality I would like to create a DataFrame from a string. Let's say my test data looks like:\n\nTESTDATA=\"\"\"col1;col2;col3\n1;4.4;99\n2;4.5;200\n3;4.7;65\n4;3.2;140\n\"\"\"\n\n\nWhat is the simplest way to read that data into a Pandas DataFrame?",
    "response": "A simple way to do this is to use StringIO.StringIO (python2) or io.StringIO (python3) and pass that to the pandas.read_csv function. E.g:\n\nimport sys\nif sys.version_info[0] < 3: \n    from StringIO import StringIO\nelse:\n    from io import StringIO\n\nimport pandas as pd\n\nTESTDATA = StringIO(\"\"\"col1;col2;col3\n    1;4.4;99\n    2;4.5;200\n    3;4.7;65\n    4;3.2;140\n    \"\"\")\n\ndf = pd.read_csv(TESTDATA, sep=\";\")",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 22604564,
      "answer_id": 22605281,
      "question_score": 507,
      "answer_score": 851,
      "is_accepted": true,
      "tags": [
        "python",
        "string",
        "pandas",
        "csv",
        "csv-import"
      ]
    }
  },
  {
    "prompt": "How to draw vertical lines on a given plot\n\nGiven a plot of a signal in time representation, how can I draw lines marking the corresponding time index?\nSpecifically, given a signal plot with a time index ranging from 0 to 2.6 (seconds), I want to draw vertical red lines indicating the corresponding time index for the list [0.22058956, 0.33088437, 2.20589566]. How can I do it?",
    "response": "The standard way to add vertical lines that will cover your entire plot window without you having to specify their actual height is plt.axvline\n\nimport matplotlib.pyplot as plt\n\nplt.axvline(x=0.22058956)\nplt.axvline(x=0.33088437)\nplt.axvline(x=2.20589566)\n\n\nOR\n\nxcoords = [0.22058956, 0.33088437, 2.20589566]\nfor xc in xcoords:\n    plt.axvline(x=xc)\n\n\nYou can use many of the keywords available for other plot commands (e.g. color, linestyle, linewidth ...).  You can pass in keyword arguments ymin and ymax if you like in axes corrdinates (e.g. ymin=0.25, ymax=0.75 will cover the middle half of the plot).  There are corresponding functions for horizontal lines (axhline) and rectangles (axvspan).",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 24988448,
      "answer_id": 24988486,
      "question_score": 504,
      "answer_score": 749,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "matplotlib",
        "seaborn"
      ]
    }
  },
  {
    "prompt": "Convert Pandas Column to DateTime\n\nI have one field in a pandas DataFrame that was imported as string format.\nIt should be a datetime variable. How do I convert it to a datetime column, and then filter based on date?\nExample:\nraw_data = pd.DataFrame({'Mycol': ['05SEP2014:00:00:00.000']})",
    "response": "Use the to_datetime function, specifying a format to match your data.\ndf['Mycol'] = pd.to_datetime(df['Mycol'], format='%d%b%Y:%H:%M:%S.%f')",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 26763344,
      "answer_id": 26763793,
      "question_score": 500,
      "answer_score": 841,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "datetime",
        "type-conversion"
      ]
    }
  },
  {
    "prompt": "Normalize columns of a dataframe\n\nI have a dataframe in pandas where each column has different value range. For example:\n\ndf:\n\nA     B   C\n1000  10  0.5\n765   5   0.35\n800   7   0.09\n\n\nAny idea how I can normalize the columns of this dataframe where each value is between 0 and 1?\n\nMy desired output is:\n\nA     B    C\n1     1    1\n0.765 0.5  0.7\n0.8   0.7  0.18(which is 0.09/0.5)",
    "response": "You can use the package sklearn and its associated preprocessing utilities to normalize the data.\n\nimport pandas as pd\nfrom sklearn import preprocessing\n\nx = df.values #returns a numpy array\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\ndf = pd.DataFrame(x_scaled)\n\n\nFor more information look at the scikit-learn documentation on preprocessing data: scaling features to a range.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 26414913,
      "answer_id": 26415620,
      "question_score": 489,
      "answer_score": 427,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "normalize"
      ]
    }
  },
  {
    "prompt": "Remove pandas rows with duplicate indices\n\nHow to remove rows with duplicate index values?\nIn the weather DataFrame below, sometimes a scientist goes back and corrects observations -- not by editing the erroneous rows, but by appending a duplicate row to the end of a file.\nI'm reading some automated weather data from the web (observations occur every 5 minutes, and compiled into monthly files for each weather station.) After parsing a file, the DataFrame looks like:\n                      Sta  Precip1hr  Precip5min  Temp  DewPnt  WindSpd ",
    "response": "I would suggest using the duplicated method on the Pandas Index itself:\ndf3 = df3.loc[~df3.index.duplicated(keep='first'), :]\n\nWhile all the other methods work, .drop_duplicates is by far the least performant for the provided example. Furthermore, while the groupby method is only slightly less performant, I find the duplicated method to be more readable.\nUsing the sample data provided:\n>>> %timeit df3.reset_index().drop_duplicates(subset='index', keep='first').set_index('index')\n1000 loops, best of 3: 1.54 ms per loop\n\n>>> %timeit df3.groupby(df3.index).first()\n1000 loops, best of 3: 580 µs per loop\n\n>>> %timeit df3.loc[~df3.index.duplicated(keep='first'), :]\n1000 loops, best of 3: 307 µs per loop\n\nNote that you can keep the last element by changing the keep argument to 'last'.\nIt should also be noted that this method works with MultiIndex as well (using df1 as specified in Paul's example):\n>>> %timeit df1.groupby(level=df1.index.names).last()\n1000 loops, best of 3: 771 µs per loop\n\n>>",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 13035764,
      "answer_id": 34297689,
      "question_score": 488,
      "answer_score": 883,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "duplicates"
      ]
    }
  },
  {
    "prompt": "How to reversibly store and load a Pandas dataframe to/from disk\n\nRight now I'm importing a fairly large CSV as a dataframe every time I run the script. Is there a good solution for keeping that dataframe constantly available in between runs so I don't have to spend all that time waiting for the script to run?",
    "response": "The easiest way is to pickle it using to_pickle:\ndf.to_pickle(file_name)  # where to save it, usually as a .pkl\n\nThen you can load it back using:\ndf = pd.read_pickle(file_name)\n\nNote: before 0.11.1 save and load were the only way to do this (they are now deprecated in favor of to_pickle and read_pickle respectively).\n\nAnother popular choice is to use HDF5 (pytables) which offers very fast access times for large datasets:\nimport pandas as pd\nstore = pd.HDFStore('store.h5')\n\nstore['df'] = df  # save it\nstore['df']  # load it\n\nMore advanced strategies are discussed in the cookbook.\n\nSince 0.13 there's also msgpack which may be be better for interoperability, as a faster alternative to JSON, or if you have python object/text-heavy data (see this question).",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 17098654,
      "answer_id": 17098736,
      "question_score": 488,
      "answer_score": 707,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe"
      ]
    }
  },
  {
    "prompt": "Convert DataFrame column type from string to datetime\n\nHow can I convert a DataFrame column of strings (in dd/mm/yyyy format) to datetime dtype?",
    "response": "The easiest way is to use to_datetime:\ndf['col'] = pd.to_datetime(df['col'])\n\nIt also offers a dayfirst argument for European times (but beware this isn't strict).\nHere it is in action:\nIn [11]: pd.to_datetime(pd.Series(['05/23/2005']))\nOut[11]:\n0   2005-05-23 00:00:00\ndtype: datetime64[ns]\n\nYou can pass a specific format:\nIn [12]: pd.to_datetime(pd.Series(['05/23/2005']), format=\"%m/%d/%Y\")\nOut[12]:\n0   2005-05-23\ndtype: datetime64[ns]",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 17134716,
      "answer_id": 17134750,
      "question_score": 488,
      "answer_score": 739,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "datetime",
        "type-conversion"
      ]
    }
  },
  {
    "prompt": "Converting between datetime, Timestamp and datetime64\n\nHow do I convert a numpy.datetime64 object to a datetime.datetime (or Timestamp)?\n\nIn the following code, I create a datetime, timestamp and datetime64 objects.\n\nimport datetime\nimport numpy as np\nimport pandas as pd\ndt = datetime.datetime(2012, 5, 1)\n# A strange way to extract a Timestamp object, there's surely a better way?\nts = pd.DatetimeIndex([dt])[0]\ndt64 = np.datetime64(dt)\n\nIn [7]: dt\nOut[7]: datetime.datetime(2012, 5, 1, 0, 0)\n\nIn [8]: ts\nOut[8]: <Timestamp: 2012-05-01 00:00:00>\n\nIn [9]",
    "response": "To convert numpy.datetime64 to datetime object that represents time in UTC on numpy-1.8:\n>>> from datetime import datetime\n>>> import numpy as np\n>>> dt = datetime.utcnow()\n>>> dt\ndatetime.datetime(2012, 12, 4, 19, 51, 25, 362455)\n>>> dt64 = np.datetime64(dt)\n>>> ts = (dt64 - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's')\n>>> ts\n1354650685.3624549\n>>> datetime.utcfromtimestamp(ts)\ndatetime.datetime(2012, 12, 4, 19, 51, 25, 362455)\n>>> np.__version__\n'1.8.0.dev-7b75899'\n\nThe above example assumes that a naive datetime object is interpreted by np.datetime64 as time in UTC.\n\nTo convert datetime to np.datetime64 and back (numpy-1.6):\n>>> np.datetime64(datetime.utcnow()).astype(datetime)\ndatetime.datetime(2012, 12, 4, 13, 34, 52, 827542)\n\nIt works both on a single np.datetime64 object and a numpy array of np.datetime64.\nThink of np.datetime64 the same way you would about np.int8, np.int16, etc and apply the same methods to convert between Python objects such as int, datetim",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 13703720,
      "answer_id": 13704307,
      "question_score": 483,
      "answer_score": 192,
      "is_accepted": true,
      "tags": [
        "python",
        "datetime",
        "numpy",
        "pandas"
      ]
    }
  },
  {
    "prompt": "How to add pandas data to an existing csv file?\n\nI want to know if it is possible to use the pandas to_csv() function to add a dataframe to an existing csv file. The csv file has the same structure as the loaded data.",
    "response": "You can specify a python write mode in the pandas to_csv function. For append it is 'a'.\nIn your case:\ndf.to_csv('my_csv.csv', mode='a', header=False)\n\nThe default mode is 'w'.\nIf the file initially might be missing, you can make sure the header is printed at the first write using this variation:\noutput_path='my_csv.csv'\ndf.to_csv(output_path, mode='a', header=not os.path.exists(output_path))",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 17530542,
      "answer_id": 17975690,
      "question_score": 479,
      "answer_score": 1000,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "csv",
        "dataframe"
      ]
    }
  },
  {
    "prompt": "Using Pandas to pd.read_excel() for multiple (but not all) worksheets of the same workbook without reloading the whole file\n\nI have a large spreadsheet file (.xlsx) that I'm processing using python pandas. It happens that I need data from two tabs (sheets) in that large file. One of the tabs has a ton of data and the other is just a few square cells.\nWhen I use pd.read_excel() on any worksheet, it looks to me like the whole file is loaded (not just the worksheet I'm interested in). So when I use the method twice (once for each sheet), I effectively have to suffer the whole workbook being read in twice (even though we'",
    "response": "Try pd.ExcelFile:\n\nxls = pd.ExcelFile('path_to_file.xls')\ndf1 = pd.read_excel(xls, 'Sheet1')\ndf2 = pd.read_excel(xls, 'Sheet2')\n\n\nAs noted by @HaPsantran, the entire Excel file is read in during the ExcelFile() call (there doesn't appear to be a way around this). This merely saves you from having to read the same file in each time you want to access a new sheet.\n\nNote that the sheet_name argument to pd.read_excel() can be the name of the sheet (as above), an integer specifying the sheet number (eg 0, 1, etc), a list of sheet names or indices, or None. If a list is provided, it returns a dictionary where the keys are the sheet names/indices and the values are the data frames. The default is to simply return the first sheet (ie, sheet_name=0).\n\nIf None is specified, all sheets are returned, as a {sheet_name:dataframe} dictionary.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 26521266,
      "answer_id": 26521726,
      "question_score": 454,
      "answer_score": 642,
      "is_accepted": true,
      "tags": [
        "python",
        "excel",
        "pandas",
        "dataframe",
        "xlsx"
      ]
    }
  },
  {
    "prompt": "Convert a Pandas DataFrame to a dictionary\n\nI have a DataFrame with four columns. I want to convert this DataFrame to a python dictionary. I want the elements of first column be keys and the elements of other columns in the same row be values.\nDataFrame:\n    ID   A   B   C\n0   p    1   3   2\n1   q    4   3   2\n2   r    4   0   9 \n\nOutput should be like this:\n{'p': [1,3,2], 'q': [4,3,2], 'r': [4,0,9]}",
    "response": "The to_dict() method sets the column names as dictionary keys so you'll need to reshape your DataFrame slightly. Setting the 'ID' column as the index and then transposing the DataFrame is one way to achieve this.\n\nto_dict() also accepts an 'orient' argument which you'll need in order to output a list of values for each column. Otherwise, a dictionary of the form {index: value} will be returned for each column.\n\nThese steps can be done with the following line:\n\n>>> df.set_index('ID').T.to_dict('list')\n{'p': [1, 3, 2], 'q': [4, 3, 2], 'r': [4, 0, 9]}\n\n\n\n\nIn case a different dictionary format is needed, here are examples of the possible orient arguments. Consider the following simple DataFrame:\n\n>>> df = pd.DataFrame({'a': ['red', 'yellow', 'blue'], 'b': [0.5, 0.25, 0.125]})\n>>> df\n        a      b\n0     red  0.500\n1  yellow  0.250\n2    blue  0.125\n\n\nThen the options are as follows.\n\ndict - the default: column names are keys, values are dictionaries of index:data pairs\n\n>>> df.to_dict('di",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 26716616,
      "answer_id": 26716774,
      "question_score": 453,
      "answer_score": 753,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dictionary",
        "dataframe"
      ]
    }
  },
  {
    "prompt": "How to invert the x or y axis\n\nI have a scatter plot graph with a bunch of random x, y coordinates. Currently the Y-Axis starts at 0 and goes up to the max value. I would like the Y-Axis to start at the max value and go up to 0.\n\npoints = [(10,5), (5,11), (24,13), (7,8)]    \nx_arr = []\ny_arr = []\nfor x,y in points:\n    x_arr.append(x)\n    y_arr.append(y)\nplt.scatter(x_arr,y_arr)",
    "response": "There is a new API that makes this even simpler.\n\nplt.gca().invert_xaxis()\n\n\nand/or\n\nplt.gca().invert_yaxis()",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 2051744,
      "answer_id": 8280500,
      "question_score": 449,
      "answer_score": 851,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "matplotlib",
        "seaborn"
      ]
    }
  },
  {
    "prompt": "Creating a Pandas DataFrame from a Numpy array: How do I specify the index column and column headers?\n\nI have a Numpy array consisting of a list of lists, representing a two-dimensional array with row labels and column names as shown below:\ndata = np.array([['','Col1','Col2'],['Row1',1,2],['Row2',3,4]])\n\nI'd like the resulting DataFrame to have Row1 and Row2 as index values, and Col1, Col2 as header values.\nI can specify the index as follows:\ndf = pd.DataFrame(data, index=data[:,0])\n\nHowever, I am unsure how to best assign column headers.",
    "response": "Specify data, index and columns to the DataFrame constructor, as follows:\n>>> pd.DataFrame(data=data[1:,1:],    # values\n...              index=data[1:,0],    # 1st column as index\n...              columns=data[0,1:])  # 1st row as the column names\n\nAs @joris mentions, you may need to change above to np.int_(data[1:,1:]) to have the correct data type.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 20763012,
      "answer_id": 20763459,
      "question_score": 444,
      "answer_score": 449,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "list",
        "numpy"
      ]
    }
  },
  {
    "prompt": "pandas get rows which are NOT in other dataframe\n\nI've two pandas data frames that have some rows in common.\nSuppose dataframe2 is a subset of dataframe1.\nHow can I get the rows of dataframe1 which are not in dataframe2?\ndf1 = pandas.DataFrame(data = {'col1' : [1, 2, 3, 4, 5], 'col2' : [10, 11, 12, 13, 14]}) \ndf2 = pandas.DataFrame(data = {'col1' : [1, 2, 3], 'col2' : [10, 11, 12]})\n\ndf1\n   col1  col2\n0     1    10\n1     2    11\n2     3    12\n3     4    13\n4     5    14\n\ndf2\n   col1  col2\n0     1    10\n1     2    11\n2     3    12\n\nExpected resu",
    "response": "One method would be to store the result of an inner merge form both dfs, then we can simply select the rows when one column's values are not in this common:\n\nIn [119]:\n\ncommon = df1.merge(df2,on=['col1','col2'])\nprint(common)\ndf1[(~df1.col1.isin(common.col1))&(~df1.col2.isin(common.col2))]\n   col1  col2\n0     1    10\n1     2    11\n2     3    12\nOut[119]:\n   col1  col2\n3     4    13\n4     5    14\n\n\nEDIT\n\nAnother method as you've found is to use isin which will produce NaN rows which you can drop:\n\nIn [138]:\n\ndf1[~df1.isin(df2)].dropna()\nOut[138]:\n   col1  col2\n3     4    13\n4     5    14\n\n\nHowever if df2 does not start rows in the same manner then this won't work:\n\ndf2 = pd.DataFrame(data = {'col1' : [2, 3,4], 'col2' : [11, 12,13]})\n\n\nwill produce the entire df:\n\nIn [140]:\n\ndf1[~df1.isin(df2)].dropna()\nOut[140]:\n   col1  col2\n0     1    10\n1     2    11\n2     3    12\n3     4    13\n4     5    14",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 28901683,
      "answer_id": 28902170,
      "question_score": 443,
      "answer_score": 264,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe"
      ]
    }
  },
  {
    "prompt": "pandas: filter rows of DataFrame with operator chaining\n\nMost operations in pandas can be accomplished with operator chaining (groupby, aggregate, apply, etc), but the only way I've found to filter rows is via normal bracket indexing\n\ndf_filtered = df[df['column'] == value]\n\n\nThis is unappealing as it requires I assign df to a variable before being able to filter on its values.  Is there something more like the following?\n\ndf_filtered = df.mask(lambda x: x['column'] == value)",
    "response": "I'm not entirely sure what you want, and your last line of code does not help either, but anyway:\n\n\"Chained\" filtering is done by \"chaining\" the criteria in the boolean index.\n\nIn [96]: df\nOut[96]:\n   A  B  C  D\na  1  4  9  1\nb  4  5  0  2\nc  5  5  1  0\nd  1  3  9  6\n\nIn [99]: df[(df.A == 1) & (df.D == 6)]\nOut[99]:\n   A  B  C  D\nd  1  3  9  6\n\n\nIf you want to chain methods, you can add your own mask method and use that one.\n\nIn [90]: def mask(df, key, value):\n   ....:     return df[df[key] == value]\n   ....:\n\nIn [92]: pandas.DataFrame.mask = mask\n\nIn [93]: df = pandas.DataFrame(np.random.randint(0, 10, (4,4)), index=list('abcd'), columns=list('ABCD'))\n\nIn [95]: df.ix['d','A'] = df.ix['a', 'A']\n\nIn [96]: df\nOut[96]:\n   A  B  C  D\na  1  4  9  1\nb  4  5  0  2\nc  5  5  1  0\nd  1  3  9  6\n\nIn [97]: df.mask('A', 1)\nOut[97]:\n   A  B  C  D\na  1  4  9  1\nd  1  3  9  6\n\nIn [98]: df.mask('A', 1).mask('D', 6)\nOut[98]:\n   A  B  C  D\nd  1  3  9  6",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 11869910,
      "answer_id": 11872393,
      "question_score": 443,
      "answer_score": 479,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe"
      ]
    }
  },
  {
    "prompt": "How to get/set a pandas index column title or name?\n\nHow do I get the index column name in Python's pandas? Here's an example dataframe:\n             Column 1\nIndex Title          \nApples              1\nOranges             2\nPuppies             3\nDucks               4  \n\nWhat I'm trying to do is get/set the dataframe's index title. Here is what I tried:\nimport pandas as pd\n\ndata = {'Column 1'   : [1., 2., 3., 4.], \n        'Index Title': [\"Apples\", \"Oranges\", \"Puppies\", \"Ducks\"]}\ndf = pd.DataFrame(data)\ndf.index = df[\"Index Title\"]\ndel df[\"Index T",
    "response": "You can just get/set the index via its name property\n\nIn [7]: df.index.name\nOut[7]: 'Index Title'\n\nIn [8]: df.index.name = 'foo'\n\nIn [9]: df.index.name\nOut[9]: 'foo'\n\nIn [10]: df\nOut[10]: \n         Column 1\nfoo              \nApples          1\nOranges         2\nPuppies         3\nDucks           4",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 18022845,
      "answer_id": 18023468,
      "question_score": 437,
      "answer_score": 625,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe",
        "indexing",
        "rename"
      ]
    }
  },
  {
    "prompt": "What does axis in pandas mean?\n\nHere is my code to generate a dataframe:\nimport pandas as pd\nimport numpy as np\n\ndff = pd.DataFrame(np.random.randn(1, 2), columns=list('AB'))\n\nthen I got the dataframe:\n          A        B\n0  0.626386  1.52325\n\nWhen I type the command dff.mean(axis=1), I get:\n0    1.074821\ndtype: float64\n\nAccording to the reference of pandas, axis=1 stands for columns and I expect the result of the command to be\nA    0.626386\nB    1.523255\ndtype: float64\n\nSo what does axis in pandas mean?",
    "response": "It specifies the axis along which the means are computed. By default axis=0. This is consistent with the numpy.mean usage when axis is specified explicitly (in numpy.mean, axis==None by default, which computes the mean value over the flattened array) , in which axis=0 along the rows (namely, index in pandas), and axis=1 along the columns. For added clarity, one may choose to specify axis='index' (instead of axis=0) or axis='columns' (instead of axis=1).\n          A        B\n0  0.626386  1.52325  → → axis=1 → →\n          ↓        ↓\n          ↓ axis=0 ↓\n          ↓        ↓",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 22149584,
      "answer_id": 22149930,
      "question_score": 434,
      "answer_score": 559,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe"
      ]
    }
  },
  {
    "prompt": "Pandas DataFrame to List of Dictionaries\n\nI have the following DataFrame:\n\ncustomer    item1      item2    item3\n1           apple      milk     tomato\n2           water      orange   potato\n3           juice      mango    chips\n\nwhich I want to translate it to list of dictionaries per row\nrows = [\n    {\n        'customer': 1,\n        'item1': 'apple',\n        'item2': 'milk',\n        'item3': 'tomato'\n    }, {\n        'customer': 2,\n        'item1':\n        'water',\n        'item2': 'orange',\n        'item3': 'potato'\n    }, {\n        ",
    "response": "Use df.to_dict('records') -- gives the output without having to transpose externally.\nIn [2]: df.to_dict('records')\nOut[2]:\n[{'customer': 1L, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},\n {'customer': 2L, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},\n {'customer': 3L, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}]",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 29815129,
      "answer_id": 29816143,
      "question_score": 434,
      "answer_score": 605,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "dataframe"
      ]
    }
  },
  {
    "prompt": "Apply multiple functions to multiple groupby columns\n\nThe docs show how to apply multiple functions on a groupby object at a time using a dict with the output column names as the keys:\nIn [563]: grouped['D'].agg({'result1' : np.sum,\n   .....:                   'result2' : np.mean})\n   .....:\nOut[563]: \n      result2   result1\nA                      \nbar -0.579846 -1.739537\nfoo -0.280588 -1.402938\n\nHowever, this only works on a Series groupby object. And when a dict is similarly passed to a groupby DataFrame, it expects the keys to be the column nam",
    "response": "The second half of the currently accepted answer is outdated and has two deprecations. First and most important, you can no longer pass a dictionary of dictionaries to the agg groupby method. Second, never use .ix.\nIf you desire to work with two separate columns at the same time I would suggest using the apply method which implicitly passes a DataFrame to the applied function. Let's use a similar dataframe as the one from above\ndf = pd.DataFrame(np.random.rand(4,4), columns=list('abcd'))\ndf['group'] = [0, 0, 1, 1]\ndf\n\n          a         b         c         d  group\n0  0.418500  0.030955  0.874869  0.145641      0\n1  0.446069  0.901153  0.095052  0.487040      0\n2  0.843026  0.936169  0.926090  0.041722      1\n3  0.635846  0.439175  0.828787  0.714123      1\n\nA dictionary mapped from column names to aggregation functions is still a perfectly good way to perform an aggregation.\ndf.groupby('group').agg({'a':['sum', 'max'], \n                         'b':'mean', \n                         '",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 14529838,
      "answer_id": 47103408,
      "question_score": 431,
      "answer_score": 625,
      "is_accepted": true,
      "tags": [
        "python",
        "group-by",
        "aggregate-functions",
        "pandas"
      ]
    }
  },
  {
    "prompt": "Combining two Series into a DataFrame in pandas\n\nI have two Series s1 and s2 with the same (non-consecutive) indices. How do I combine s1 and s2 to being two columns in a DataFrame and keep one of the indices as a third column?",
    "response": "I think concat is a nice way to do this. If they are present it uses the name attributes of the Series as the columns (otherwise it simply numbers them):\n\nIn [1]: s1 = pd.Series([1, 2], index=['A', 'B'], name='s1')\n\nIn [2]: s2 = pd.Series([3, 4], index=['A', 'B'], name='s2')\n\nIn [3]: pd.concat([s1, s2], axis=1)\nOut[3]:\n   s1  s2\nA   1   3\nB   2   4\n\nIn [4]: pd.concat([s1, s2], axis=1).reset_index()\nOut[4]:\n  index  s1  s2\n0     A   1   3\n1     B   2   4\n\n\nNote: This extends to more than 2 Series.",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 18062135,
      "answer_id": 18062521,
      "question_score": 426,
      "answer_score": 610,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "series",
        "dataframe"
      ]
    }
  },
  {
    "prompt": "Keep only date part when using pandas.to_datetime\n\nI use pandas.to_datetime to parse the dates in my data. Pandas by default represents the dates with datetime64[ns] even though the dates are all daily only.\nI wonder whether there is an elegant/clever way to convert the dates to datetime.date or datetime64[D] so that, when I write the data to CSV, the dates are not appended with 00:00:00. I know I can convert the type manually element-by-element:\n\n[dt.to_datetime().date() for dt in df.dates]\n\n\nBut this is really slow since I have many rows and i",
    "response": "Since version 0.15.0 this can now be easily done using .dt to access just the date component:\ndf['just_date'] = df['dates'].dt.date\n\nThe above returns datetime.date, so object dtype. If you want to keep the dtype as datetime64 then you can just normalize:\ndf['normalised_date'] = df['dates'].dt.normalize()\n\nThis sets the time component to midnight, i.e. 00:00:00, but the display shows just the date value.\n\npandas.Series.dt",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 16176996,
      "answer_id": 34277514,
      "question_score": 425,
      "answer_score": 641,
      "is_accepted": false,
      "tags": [
        "python",
        "pandas",
        "csv",
        "datetime",
        "series"
      ]
    }
  },
  {
    "prompt": "how do I insert a column at a specific column index in pandas?\n\nCan I insert a column at a specific column index in pandas? \n\nimport pandas as pd\ndf = pd.DataFrame({'l':['a','b','c','d'], 'v':[1,2,1,2]})\ndf['n'] = 0\n\n\nThis will put column n as the last column of df, but isn't there a way to tell df to put n at the beginning?",
    "response": "see docs: http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.insert.html\nusing loc = 0 will insert at the beginning\ndf.insert(loc, column, value)\n\n\ndf = pd.DataFrame({'B': [1, 2, 3], 'C': [4, 5, 6]})\n\ndf\nOut: \n   B  C\n0  1  4\n1  2  5\n2  3  6\n\nidx = 0\nnew_col = [7, 8, 9]  # can be a list, a Series, an array or a scalar   \ndf.insert(loc=idx, column='A', value=new_col)\n\ndf\nOut: \n   A  B  C\n0  7  1  4\n1  8  2  5\n2  9  3  6",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 18674064,
      "answer_id": 18674915,
      "question_score": 422,
      "answer_score": 740,
      "is_accepted": true,
      "tags": [
        "python",
        "indexing",
        "pandas"
      ]
    }
  },
  {
    "prompt": "Pandas &#39;count(distinct)&#39; equivalent\n\nI am using Pandas as a database substitute as I have multiple databases (Oracle, SQL Server, etc.), and I am unable to make a sequence of commands to a SQL equivalent.\nI have a table loaded in a DataFrame with some columns:\nYEARMONTH, CLIENTCODE, SIZE, etc., etc.\n\nIn SQL, to count the amount of different clients per year would be:\nSELECT count(distinct CLIENTCODE) FROM table GROUP BY YEARMONTH;\n\nAnd the result would be\n201301    5000\n201302    13245\n\nHow can I do that in Pandas?",
    "response": "I believe this is what you want:\n\ntable.groupby('YEARMONTH').CLIENTCODE.nunique()\n\n\nExample:\n\nIn [2]: table\nOut[2]: \n   CLIENTCODE  YEARMONTH\n0           1     201301\n1           1     201301\n2           2     201301\n3           1     201302\n4           2     201302\n5           2     201302\n6           3     201302\n\nIn [3]: table.groupby('YEARMONTH').CLIENTCODE.nunique()\nOut[3]: \nYEARMONTH\n201301       2\n201302       3",
    "metadata": {
      "source": "stackoverflow",
      "question_id": 15411158,
      "answer_id": 15411596,
      "question_score": 421,
      "answer_score": 601,
      "is_accepted": true,
      "tags": [
        "python",
        "pandas",
        "group-by",
        "count",
        "duplicates"
      ]
    }
  }
]